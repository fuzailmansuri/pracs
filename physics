INDEX

Practical No.
Description 
Page  
No.
Date
Sign
1. 

Conversion from different file formats to HORUS format.

09/10/23


A 
Conversion from text delimited csv to HORUS format.




B 
Conversion from XML to HORUS format.




C 
Conversion from JSON to HORUS format.




D 
Conversion from MYSQL database to HORUS format.




E 
Conversion from picture (jpeg) to HORUS format.




F 
Conversion from video to HORUS format.




G 
Conversion from audio to HORUS format.



2. 

Utilities and Auditing

16/10/23


A 
Fixer Utilities 
1. Removing leading and lagging spaces from data entry 
2. Removing non-printable characters from data entry 
3. Reformatting data entry to match specific formatting criteria




B 
Data Binning and Bucketing




C 
Averaging of data




D 
Outlier Detection




E 
Logging: Write python/R program for basic logging in Data Science.



3 

Retrieving Data

30/10/23


A 
Perform data processing using R




B 
Program to retrieve different attributes of data




C 
Data Pattern  
Example 1 
Example 2




D 
Loading IP_DATA_ALL 
Haversine distance calculation using Vermeulen dataset




E 
Building a diagram for scheduling of jobs

06/11/23


F 
Picking content for billboards




G
Understanding your online visitor data




H
XML Processing




I
Connecting to other data sources
SQLite
MySQL
MS Excel



4 

Assessing Data

27/11/23


A 
Perform error management on the given data 
1. Drop the columns where all elements are missing values
2. Drop the columns where any of the elements is missing values 3. Keep only the rows that contain a maximum of two non-missing values 
4. Fill All Missing Values with the Mean, Median, Mode, Minimum, and Maximum of the Particular Numeric Column




B 
Write python/R program to create the network routing diagram from the given data on routers in assess superstep.




C 
Write python/R program to build directed acyclic graph.
Example 1: Company location DAG
Example 2: Customer location DAG 
Example 3: DAG using GPS data

04/12/23


D 
Write python/R program to pick the content for Billboards for the given data.




E
Write python/R program to plan the locations of warehouses



5 

Processing Data

10/10/23


A 
Build the time hub, links and satellites.




B 
Golden Nominal




C 
Vehicles as object hub and satellite using python and SQLite.




D 
Human-Environment interaction



6 

Transforming Data

17/10/23


A 
Program to show the details of hub: person being born.




B 
Building dimension Person, Time and fact PersonBornAtTime




C 
Building a Data warehouse for transform superstep.




D 
Write python program to demonstrate Simple Linear Regression.



7 

Organizing Data

25/10/23


A 
Write python/R program to perform the horizontal style subset or slice of the data warehouse data. 




B 
Write python/R program to perform the vertical style subset or slice of th
e data warehouse data. 




C 
Write python/R program to perform the island style subset or slice of the data warehouse data. 




D 
Write python/R program to perform the secure vault style subset or slice of the data warehouse data and attach the result to the person who performs the query.

31/10/23


E 
Write python program to demonstrate Association rule mining.




F 
Write python program to create network routing diagram in organizing superstep.



8 

Generating Data

7/11/23


A 
Write python/R program to perform data visualization to create following graphs using profit data. 
1. Pie Graph 
2. Double Pie Graph 
3. Line Graph 
4. Vertical Bar Graph 
5. Horizontal Bar Graph 
6. Area Graph 
7. Scatter Graph 
8. Hex Bin Graph




B 
Write python/R program to perform data visualization to create following advanced graphs/plots for the data. 
1. Kernel Density Estimation (KDE) Graph 
2. Scatter Matrix 
3. Andrew’s Curves 
4. Parallel Coordinates 
5. RADVIZ method 
6. Lag Plot 
7. Autocorrelation Plot 
8. Bootstrap Plot 
9. Contour Graphs 
10. 3D Graph

28/11/23

9 

Data Visualization with Power BI.

5/12/23

















Practical 1
AIM: Write Python program to convert files from different formats to HORUS format.
Theory:
The Homogeneous Ontology for Recursive Uniform Schema (HORUS) is used as an internal data format structure that enables the framework to reduce the permutations of transformations required by the framework. The use of HORUS methodology results in a hub-and-spoke data transformation approach. External data formats are converted to HORUS format, and then a HORUS format is transformed into any other external format. The basic concept is to take native raw data and then transform it first to a single format. That means that there is only one format for text files, one format for JSON or XML, one format for images and video. Therefore, to achieve any-to-any transformation coverage, the framework’s only requirements are a dataformat- to-HORUS and HORUS-to- data-format converter.

    A. Text Delimited CSV format to HORUS format.
Code:
import pandas as pd
#input agreement===========================================
sInputFileName='D:/DSPrac/Country_Code.csv'
InputData=pd.read_csv(sInputFileName, encoding="latin-1")
print('Input Data Values====================')
print(InputData)
print('=====================')
#Processing Rules==========================================
ProcessData=InputData
#remove columns ISO-2-code and ISO-3-code==================
ProcessData.drop('ISO-2-CODE',axis=1,inplace=True)
ProcessData.drop('ISO-3-Code',axis=1,inplace=True)
#rename Country and ISO-M49================================
ProcessData.rename(columns={'Country':'CountryName'},inplace=True)
ProcessData.rename(columns={'ISO-M49':'CountryNumber'},inplace=True)
#set new Index=============================================
ProcessData.set_index('CountryNumber',inplace=False)
#sort data by CurrencyNumber===============================
ProcessData.sort_values('CountryName',axis=0,ascending=False,inplace=True)
print('Process Data Values')
print(ProcessData)
#Output Agreement==========================================
OutputData=ProcessData
sOutputFileName='D:/DSPrac/HORUS-CSV-Country.csv'

OutputData.to_csv(sOutputFileName,index=False)
print('CSV to HORUS - Done')
print('Nishi Jain-53004230036')

Output:



    B. XML format to HORUS format.
Code:
import pandas as pd
import xml.etree.ElementTree as ET
#=============================================================
def df2xml(data):
    header = data.columns
    root = ET.Element('root')
    for row in range(data.shape[0]):
        entry = ET.SubElement(root,'entry')
        for index in range(data.shape[1]):
            schild=str(header[index])
            child = ET.SubElement(entry, schild)
            if str(data[schild][row]) != 'nan':
                child.text = str(data[schild][row])
            else:
                child.text = 'n/a'
            entry.append(child)
    result = ET.tostring(root)
    return result
#=============================================================
def xml2df(xml_data):
    root = ET.XML(xml_data) 
    all_records = []
    for i, child in enumerate(root):
        record = {}
        for subchild in child:
            record[subchild.tag] = subchild.text
        all_records.append(record)
    return pd.DataFrame(all_records)
#=============================================================
# Input Agreement ============================================
#=============================================================
sInputFileName='D:/DSPrac/practical-data-science-master/VKHCG/05-DS/9999-Data/Country_Code.xml'
InputData = open(sInputFileName).read()
print('Input Data Values ===================================')
print(InputData)
#=============================================================
# Processing Rules ===========================================
#=============================================================
ProcessDataXML=InputData
# XML to Data Frame
ProcessData=xml2df(ProcessDataXML)
# Remove columns ISO-2-Code and ISO-3-CODE
ProcessData.drop('ISO-2-CODE', axis=1,inplace=True)
ProcessData.drop('ISO-3-Code', axis=1,inplace=True)
# Rename Country and ISO-M49
ProcessData.rename(columns={'Country': 'CountryName'}, inplace=True)
ProcessData.rename(columns={'ISO-M49': 'CountryNumber'}, inplace=True)
# Set new Index
ProcessData.set_index('CountryNumber', inplace=False)
# Sort data by CurrencyNumber
ProcessData.sort_values('CountryName', axis=0, ascending=False, inplace=True)
print('Process Data Values =================================')
print(ProcessData)
#=============================================================
# Output Agreement ===========================================
#=============================================================
OutputData=ProcessData
sOutputFileName='D:/DSPrac/HORUS-XML-Country.csv'
OutputData.to_csv(sOutputFileName, index = False)
print('XML to HORUS - Done')
print('Nishi Jain-53004230036')

Output: 




    C. JSON to HORUS format.
Code:
import pandas as pd
sInputFileName='D:/DSPrac/Files/Country_Code.json'
InputData=pd.read_json(sInputFileName,orient='index',encoding="latin-1")
print('Input Data Values')
print(InputData)
ProcessData=InputData
ProcessData.drop('ISO-2-CODE',axis=1,inplace=True)
ProcessData.drop('ISO-3-Code',axis=1,inplace=True)
ProcessData.rename(columns={'Country':'CountryName'},inplace=True)
ProcessData.rename(columns={'ISO-M49':'CountryNumber'},inplace=True)
#Set new Index
ProcessData.set_index('CountryNumber',inplace=False)
#Sort data by currency number
ProcessData.sort_values('CountryName',axis=0,ascending=False,inplace=True)
print('Process Data Values')
print(ProcessData)
OutputData=ProcessData
sOutputFileName='D:/DsPrac/Files/HORUS-JSON-Country.csv'
OutputData.to_csv(sOutputFileName,index=False)
print('JSON to HORUS-Done')
print('Nishi Jain-53004230036')

Output: 

    D. MySql Database to HORUS format.
Code:
import pandas as pd
import sqlite3 as sq
sInputFileName='D:/DSPrac/Files/utility.db'
sInputTable='Country_Code'
conn = sq.connect(sInputFileName)
sSQL='select * FROM ' + sInputTable + ';'
InputData=pd.read_sql_query(sSQL, conn)
print('Input Data Values ===================================')
print(InputData)
print('=====================================================')
ProcessData=InputData
ProcessData.drop('ISO-2-CODE', axis=1,inplace=True)
ProcessData.drop('ISO-3-Code', axis=1,inplace=True)
ProcessData.rename(columns={'Country': 'CountryName'}, inplace=True)
ProcessData.rename(columns={'ISO-M49': 'CountryNumber'}, inplace=True)
ProcessData.set_index('CountryNumber', inplace=False)
ProcessData.sort_values('CountryName', axis=0, ascending=False, inplace=True)
print('Process Data Values =================================')
print(ProcessData)
print('=====================================================')
OutputData=ProcessData
sOutputFileName='D:/DSPrac/Files/db-Horus.csv'
OutputData.to_csv(sOutputFileName, index = False)
print('Database to HORUS - Done')
print('Nishi Jain-53004230036')

Output: 

    E. Picture (JPEG) to HORUS format.
Code:
!pip install scikit-fuzzy
from matplotlib.pyplot import imread
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
sInputFileName='D:/DSPrac/practical-data-science-master/VKHCG/05-DS/9999-Data/Angus.jpg'
InputData = imread(sInputFileName)
print('Input Data Values ===================================')
print('X: ',InputData.shape[0])
print('Y: ',InputData.shape[1])
print('RGBA: ', InputData.shape[2])
print('=====================================================')
ProcessRawData=InputData.flatten()
y=InputData.shape[2] + 3
x=int(ProcessRawData.shape[0]/y)
ProcessData=pd.DataFrame(np.reshape(ProcessRawData, (x, y)))
sColumns= ['XAxis','YAxis','Red', 'Green', 'Blue','Alpha']
ProcessData.columns=sColumns
ProcessData.index.names =['ID']
print('Rows: ',ProcessData.shape[0])
print('Columns :',ProcessData.shape[1])
print('=====================================================')
print('Process Data Values =================================')
print('=====================================================')
plt.imshow(InputData)
plt.show() 
print('=====================================================')
OutputData=ProcessData
print('Storing File')
sOutputFileName='D:/DSPrac/Files/HORUS-Picture.csv'
OutputData.to_csv(sOutputFileName, index = False)
print('Picture to HORUS - Done')
print('Nishi Jain-53004230036')

Output:




    F. Video to HORUS format. 
Code:

Output:
    G. Audio to HORUS format.
Code:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from scipy.io import wavfile
def show_info(aname, a,r):
    print ('----------------')
    print ("Audio:", aname)
    print ('----------------')
    print ("Rate:", r)
    print ('----------------')
    print ("shape:", a.shape)
    print ("dtype:", a.dtype)
    print ("min, max:", a.min(), a.max())
    print ('----------------')
    plot_info(aname, a,r)
def plot_info(aname, a,r):        
    sTitle= 'Signal Wave - '+ aname + ' at ' + str(r) + 'hz'
    plt.title(sTitle)
    sLegend=[]
    for c in range(a.shape[1]):
        sLabel = 'Ch' + str(c+1)
        sLegend=sLegend+[str(c+1)]
        plt.plot(a[:,c], label=sLabel)
    plt.legend(sLegend)
    plt.show()
sInputFileName='D:/DSPrac/practical-data-science-master/VKHCG/05-DS/9999-Data/2ch-sound.wav'
print('=====================================================')
print('Processing : ', sInputFileName)
print('=====================================================')
InputRate, InputData = wavfile.read(sInputFileName)
show_info("2 channel", InputData,InputRate)
ProcessData=pd.DataFrame(InputData)
sColumns= ['Ch1','Ch2']
ProcessData.columns=sColumns
OutputData=ProcessData
sOutputFileName='D:/DSPrac/Files/HORUS-Audio-2ch.csv'
OutputData.to_csv(sOutputFileName, index = False)
sInputFileName='D:/DSPrac/practical-data-science-master/VKHCG/05-DS/9999-Data/4ch-sound.wav'
print('=====================================================')
print('Processing : ', sInputFileName)
print('=====================================================')
InputRate, InputData = wavfile.read(sInputFileName)
show_info("4 channel", InputData,InputRate)
ProcessData=pd.DataFrame(InputData)
sColumns= ['Ch1','Ch2','Ch3', 'Ch4']
ProcessData.columns=sColumns
OutputData=ProcessData
sOutputFileName='D:/DSPrac/Files/HORUS-Audio-4ch.csv'
OutputData.to_csv(sOutputFileName, index = False)
sInputFileName='D:/DSPrac/practical-data-science-master/VKHCG/05-DS/9999-Data/6ch-sound.wav'
print('=====================================================')
print('Processing : ', sInputFileName)
print('=====================================================')
InputRate, InputData = wavfile.read(sInputFileName)
show_info("6 channel", InputData,InputRate)
ProcessData=pd.DataFrame(InputData)
sColumns= ['Ch1','Ch2','Ch3', 'Ch4', 'Ch5','Ch6']
ProcessData.columns=sColumns
OutputData=ProcessData
sOutputFileName='D:/DSPrac/Files/HORUS-Audio-6ch.csv'
OutputData.to_csv(sOutputFileName, index = False)
sInputFileName='D:/DSPrac/practical-data-science-master/VKHCG/05-DS/9999-Data/8ch-sound.wav'
print('=====================================================')
print('Processing : ', sInputFileName)
print('=====================================================')
InputRate, InputData = wavfile.read(sInputFileName)
show_info("8 channel", InputData,InputRate)
ProcessData=pd.DataFrame(InputData)
sColumns= ['Ch1','Ch2','Ch3', 'Ch4', 'Ch5','Ch6','Ch7','Ch8']
ProcessData.columns=sColumns
OutputData=ProcessData
sOutputFileName='D:/DSPrac/Files/HORUS-Audio-8ch.csv'
OutputData.to_csv(sOutputFileName, index = False)
print('Audio to HORUS - Done')
print('Nishi Jain-53004230036')

Output:

 












Practical 2
AIM: Utilities and Auditing
Theory:
Basic Utility Design:
1. Load data as per input agreement.
2. Apply processing rules of utility.
3. Save data as per output agreement.
There are three types of utilities:
1. Data processing utilities
2. Maintenance utilities
3. Processing utility
    A. Fixer Utility.
Code:
import string 
import datetime as dt
#1 Removing leading or lagging spaces from a data entry
print('1.Removing leading or lagging spaces from a data entry');
baddata="Data Science with too many spaces is bad!!!"
print('>',baddata,'<')
cleandata=baddata.strip()
print('>',cleandata,'<')
#2 Removing nonprintable characters from a data entry
print('2.Removing nonprintable characters from a data entry')
printable=set(string.printable)
baddata="Data\x00Science with\x02 funny characters is \x10bad!!!"
cleandata="join.
(filter(lambda x: x in string.printable,baddata))"
print('Bad Data :',baddata);
print('Clean Data:',cleandata);
#3 Reformatting data entry to match specific formatting criteria.
# Convert YYYY/MM/DD to DD Month YYYY
print('3.Reformatting data entry to match specific formatting criteria.')
baddate=dt.date(2022,11,10)
baddata=format(baddate,'%Y-%m-%d')
gooddate=dt.datetime.strptime(baddata,'%Y-%m-%d')
gooddata=format(gooddate,'%d%B%Y')
print('Bad Data:',baddata)
print('Good Data:',gooddata)
print('Nishi Jain-53004230036')

Output:



    B. Data Binning or Bucketing.
Binning is a data preprocessing technique used to reduce the effects of minor observation errors. Statistical data binning is a way to group a number of more or less continuous values into a smaller number of “bins.”
Code:
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
np.random.seed(0)
#example data
mu=90         #mean of distribution
sigma=25      #Standard deviation of distribution 
x = mu + sigma * np.random.randn(5000)
num_bins=25
fig, ax =plt.subplots()
#the histogram of the data
n, bins, patches = ax.hist(x,num_bins,density=1)
#add a 'best fit' line
y = stats.norm.pdf(bins,mu,sigma)
#mlab.normpdf(bins,mu,sigma)
ax.plot(bins,y,'--')
ax.set_xlabel('Example Data')
ax.set_ylabel('Probablity density')
sTitle=r'Histogram'+str(len(x))+'entries into'+str(num_bins)+'Bins: $\mu='+str(mu) + '$,$\sigma='+str(sigma)+'$'
ax.set_title(sTitle)
fig.tight_layout()
sPathFig='D:/DSPrac/practical-data-science-master/VKHCG/05-DS/4000-UL/0200-DU/DU-Histogram.png'
fig.savefig(sPathFig)
plt.show()
print('Nishi Jain-53004230036')

Output:



    C. Averaging of Data.
The use of averaging of features value enables the reduction of data volumes in a control fashion to improve effective data processing.
Code: 
import pandas as pd
InputFileName='IP_DATA_CORE.csv'
OutputFileName='Retrieve_Router_Location.csv'
Base='D:/DSPrac/Files/'
print('Working Base:',Base,'using')
print('==================================')
sFileName = Base + InputFileName
print('Loading:',sFileName)
print('==================================')
IP_DATA_ALL=pd.read_csv(sFileName,header=0,low_memory=False,usecols=['Country','Place Name','Latitude','Longitude'],encoding="latin-1")
IP_DATA_ALL.rename(columns={'Place Name':'Place_Name'},inplace=True)
AllData=IP_DATA_ALL[['Country','Place_Name','Latitude']]
print(AllData)
MeanData=AllData.groupby(['Country','Place_Name'])['Latitude'].mean()
print(MeanData)
print('==================================')
print('Nishi Jain-53004230036')
Output:


    D. Outlier Detection.
Outliers are data that is so different from the rest of the data in the data set that it may be caused by an error in the data source. There is a technique called outlier detection that, with good data science, will identify these outliers.
Code: 
import pandas as pd
InputFileName='IP_DATA_CORE.csv'
OutputFileName='Retrieve_Router_Location.csv'
Base='D:/DSPrac/Files/'
print('Working Base:',Base)
print('==================================')
sFileName = Base + InputFileName
print('Loading:',sFileName)
IP_DATA_ALL=pd.read_csv(sFileName,header=0,low_memory=False,usecols=['Country','Place Name','Latitude','Longitude'],encoding="latin-1")
IP_DATA_ALL.rename(columns={'Place Name':'Place_Name'},inplace=True)
LondonData=IP_DATA_ALL.loc[IP_DATA_ALL['Place_Name']=='London']
AllData=LondonData[['Country','Place_Name','Latitude']]
print('AllData')
print(AllData)
MeanData=AllData.groupby(['Country','Place_Name'])['Latitude'].mean()
StdData=AllData.groupby(['Country','Place_Name'])['Latitude'].std()
print('Outliers')
UpperBound=float(MeanData+StdData)
print('Higher than',UpperBound)
OutliersHigher=AllData[AllData.Latitude>UpperBound]
print(OutliersHigher)
LowerBound=float(MeanData-StdData)
print('Lower than',LowerBound)
OutliersLower=AllData[AllData.Latitude<LowerBound]
print(OutliersLower)
print('Not Outliers')
OutliersNot=AllData[(AllData.Latitude>=LowerBound)&(AllData.Latitude<=UpperBound)]
print(OutliersNot)
print('Nishi Jain-53004230036')

Output:
 
    E. Logging.
Code: 
import pandas as pd
import logging
import uuid
import shutil
import time
print('Nishi Jain-53004230036')
Base='D:/DSPrac/practical-data-science-master/VKHCG'
sCompanies=['01-Vermeulen','02-Krennwallner','03-Hillman','04-Clark']
sLayers=['01-Retrieve','02-Assess','03-Process','04-Transform','05-Organise','06-Report']
sLevels=['debug','info','warning','error']
for sCompany in sCompanies:
    sFileDir=Base + '/' + sCompany 
    for sLayer in sLayers:
        log = logging.getLogger()  # root logger
        for hdlr in log.handlers[:]:  # remove all old handlers
            log.removeHandler(hdlr)
        ############################################################  
        sFileDir=Base + '/' + sCompany + '/' + sLayer + '/Logging'
        time.sleep(2)
        skey=str(uuid.uuid4())       
        sLogFile=Base + '/' + sCompany + '/' + sLayer + '/Logging/Logging_'+skey+'.log'
        print('Set up:',sLogFile)
        # set up logging to file - see previous section for more details
        logging.basicConfig(level=logging.DEBUG,
                            format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s',
                            datefmt='%m-%d %H:%M',
                            filename=sLogFile,
                            filemode='w')
        # define a Handler which writes INFO messages or higher to the sys.stderr
        console = logging.StreamHandler()
        console.setLevel(logging.INFO)
        # set a format which is simpler for console use
        formatter = logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s')
        # tell the handler to use this format
        console.setFormatter(formatter)
        # add the handler to the root logger
        logging.getLogger('').addHandler(console)
        # Now, we can log to the root logger, or any other logger. First the root...
        logging.info('Practical Data Science is fun!.')
        for sLevel in sLevels:
            sApp='Apllication-'+ sCompany + '-' + sLayer + '-' + sLevel
            logger = logging.getLogger(sApp)
            if sLevel == 'debug': 
                logger.debug('Practical Data Science logged a debugging message.')
            if sLevel == 'info': 
                logger.info('Practical Data Science logged information message.')
            if sLevel == 'warning': 
                logger.warning('Practical Data Science logged a warning message.')
            if sLevel == 'error': 
                logger.error('Practical Data Science logged an error message.')

Output:















Practical 3
AIM: Retrieve Superstep
Theory:
The Retrieve superstep is a practical method for importing completely into the processing ecosystem a data lake consisting of various external data sources. The Retrieve superstep is the first contact between your data science and the source systems. I will guide you through a methodology of how to handle this discovery of the data up to the point you have all the data you need to evaluate the system you are working with, by deploying your data science skills. The successful retrieval of the data is a major stepping-stone to ensuring that you are performing good data science. Data lineage delivers the audit trail of the data elements at the lowest granular level, to ensure full data governance.
    A. Program the following data processing using R.
Code: 
> IP_DATA_ALL<-read.csv("D:/DSPrac/Files/IP_DATA_ALL.csv")
> View(IP_DATA_ALL)

>spec(IP_DATA_ALL)
>library(tibble)
>set_tidy_names(IP_DATA_ALL,syntactic=TRUE,quiet=FALSE)
>IP_DATA_ALL_FIX=set tidy names(IP_DATA_ALL, syntactic= TRUE, quiet=TRUE)
>sapply(IP_DATA_ALL_FIX,typeof)
>library(data.table)
>hist_country=data.table(Country=unique(IP_DATA_ALL_FIX[is.na(IP_DATA_ALL_FIX[‘Country’])==0,]$Country))
>setorder(hist_country,’Country’)
>hist_country_with_id=rowid_to_column(hist_country,var=”RowIDCountry”)
>View(hist_country_fix)
>IP_DATA_COUNTRY_FREQ=data.table(with(IP_DATA_ALL_FIX,table(Country)))
>View(IP_DATA_COUNTRY_FREQ)

>sapply(IP_DATA_ALL_FIX[,’Latitude’],min,na.rm=TRUE)
>sapply(IP_DATA_ALL_FIX[,’Country’],min,na.rm=TRUE)
>sapply(IP_DATA_ALL_FIX[,’Latitude’],max,na.rm=TRUE)
>sapply(IP_DATA_ALL_FIX[,’Country’],max,na.rm=TRUE)
>sapply(IP_DATA_ALL_FIX[,’Latitude’],mean,na.rm=TRUE)
>sapply(IP_DATA_ALL_FIX[,’Latitude’],median,na.rm=TRUE)
>sapply(IP_DATA_ALL_FIX[,’Latitude’],range,na.rm=TRUE)
>sapply(IP_DATA_ALL_FIX[,’Latitude’],quantile,na.rm=TRUE)
>sapply(IP_DATA_ALL_FIX[,’Latitude’],sd,na.rm=TRUE)
>sapply(IP_DATA_ALL_FIX[,’Longitude’],sd,na.rm=TRUE)

Output:



    B. Program to retrieve different attributes of data.
Code: 
import pandas as pd
sFileName='D:/DSPrac/Files/IP_DATA_ALL.csv'
print('Loading :',sFileName)
IP_DATA_ALL=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1")
sFileDir='D:/DSPrac/practical-data-science-master/VKHCG/01-Vermeulen/01-Retrieve/01-EDS/02-Python'
print('Rows:', IP_DATA_ALL.shape[0])
print('Columns:', IP_DATA_ALL.shape[1])
print('### Raw Data Set #####################################')
for i in range(0,len(IP_DATA_ALL.columns)):
    print(IP_DATA_ALL.columns[i],type(IP_DATA_ALL.columns[i]))
print('### Fixed Data Set ###################################')
IP_DATA_ALL_FIX=IP_DATA_ALL
for i in range(0,len(IP_DATA_ALL.columns)):
    cNameOld=IP_DATA_ALL_FIX.columns[i] + '     '
    cNameNew=cNameOld.strip().replace(" ", ".")
    IP_DATA_ALL_FIX.columns.values[i] = cNameNew
    print(IP_DATA_ALL.columns[i],type(IP_DATA_ALL.columns[i]))
#print(IP_DATA_ALL_FIX.head())
print('Fixed Data Set with ID')
IP_DATA_ALL_with_ID=IP_DATA_ALL_FIX
IP_DATA_ALL_with_ID.index.names = ['RowID']
#print(IP_DATA_ALL_with_ID.head())
sFileName2=sFileDir + '/Retrieve_IP_DATA.csv'
IP_DATA_ALL_with_ID.to_csv(sFileName2, index = True, encoding="latin-1")
print('Done!! ############################################')
print('Nishi Jain-53004230036')

Output:






    C. Data Pattern using R.
To determine a pattern of the data values, replace all alphabet values with an uppercase case A, all numbers with an uppercase N, and replace any spaces with a lowercase letter b and all other unknown characters with a lowercase u. As a result, “Good Book 101” becomes “AAAAbAAAAbNNNu”. This pattern creation is beneficial for designing any specific assess rules. This pattern view of data is a quick way to identify common patterns or determine standard layouts.
Code: 
> library(readr)
> library(data.table)
data.table 1.14.4 using 2 threads (see ?getDTthreads).  Latest news: r-datatable.com
> IP_DATA_ALL<-read_csv(FileName)
Error in standardise_path(file) : object 'FileName' not found
> FileName=paste0('D:/DSPrac/Files/IP_DATA_ALL.csv')
> IP_DATA_ALL<-read_csv(FileName)
[1mindexing[0m [34mIP_DATA_ALL.csv[0m [==--------------------------------] [32m2.15GB/s[0m, eta: [36m 0s[0m
[1mindexing[0m [34mIP_DATA_ALL.csv[0m [================================] [32m497.14MB/s[0m, eta: [36m 0s[0m                                                                                                        
New names:
• `` -> `...1`
Rows: 1247502 Columns: 9
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
chr (3): Country, Place.Name, Post.Code
dbl (6): ...1, ID, Latitude, Longitude, First.IP.Number, Last.IP.Number

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
> 
> hist_country=data.table(Country=unique(IP_DATA_ALL$Country))
> pattern_country=data.table(Country=hist_country$Country,PatternCountry=hist_country$Country)
> oldchar=c(letters,LETTERS)
> newchar=replicate(length(oldchar),"A")
> 
> for(r in seq(nrow(pattern_country)))> 
> for(r in seq(nrow(pattern_country))){
+ s=pattern_country[r,]$PatternCountry;
+ for(c in seq(length(oldchar))){
+ s=chartr(oldchar[c],newchar[c],s)
+ };

+ for(n in seq(0,9,1> 
> for(r in seq(nrow(pattern_country))){
+ s=pattern_country[r,]$PatternCountry;
+ for(c in seq(length(oldchar))){
+ s=chartr(oldchar[c],newchar[c],s)
+ };
+ for(n in seq(0,9,1)){
+ s=chartr(as.character(n),"N",s)
+ };
+ s=chartr("","b",s)
+ s=chartr(".","u",s)
+ pattern_country[r,]$PatternCountry=s;
+ };
> View(pattern_country)
> library(readr)
> library(data.table)
> Base='C:/VKHCG'
> FileName=paste0(‘D:/DSPrac/Files/IP_DATA_ALL.csv’)
> IP_DATA_ALL<-read_csv(FileName)
[1mindexing[0m [34mIP_DATA_ALL.csv[0m [=---------------------------------] [32m2.15GB/s[0m, eta: [36m 0s[0m
[1mindexing[0m [34mIP_DATA_ALL.csv[0m [================================] [32m459.20MB/s[0m, eta: [36m 0s[0m
[1mindexing[0m [34mIP_DATA_ALL.csv[0m [================================] [32m456.06MB/s[0m, eta: [36m 0s[0m                                                                                                             
New names:
• `` -> `...1`
Rows: 1247502 Columns: 9
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
chr (3): Country, Place.Name, Post.Code
dbl (6): ...1, ID, Latitude, Longitude, First.IP.Number, Last.IP.Number

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
> hist_latitude=data.table(Latitude=unique(IP_DATA_ALL$Latitude))
> pattern_latitude=data.table(latitude=hist_latitude$Latitude,
+ Patternlatitude=as.character(hist_latitude$Latitude))
> oldchar=c(letters,LETTERS)
> oldc rah = (c le tt ,sre L )
> newchar=replicate(length(oldchar),"A")
> for(r in seq(nrow(pattern_latitude))){
+ s=pattern_latitude[r,]$Patternlatitude;
+ for(c in seq(length(oldchar))){ 
+ s=chartr(oldchar[c],newchar[c],s)
+ };
+ for (n in seq(0,9,1)
+ ){
+ s=chartr(as.character(n),"N",s)
+ };
+ s=chartr("","b",s)
+ s=chartr("+","u",s)
+ s=chartr("-","u",s)
+ s=chartr(".","u",s)
+ pattern_latitude[r,]$Patternlatitude=s;
+ };
> setorder(pattern_latitude,latitude)
> View(pattern_latitude[1:3])

Output: 


Example 2
This is a common use of patterns to separate common standards and structures. Pattern can be
loaded in separate retrieve procedures. If the same two patterns, NNNNuNNuNN and uuNNuNNuNN, are found, you can send NNNNuNNuNN directly to be converted into a date, while uuNNuNNuNN goes through a quality-improvement process to then route back to the same queue as NNNNuNNuNN, once it complies.
Code:
library(readr)
library(data.table)
Base='D:/DSPrac/Files'
FileName=paste0(Base,' /IP_DATA_ALL.csv')
IP_DATA_ALL <- read_csv(FileName)
hist_latitude=data.table(Latitude=unique(IP_DATA_ALL$Latitude))
pattern_latitude=data.table(latitude=hist_latitude$Latitude,
Patternlatitude=as.character(hist_latitude$Latitude))
oldchar=c(letters,LETTERS)
newchar=replicate(length(oldchar),"A")
for (r in seq(nrow(pattern_latitude))){
s=pattern_latitude[r,]$Patternlatitude;
for (c in seq(length(oldchar))){
s=chartr(oldchar[c],newchar[c],s)
};
for (n in seq(0,9,1)){
s=chartr(as.character(n),"N",s)
};
s=chartr(" ","b",s)
s=chartr("+","u",s)
s=chartr("-","u",s)
s=chartr(".","u",s)
pattern_latitude[r,]$Patternlatitude=s;
};
setorder(pattern_latitude,latitude)
View(pattern_latitude[1:3])

Output:


    D. 1] Loading IP_DATA_ALL
This data set contains all the IP address allocation in the world. It will help to locate your customer when interacting with them online.
Create a new python script and save it as Retrieve-IP_DATA_ALL.py in directory.
Code: 
import pandas as pd
sFileName='D:/DSPrac/Files/IP_DATA_ALL.csv'
print('Loading :',sFileName)
IP_DATA_ALL=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1")
sFileDir='D:/DSPrac/practical-data-science-master/VKHCG/01-Vermeulen/01-Retrieve/01-EDS/02-Python'
print('Rows:', IP_DATA_ALL.shape[0])
print('Columns:', IP_DATA_ALL.shape[1])
print('### Raw Data Set #####################################')
for i in range(0,len(IP_DATA_ALL.columns)):
    print(IP_DATA_ALL.columns[i],type(IP_DATA_ALL.columns[i]))
print('### Fixed Data Set ###################################')
IP_DATA_ALL_FIX=IP_DATA_ALL
for i in range(0,len(IP_DATA_ALL.columns)):
    cNameOld=IP_DATA_ALL_FIX.columns[i] + '     '
    cNameNew=cNameOld.strip().replace(" ", ".")
    IP_DATA_ALL_FIX.columns.values[i] = cNameNew
    print(IP_DATA_ALL.columns[i],type(IP_DATA_ALL.columns[i]))
#print(IP_DATA_ALL_FIX.head())
print('Fixed Data Set with ID')
IP_DATA_ALL_with_ID=IP_DATA_ALL_FIX
IP_DATA_ALL_with_ID.index.names = ['RowID']
#print(IP_DATA_ALL_with_ID.head())
sFileName2=sFileDir + '/Retrieve_IP_DATA.csv'
IP_DATA_ALL_with_ID.to_csv(sFileName2, index = True, encoding="latin-1")
print('Done!! ############################################')
print('Nishi Jain-53004230036')

Output:

 



2] Haversine distance calculating using Vermeulen dataset
The company has two main jobs on which to focus your attention:
-> Designing a routing diagram for company
->Planning a schedule of jobs to be performed for the router network.
Code: 
import pandas as pd
from math import radians, cos, sin, asin, sqrt
def haversine(lon1, lat1, lon2, lat2,stype):
    """
    Calculate the great circle distance between two points 
    on the earth (specified in decimal degrees)
    """
    # convert decimal degrees to radians 
    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])
    # haversine formula 
    dlon = lon2 - lon1 
    dlat = lat2 - lat1 
    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
    c = 2 * asin(sqrt(a)) 
    if stype == 'km':
        r = 6371 # Radius of earth in kilometers
    else:
        r = 3956 # Radius of earth in miles
    d=round(c * r,3)
    return d
sFileName='D:/DSPrac/Files/IP_DATA_CORE.csv'
print('Loading :',sFileName)
IP_DATA_ALL=pd.read_csv(sFileName,header=0,low_memory=False,
  usecols=['Country','Place Name','Latitude','Longitude'], encoding="latin-1")
sFileDir='D:/DSPrac/practical-data-science-master/VKHCG/01-Vermeulen/01-Retrieve/01-EDS/02-Python'
IP_DATA = IP_DATA_ALL.drop_duplicates(subset=None, keep='first', inplace=False)
IP_DATA.rename(columns={'Place Name': 'Place_Name'}, inplace=True)
IP_DATA1 = IP_DATA
IP_DATA1.insert(0, 'K', 1)
IP_DATA2 = IP_DATA1
print(IP_DATA1.shape)
IP_CROSS=pd.merge(right=IP_DATA1,left=IP_DATA2,on='K')
IP_CROSS.drop('K', axis=1, inplace=True)
IP_CROSS.rename(columns={'Longitude_x': 'Longitude_from', 'Longitude_y': 'Longitude_to'}, inplace=True)
IP_CROSS.rename(columns={'Latitude_x': 'Latitude_from', 'Latitude_y': 'Latitude_to'}, inplace=True)
IP_CROSS.rename(columns={'Place_Name_x': 'Place_Name_from', 'Place_Name_y': 'Place_Name_to'}, inplace=True)
IP_CROSS.rename(columns={'Country_x': 'Country_from', 'Country_y': 'Country_to'}, inplace=True)
IP_CROSS['DistanceBetweenKilometers'] = IP_CROSS.apply(lambda row: 
    haversine(
            row['Longitude_from'],
            row['Latitude_from'],
            row['Longitude_to'],
            row['Latitude_to'],
            'km')
            ,axis=1)
IP_CROSS['DistanceBetweenMiles'] = IP_CROSS.apply(lambda row: 
    haversine(
            row['Longitude_from'],
            row['Latitude_from'],
            row['Longitude_to'],
            row['Latitude_to'],
            'miles')
            ,axis=1)
print(IP_CROSS.shape)
sFileName2=sFileDir + '/Retrieve_IP_Routing.csv'
IP_CROSS.to_csv(sFileName2, index = False, encoding="latin-1")
print('### Done!! ############################################')
print('Nishi Jain-53004230036')

Output:


    E. Building a Diagram for the Scheduling of Jobs
Start your Python editor and create a text file named Retrieve-Router-Location,py in directory
 Code:
import pandas as pd
InputFileName='IP_DATA_CORE.csv'
OutputFileName='Retrieve_Router_Location.csv'
sFileName='D:/DSPrac/Files/' + InputFileName
print('Loading :',sFileName)
IP_DATA_ALL=pd.read_csv(sFileName,header=0,low_memory=False,
  usecols=['Country','Place Name','Latitude','Longitude'], encoding="latin-1")
IP_DATA_ALL.rename(columns={'Place Name': 'Place_Name'}, inplace=True)
sFileDir='D:/DSPrac/practical-data-science-master/VKHCG/01-Vermeulen/01-Retrieve/01-EDS/02-Python'
ROUTERLOC = IP_DATA_ALL.drop_duplicates(subset=None, keep='first', inplace=False)
print('Rows :',ROUTERLOC.shape[0])
print('Columns :',ROUTERLOC.shape[1])
sFileName2=sFileDir + '/' + OutputFileName
ROUTERLOC.to_csv(sFileName2, index = False, encoding="latin-1")
print('### Done!! ############################################')
print('Nishi Jain-53004230036')

Output:




    F. Picking Content for billboards
Start your Python editor and create a text file named Retrieve-DE-Billboard-Location,py in directory
Code:
import pandas as pd
InputFileName='DE_Billboard_Locations.csv'
OutputFileName='Retrieve_DE_Billboard_Locations.csv'
sFileName='D:/DSPrac/Files/' + InputFileName
print('Loading :',sFileName)
IP_DATA_ALL=pd.read_csv(sFileName,header=0,low_memory=False,
  usecols=['Country','PlaceName','Latitude','Longitude'])
IP_DATA_ALL.rename(columns={'PlaceName': 'Place_Name'}, inplace=True)
sFileDir='D:/DSPrac/practical-data-science-master/VKHCG/02-Krennwallner/01-Retrieve/01-EDS/02-Python'
ROUTERLOC = IP_DATA_ALL.drop_duplicates(subset=None, keep='first', inplace=False)
print('Rows :',ROUTERLOC.shape[0])
print('Columns :',ROUTERLOC.shape[1])
sFileName2=sFileDir + '/' + OutputFileName
ROUTERLOC.to_csv(sFileName2, index = False)
print('### Done!! ############################################')
print('Nishi Jain-53004230036')

Output:







    G. Understanding your online visitor data
Let us retrieve the visitor data for the billboard we have in Germany.
Several times it was found that common and important information is buried somewhere in the company’s various data sources. Investigating any direct suppliers or consumers’ upstream or downstream data sources attached to the specific business process is necessary. That is part of your skills that you are applying to data science. Numerous insightful fragments of information were found within data sources surrounding a customer’s business processes.
Start your Python editor and create a file named Retrieve-Online-Visitor.py in directory

Code:
import pandas as pd
import gzip as gz
InputFileName='IP_DATA_ALL.csv' 
OutputFileName='Retrieve_Online_Visitor' 
CompanyIn= '01-Vermeulen' 
CompanyOut= '02-Krennwallner'
Base='D:/DSPrac/Files/'
sFileName=Base + InputFileName 
print('Loading :',sFileName) 
IP_DATA_ALL=pd.read_csv(sFileName, header=0, low_memory=False,usecols=['Country','Place.Name','Latitude','Longitude','First.IP.Number','Last.IP.Number'])
IP_DATA_ALL.rename(columns={'Place.Name': 'Place_Name'}, inplace=True) 
IP_DATA_ALL.rename(columns={'First.IP.Number':'First_IP_Number'}, inplace=True)
IP_DATA_ALL.rename(columns={'Last.IP.Number':'Last_IP_Number'}, inplace=True)
sFileDir='D:/DSPrac/practical-data-science-master/VKHCG' + '/' + CompanyOut + '/01-Retrieve/01-EDS/02-Python'
visitordata = IP_DATA_ALL.drop_duplicates(subset=None, keep='first', inplace=False)
visitordatal0 = visitordata.head(10)
print('Rows :',visitordata.shape[0]) 
print('Columns :',visitordata.shape[1])
print('Export CSV')
sFileName2=sFileDir + '/' + OutputFileName + '.csv' 
visitordata.to_csv(sFileName2, index = False) 
print('Store All:',sFileName2)
sFileName3=sFileDir + '/' + OutputFileName + '_10.csv' 
visitordatal0.to_csv(sFileName3, index = False) 
print('Store 10:',sFileName3)
for z in ['gzip', 'bz2', 'xz']: 
          if z == 'gzip':
              sFileName4=sFileName2 + '.gz'
          else:
              sFileName4=sFileName2 + '.' + z 
visitordata.to_csv(sFileName4, index = False, compression=z) 
print('Store :',sFileName4)
print('Export JSON')
for sOrient in ['split','records','index', 'columns','values','table']: 
                sFileName2=sFileDir + '/' + OutputFileName + '_' + sOrient + '.json' 
                visitordata.to_json(sFileName2,orient=sOrient,force_ascii=True) 
                print('Store All:',sFileName2)
                sFileName3=sFileDir + '/' + OutputFileName + '_10_' + sOrient + '.json'
                visitordata.to_json(sFileName3,orient=sOrient,force_ascii=True) 
                print('Store 10:',sFileName3)
sFileName4=sFileName2 + '.gz' 
file_in = open(sFileName2, 'rb') 
file_out = gz.open(sFileName4, 'wb') 
file_out.writelines(file_in) 
file_in.close()
file_out.close()
print('Store GZIP All:',sFileName4)
sFileName5=sFileDir + '/' + OutputFileName + '_' + sOrient + '_UnGZip.json' 
file_in = gz.open(sFileName4, 'rb')
file_out = open(sFileName5, 'wb') 
file_out.writelines(file_in) 
file_in.close()
file_out.close()
print('Store UnGZIP All:',sFileName5)
print('Done!!')
print('Nishi Jain-53004230036')

Output:
 
 


    H. XML processing.
Start Python editor and create a file named Retrieve-Online-Visitor-XML.py in directory
Code:
import pandas as pd
import xml.etree.ElementTree as ET
def df2xml(data):
    header = data.columns
    root = ET.Element('root')
    for row in range(data.shape[0]):
        entry = ET.SubElement(root,'entry') 
    for index in range(data.shape[1]):
        schild=str(header[index])
        child = ET.SubElement(entry, schild) 
        if str(data[schild][row])!= 'nan':
            child.text = str(data[schild][row]) 
        else:
            child.text = 'n/a'
            entry.append(child)
    result = ET.tostring(root) 
    return result
def xml2df(xml_data):
    root = ET.XML(xml_data) 
    all_records = []
    for i, child in enumerate(root): 
        record = {}
    for subchild in child:
        record[subchild.tag] = subchild.text 
    all_records.append(record)
    return pd.DataFrame(all_records)
InputFileName='IP_DATA_ALL.csv'
OutputFileName='Retrieve_Online_Visitor.xml' 
CompanyIn= '01-Vermeulen'
CompanyOut= '02-Krennwallner'
Base='D:/DSPrac/Files/'
print('################################')
sFileName=Base + InputFileName 
print('Loading :',sFileName)
IP_DATA_ALL=pd.read_csv(sFileName,header=0,low_memory=False)
IP_DATA_ALL.rename(columns={'Place.Name':'Place_Name'}, inplace=True) 
IP_DATA_ALL.rename(columns={'First.IP.Number': 'First_IP_Number'}, inplace=True) 
IP_DATA_ALL.rename(columns={'Last.IP.Number': 'Last_IP_Number'}, inplace=True) 
IP_DATA_ALL.rename(columns={'Post Code': 'Post_Code'}, inplace=True)
sFileDir='D:/DSPrac/practical-data-science-master/VKHCG' + '/' + CompanyOut + '/01-Retrieve/01-EDS/02-Python'
visitordata = IP_DATA_ALL.head(10000)
print('Original Subset Data Frame') 
print('Rows :',visitordata.shape[0]) 
print('Columns :',visitordata.shape[1]) 
print(visitordata)
print('Export XML') 
sXML=df2xml(visitordata)
sFileName=sFileDir + '/' + OutputFileName 
file_out = open(sFileName, 'wb') 
file_out.write(sXML)
file_out.close()
print('Store XML:',sFileName) 
xml_data = open(sFileName).read()
unxmlrawdata=xml2df(xml_data) 
print('Raw XML Data Frame') 
print('Rows :',unxmlrawdata.shape[0]) 
print('Columns :',unxmlrawdata.shape[1]) 
print(unxmlrawdata)
unxmldata = unxmlrawdata.drop_duplicates(subset=None, keep='first', inplace=False)
print('Deduplicated XML Data Frame') 
print('Rows :',unxmldata.shape[0]) 
print('Columns :',unxmldata.shape[1]) 
print(unxmldata)
print('Done!!')
print('Nishi Jain-53004230036')

Output:

    I. Connecting to other Data Sources
Program to connect to different data sources.
    A. SQLite
import sqlite3 as sq 
import pandas as pd
Base='D:/DSPrac/practical-data-science-master/VKHCG'
sDatabaseName=Base + '/01-Vermeulen/04-Transform/SQLite/vermeulen.db' 
conn = sq.connect(sDatabaseName)
sFileName=Base + '/01-Vermeulen/01-Retrieve/01-EDS/02-Python/Retrieve_IP_DATA.csv' 
print('Loading :',sFileName) 
IP_DATA_ALL_FIX=pd.read_csv(sFileName,header=0,low_memory=False) 
IP_DATA_ALL_FIX.index.names = ['RowIDCSV']
sTable='IP_DATA_ALL'
print('Storing :',sDatabaseName,' Table:',sTable) 
IP_DATA_ALL_FIX.to_sql(sTable, conn, if_exists='replace') 
print('Loading :',sDatabaseName,'Table:',sTable) 
TestData=pd.read_sql_query("select * from IP_DATA_ALL;", conn) 
print('################')
print('Data Values') 
print(TestData) 
print('\nData Profile') 
print('\nRows :',TestData.shape[0]) 
print('Columns :',TestData.shape[1]) 
print('Done!!')
print('Nishi Jain-53004230036')

Output:


    B. MySQL:
Open MySql. 
Create a database “DataScience”
Create a python file and add the following code:
################ Connection With MySQL ######################
Code:
import mysql.connector
conn = mysql.connector.connect(host='localhost',
database='DataScience', 
user='root', 
password='root'
conn.connect
if(conn.is_connected): 
print('###### Connection With MySql Established Successfullly ##### ')
else:
print('Not Connected -- Check Connection Properites')

Output:


    C. Microsoft Excel
Code:
import pandas as pd 
Base='D:/DSPrac/practical-data-science-master/VKHCG'
sFileDir=Base + '/01-Vermeulen/01-Retrieve/01-EDS/02-Python' 
CurrencyRawData = pd.read_excel(Base + '/01-Vermeulen/00-RawData/Country_Currency.xlsx') 
sColumns = ['Country or territory', 'Currency', 'ISO-4217']
CurrencyData = CurrencyRawData[sColumns] 
CurrencyData.rename(columns={'Country or territory':'Country','ISO-4217':'CurrencyCode'},inplace=True) 
CurrencyData.dropna(subset=['Currency'],inplace=True) 
CurrencyData['Country'] = CurrencyData['Country'].map(lambda x: x.strip()) 
CurrencyData['Currency'] = CurrencyData['Currency'].map(lambda x: x.strip())
CurrencyData['CurrencyCode'] = CurrencyData['CurrencyCode'].map(lambda x: x.strip())
print(CurrencyData)
print('  Data from Excel Sheet Retrived Successfully    ')
sFileName=sFileDir + '/Retrieve-Country-Currency.csv' 
CurrencyData.to_csv(sFileName, index = False)
print('Done!!')
print('Nishi Jain-53004230036')

Output:


Practical 4
AIM: Assessing Data
Theory:
Data quality refers to the condition of a set of quantitative or qualitative variables. Dataquality is a multidimensional measurement of the acceptability of specific data sets. Inbusiness, data quality is measured to determine whether data can be used as a basis forreliable intelligence extraction for supporting organizational decisions. Data profiling involves observing in your data source all the viewpoints that the information offers. The main goal is to determine if individual viewpoints are accurate complete. The Assess superstep determines what additional processing to apply to the entries that are noncompliant.
Errors:
Typically, one of four things can be done with an error to the data 
    1) Accept the Error
    2) Reject the Error 
    3) Correct the Error
    4) Create a Default Value
    A. Program error management on the given data using pandas package
Missing Values in Pandas
1. Drop the Columns Where All Elements are Missing Values 
Code: 
import pandas as pd
sInputFileName='Good-or-Bad.csv'
sOutputFileName='Good-or-Bad-01.csv'
sFileDir='D:/DSPrac/Files/'
sFileName= sFileDir + sInputFileName
print('Loading :',sFileName)
RawData=pd.read_csv(sFileName,header=0)  
print('Raw Data Values')    
print(RawData)  
print('Data Profile') 
print('Rows :',RawData.shape[0])
print('Columns :',RawData.shape[1])
sFileName= sFileDir + sInputFileName
RawData.to_csv(sFileName, index = False)
TestData=RawData.dropna(axis=1, how='all')
print('################################')  
print('Test Data Values')   
print(TestData)  
print('Data Profile') 
print('Rows :',TestData.shape[0])
print('Columns :',TestData.shape[1])
sFileName= sFileDir + sOutputFileName
TestData.to_csv(sFileName, index = False)
print('Nishi Jain-53004230036')

Output:
 

2. Drop the Columns where any of the Elements has Missing Values 
Code:
import pandas as pd
sInputFileName='Good-or-Bad.csv'
sOutputFileName='Good-or-Bad-02.csv'
sFileDir='D:/DSPrac/Files/'
sFileName= sFileDir + sInputFileName
print('Loading :',sFileName)
RawData=pd.read_csv(sFileName,header=0)  
print('Raw Data Values')    
print(RawData)  
print('Data Profile') 
print('Rows :',RawData.shape[0])
print('Columns :',RawData.shape[1])
sFileName= sFileDir + sInputFileName
RawData.to_csv(sFileName, index = False)
TestData=RawData.dropna(axis=1, how='any')
print('################################')  
print('Test Data Values')   
print(TestData)  
print('Data Profile') 
print('Rows :',TestData.shape[0])
print('Columns :',TestData.shape[1])
sFileName= sFileDir + sOutputFileName
TestData.to_csv(sFileName, index = False)
print('Nishi Jain-53004230036')

Output:





3. Keep only the rows that contain a maximum of two Missing Values 
Code:
import pandas as pd
sInputFileName='Good-or-Bad.csv'
sOutputFileName='Good-or-Bad-03.csv'
sFileDir='D:/DSPrac/Files/'
sFileName= sFileDir + sInputFileName
print('Loading :',sFileName)
RawData=pd.read_csv(sFileName,header=0)  
print('Raw Data Values')    
print(RawData)  
print('Data Profile') 
print('Rows :',RawData.shape[0])
print('Columns :',RawData.shape[1])
sFileName= sFileDir + sInputFileName
RawData.to_csv(sFileName, index = False)
TestData=RawData.dropna(thresh=2)
print('################################')  
print('Test Data Values')   
print(TestData)  
print('Data Profile') 
print('Rows :',TestData.shape[0])
print('Columns :',TestData.shape[1])
sFileName= sFileDir + sOutputFileName
TestData.to_csv(sFileName, index = False)
print('Nishi Jain-53004230036')

Output:
 


4. Fill all Missing Values with the Mean, Median, Mode, Minimum and Maximum of the Particular Numeric Column 
Code:
import pandas as pd 
Base='D:/DSPrac/Files/'
print('AIM: Fill all the missing values with Mean Median Mode Minimum Maximum of numeric column\n  ')
print('Working Base :',Base) 
sInputFileName='Good-or-Bad.csv' 
sOutputFileNameA='Good-or-Bad-04-A.csv' #MEAN 
sOutputFileNameB='Good-or-Bad-04-B.csv' #MEDIAN 
sOutputFileNameC='Good-or-Bad-04-C.csv' #MODE 
sOutputFileNameD='Good-or-Bad-04-D.csv' #MIN 
sOutputFileNameE='Good-or-Bad-04-E.csv' #MAX 
sFileName=Base + sInputFileName 
print('Loading :',sFileName) 
RawData=pd.read_csv(sFileName,header=0)
print('Raw Data Values') 
print(RawData)    
print('Data Profile') 
print('Rows :',RawData.shape[0]) 
print('Columns :',RawData.shape[1]) 
sFileName=Base + sInputFileName 
RawData.to_csv(sFileName, index = False) 
TestData=RawData.fillna(RawData.mean())
print('     ')
print('Test Data Values') 
print(TestData)          
print('Data Profile') 
print('Rows :',TestData.shape[0]) 
print('Columns :',TestData.shape[1]) 
sFileName=Base + sOutputFileNameA 
TestData.to_csv(sFileName, index = False) 
TestData=RawData.fillna(RawData.median()) 
print('     ')
print('Test Data Values') 
print(TestData)          
print('Data Profile') 
print('Rows :',TestData.shape[0]) 
print('Columns :',TestData.shape[1]) 
sFileName=Base + sOutputFileNameB 
TestData.to_csv(sFileName, index = False) 
TestData=RawData.fillna(RawData.mode()) 
print('     ')
print('Test Data Values') 
print(TestData)          
print('Data Profile') 
print('Rows :',TestData.shape[0]) 
print('Columns:',TestData.shape[1]) 
sFileName=Base + sOutputFileNameC 
TestData.to_csv(sFileName, index = False) 
TestData=RawData.fillna(RawData.min()) 
print('     ')
print('Test Data Values') 
print(TestData)          
print('Data Profile') 
print('Rows :',TestData.shape[0]) 
print('Columns:',TestData.shape[1]) 
sFileName=Base + sOutputFileNameD 
TestData.to_csv(sFileName, index = False) 
TestData=RawData.fillna(RawData.max()) 
print('     ')
print('Test Data Values') 
print(TestData)          
print('Data Profile') 
print('Rows :',TestData.shape[0]) 
print('Columns:',TestData.shape[1]) 
sFileName=Base + sOutputFileNameE 
TestData.to_csv(sFileName, index = False) 
print('Done!!!!!')
print('Nishi Jain-53004230036')

Output:
 
   


B. Write a Python program to create the network routing diagram from the given data on routers in assess superstep.
Code:
import pandas as pd
pd.options.mode.chained_assignment = None
Base='D:/DSPrac/practical-data-science-master/VKHCG'
sInputFileName1='01-Retrieve/01-EDS/01-R/Retrieve_Country_Code.csv'
sInputFileName2='01-Retrieve/01-EDS/02-Python/Retrieve_Router_Location.csv'
sInputFileName3='01-Retrieve/01-EDS/01-R/Retrieve_IP_DATA.csv'
sOutputFileName='Assess-Network-Routing-Company.csv'
Company='01-Vermeulen'
sFileName=Base + '/' + Company + '/' + sInputFileName1
print('################################')
print('Loading :',sFileName)
print('################################')
CountryData=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1")
print('Loaded Country:',CountryData.columns.values)
print('################################')
print('Changed :',CountryData.columns.values)
CountryData.rename(columns={'Country': 'Country_Name'}, inplace=True)
CountryData.rename(columns={'ISO-2-CODE': 'Country_Code'}, inplace=True)
CountryData.drop('ISO-M49', axis=1, inplace=True)
CountryData.drop('ISO-3-Code', axis=1, inplace=True)
CountryData.drop('RowID', axis=1, inplace=True)
print('To :',CountryData.columns.values)
print('################################')
sFileName=Base + '/' + Company + '/' + sInputFileName2
print('################################')
print('Loading :',sFileName)
print('################################')
CompanyData=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1")
print('Loaded Company :',CompanyData.columns.values)
print('################################')
print('Changed :',CompanyData.columns.values)
CompanyData.rename(columns={'Country': 'Country_Code'}, inplace=True)
print('To :',CompanyData.columns.values)
print('################################')
sFileName=Base + '/' + Company + '/' + sInputFileName3
print('################################')
print('Loading :',sFileName)
print('################################')
CustomerRawData=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1")
print('################################')
print('Loaded Customer :',CustomerRawData.columns.values)
print('################################')
CustomerData=CustomerRawData.dropna(axis=0, how='any')
print('################################')
print('Remove Blank Country Code')
print('Reduce Rows from', CustomerRawData.shape[0],' to ', CustomerData.shape[0])
print('################################')
print('Changed :',CustomerData.columns.values)
CustomerData.rename(columns={'Country': 'Country_Code'}, inplace=True)
print('To :',CustomerData.columns.values)
print('################################')
print('Merge Company and Country Data')
CompanyNetworkData=pd.merge(CompanyData,CountryData,how='inner',on='Country_Code')
print('################################')
print('Change ',CompanyNetworkData.columns.values)
for i in CompanyNetworkData.columns.values:
    j='Company_'+i
CompanyNetworkData.rename(columns={i: j}, inplace=True)
print('To ', CompanyNetworkData.columns.values)
sFileDir=Base + '/' + Company + '/02-Assess/01-EDS/02-Python'
sFileName=sFileDir + '/' + sOutputFileName
print('################################')
print('Storing :', sFileName)
print('################################')
CompanyNetworkData.to_csv(sFileName, index = False, encoding="latin-1")
print('Done!!')
print('Nishi Jain-53004230036')

Output:





C. Write a Python program to build Directed Cyclic Graph
Directed Acyclic Graph (DAG)
A directed acyclic graph is a specific graph that only has one path through the graph.

    Example 1: Company location DAG  
Code:
import networkx as nx
import matplotlib.pyplot as plt
import pandas as pd
Base='D:/DSPrac/practical-data-science-master/VKHCG'
print('################################')
sInputFileName='01-Retrieve/01-EDS/02-Python/Retrieve_Router_Location.csv'
sOutputFileName1='Assess-DAG-Company-Country.png'
sOutputFileName2='Assess-DAG-Company-Country-Place.png'
Company='01-Vermeulen'
sFileName=Base + '/' + Company + '/' + sInputFileName
print('################################')
print('Loading :',sFileName)
print('################################')
CompanyData=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1")
print('Loaded Company :',CompanyData.columns.values)
print('################################')
print(CompanyData)
print('################################')
print('Rows : ',CompanyData.shape[0])
print('################################')
G1=nx.DiGraph()
G2=nx.DiGraph()
for i in range(CompanyData.shape[0]):
    G1.add_node(CompanyData['Country'][i])
    sPlaceName= CompanyData['Place_Name'][i] + '-' + CompanyData['Country'][i]
    G2.add_node(sPlaceName)
print('################################')
for n1 in G1.nodes():
    for n2 in G1.nodes():
        if n1 != n2:
            print('Link :',n1,' to ', n2)
            G1.add_edge(n1,n2)
print('################################')
print('################################')
print("Nodes of graph: ")
print(G1.nodes())
print("Edges of graph: ")
print(G1.edges())
print('################################')
sFileDir=Base + '/' + Company + '/02-Assess/01-EDS/02-Python'
sFileName=sFileDir + '/' + sOutputFileName1
print('################################')
print('Storing :', sFileName)
print('################################')
nx.draw(G1,pos=nx.spectral_layout(G1),node_color='r',edge_color='g',with_labels=True,node_size=8000,font_size=12)
plt.savefig(sFileName)
plt.show()
print('################################')
for n1 in G2.nodes():
    for n2 in G2.nodes():
        if n1 != n2:
            print('Link :',n1,' to ', n2)
            G2.add_edge(n1,n2)
print('################################')
print('################################')
print("Nodes of graph: ")
print(G2.nodes())
print("Edges of graph: ")
print(G2.edges())
print('################################')
sFileDir=Base + '/' + Company + '/02-Assess/01-EDS/02-Python'
sFileName=sFileDir + '/' + sOutputFileName2
print('################################')
print('Storing :', sFileName)
print('################################')
nx.draw(G2,pos=nx.spectral_layout(G2),node_color='r',edge_color='b',with_labels=True,node_size=8000,font_size=12)
plt.savefig(sFileName)
plt.show() 
print('Done!!')
print('Nishi Jain-53004230036')

Output:
  
Example 2: Customer location DAG  
Code:
import networkx as nx
import matplotlib.pyplot as plt
import pandas as pd
Base='D:/DSPrac/practical-data-science-master/VKHCG'
sInputFileName='01-Retrieve/01-EDS/02-Python/Retrieve_Router_Location.csv'
sOutputFileName1='Assess-DAG-Company-Country.png'
sOutputFileName2='Assess-DAG-Company-Country-Place.png'
Company='01-Vermeulen'
sFileName=Base + '/' + Company + '/' + sInputFileName
print('Loading :',sFileName)
print('################################')
CompanyData=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1")
print('Loaded Company :',CompanyData.columns.values)
print('################################')
print(CompanyData)
print('################################')
print('Rows : ',CompanyData.shape[0])
print('################################')
G1=nx.DiGraph()
G2=nx.DiGraph()
for i in range(CompanyData.shape[0]):
    G1.add_node(CompanyData['Country'][i])
    sPlaceName= CompanyData['Place_Name'][i] + '-' + CompanyData['Country'][i]
    G2.add_node(sPlaceName)
for n1 in G1.nodes():
    for n2 in G1.nodes():
        if n1 != n2:
            print('Link :',n1,' to ', n2)
            G1.add_edge(n1,n2)
print("Nodes of graph: ")
print(G1.nodes())
print("Edges of graph: ")
print(G1.edges())
sFileDir=Base + '/' + Company + '/02-Assess/01-EDS/02-Python'
sFileName=sFileDir + '/' + sOutputFileName1
print('Storing :', sFileName)
print('################################')
nx.draw(G1,pos=nx.spectral_layout(G1),node_color='r',edge_color='g',with_labels=True,node_size=8000,font_size=12)
plt.savefig(sFileName)
plt.show()
for n1 in G2.nodes():
    for n2 in G2.nodes():
        if n1 != n2:
            print('Link :',n1,' to ', n2)
            G2.add_edge(n1,n2)
print("Nodes of graph: ")
print(G2.nodes())
print("Edges of graph: ")
print(G2.edges())
sFileDir=Base + '/' + Company + '/02-Assess/01-EDS/02-Python'
sFileName=sFileDir + '/' + sOutputFileName2
print('Storing :', sFileName)
print('################################')
nx.draw(G2,pos=nx.spectral_layout(G2),node_color='r',edge_color='b',with_labels=True,node_size=8000,font_size=12)
plt.savefig(sFileName)
plt.show()
print('Done!!')
print('Nishi Jain-53004230036')

Output:
  


Example 3: DAG using GPS data 
Code:
import networkx as nx
import matplotlib.pyplot as plt
import pandas as pd
Base='D:/DSPrac/practical-data-science-master/VKHCG'
sInputFileName='01-Retrieve/01-EDS/02-Python/Retrieve_Router_Location.csv'
sOutputFileName='Assess-DAG-Company-GPS.png'
Company='01-Vermeulen'
sFileName=Base + '/' + Company + '/' + sInputFileName
print('Loading :',sFileName)
CompanyData=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1")
print('Loaded Company :',CompanyData.columns.values)
print(CompanyData)
print('Rows : ',CompanyData.shape[0])
G=nx.Graph()
for i in range(CompanyData.shape[0]):
    nLatitude=round(CompanyData['Latitude'][i],1)
    nLongitude=round(CompanyData['Longitude'][i],1)
    if nLatitude < 0:
        sLatitude = str(nLatitude*-1) + ' S'
    else:
        sLatitude = str(nLatitude) + ' N'
    if nLongitude < 0:
        sLongitude = str(nLongitude*-1) + ' W'
    else:
        sLongitude = str(nLongitude) + ' E'
    sGPS= sLatitude + '-' + sLongitude
    G.add_node(sGPS)
for n1 in G.nodes():
    for n2 in G.nodes():
        if n1 != n2:
            print('Link :',n1,' to ', n2)
            G.add_edge(n1,n2)
print("Nodes of graph: ")
print(G.nodes())
print("Edges of graph: ")
print(G.edges())
sFileDir=Base + '/' + Company + '/02-Assess/01-EDS/02-Python'
sFileName=sFileDir + '/' + sOutputFileName
print('Storing :', sFileName)
pos=nx.circular_layout(G,dim=2, scale=2)
nx.draw(G,pos=pos, node_color='r',edge_color='b',with_labels=True,node_size=4000,font_size=9)
plt.savefig(sFileName)
plt.show() 
print('Nishi Jain-53004230036')

Output:




    D. Write a Python program to pick the content for Billboards for the given data.
Code:
import sqlite3 as sq
import pandas as pd
Base='D:/DSPrac/practical-data-science-master/VKHCG'
sInputFileName1='01-Retrieve/01-EDS/02-Python/Retrieve_DE_Billboard_Locations.csv'
sInputFileName2='01-Retrieve/01-EDS/02-Python/Retrieve_Online_Visitor.csv'
sOutputFileName='Assess-DE-Billboard-Visitor.csv'
Company='02-Krennwallner'
sDataBaseDir=Base + '/' + Company + '/02-Assess/SQLite'
sDatabaseName=sDataBaseDir + '/krennwallner.db'
conn = sq.connect(sDatabaseName)
sFileName=Base + '/' + Company + '/' + sInputFileName1
print('Loading :',sFileName)
BillboardRawData=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1")
BillboardRawData.drop_duplicates(subset=None, keep='first', inplace=True)
BillboardData=BillboardRawData
print('Loaded Company :',BillboardData.columns.values) 
sTable='Assess_BillboardData'
print('Storing :',sDatabaseName,' Table:',sTable)
BillboardData.to_sql(sTable, conn, if_exists="replace")
print(BillboardData.head())
print('Rows : ',BillboardData.shape[0])
sFileName=Base + '/' + Company + '/' + sInputFileName2
print('Loading :',sFileName)
VisitorRawData=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1")
VisitorRawData.drop_duplicates(subset=None, keep='first', inplace=True)
VisitorData=VisitorRawData[VisitorRawData.Country=='DE']
print('Loaded Company :',VisitorData.columns.values)  
sTable='Assess_VisitorData'
print('Storing :',sDatabaseName,' Table:',sTable)
VisitorData.to_sql(sTable, conn, if_exists="replace")
print(VisitorData.head())
print('Rows : ',VisitorData.shape[0]) 
sTable='Assess_BillboardVisitorData'
print('Loading :',sDatabaseName,' Table:',sTable)
sSQL="select distinct"
sSQL=sSQL+ " A.Country AS BillboardCountry,"
sSQL=sSQL+ " A.Place_Name AS BillboardPlaceName,"
sSQL=sSQL+ " A.Latitude AS BillboardLatitude, "
sSQL=sSQL+ " A.Longitude AS BillboardLongitude,"
sSQL=sSQL+ " B.Country AS VisitorCountry,"
sSQL=sSQL+ " B.Place_Name AS VisitorPlaceName,"
sSQL=sSQL+ " B.Latitude AS VisitorLatitude, "
sSQL=sSQL+ " B.Longitude AS VisitorLongitude,"
sSQL=sSQL+ " (B.Last_IP_Number - B.First_IP_Number) * 365.25 * 24 * 12 AS VisitorYearRate"
sSQL=sSQL+ " from"
sSQL=sSQL+ " Assess_BillboardData as A"
sSQL=sSQL+ " JOIN "
sSQL=sSQL+ " Assess_VisitorData as B"
sSQL=sSQL+ " ON "
sSQL=sSQL+ " A.Country = B.Country"
sSQL=sSQL+ " AND "
sSQL=sSQL+ " A.Place_Name = B.Place_Name;"
BillboardVistorsData=pd.read_sql_query(sSQL, conn)  
sTable='Assess_BillboardVistorsData'
print('Storing :',sDatabaseName,' Table:',sTable)
BillboardVistorsData.to_sql(sTable, conn, if_exists="replace")
print(BillboardVistorsData.head())
print('Rows : ',BillboardVistorsData.shape[0])
sFileDir=Base + '/' + Company + '/02-Assess/01-EDS/02-Python'
print('Storing :', sFileName)
sFileName=sFileDir + '/' + sOutputFileName
BillboardVistorsData.to_csv(sFileName, index = False)
print('Done')
print('Nishi Jain-53004230036')

Output: 














    E. Write a Python/ R program to plan the locations of the warehouses from the given data.
Planning the Locations of the Warehouses

Planning the location of the warehouses requires the assessment of the GPS locations of these warehouses against the requirements for Hillman's logistics needs.
Open your editor and create a file named Assess-Warehouse-Address.py in directory

Code:
!pip install geopy --user
import pandas as pd
from geopy.geocoders import Nominatim 
geolocator = Nominatim()
InputDir='01-Retrieve/01-EDS/01-R' 
InputFileName='Retrieve_GB_Postcode_ Warehouse.csv' 
EDSDir='02-Assess/01-EDS'
OutputDir=EDSDir + '/02-Python' 
OutputFileName='Assess_GB_Warehouse_Address.csv' 
Company='03-Hillman'
Base='D:/DSPrac/practical-data-science-master/VKHCG'
sFileDir=Base +'/'+Company+'/'+ EDSDir 
sFileName=Base +'/'+Company+'/'+ InputDir + '/' + InputFileName 
print('###########')
print('Loading :',sFileName) 
Warehouse=pd.read_csv(sFileName,header=0,low_memory=False) 
Warehouse.sort_values(by='postcode', ascending= 1)
WarehouseGoodHead=Warehouse[Warehouse.latitude != 0].head(5) 
WarehouseGoodTail=Warehouse[Warehouse.latitude != 0].tail(5)
WarehouseGoodHead['Warehouse_Point']=WarehouseGoodHead.apply(lambda row: (str(row['latitude'])+','+str(row['longitude'])),axis=1)
WarehouseGoodHead['Warehouse_Address']=WarehouseGoodHead.apply(lambda row: geolocator.reverse(row['Warehouse_Point']).address,axis=1)
WarehouseGoodHead.drop('Warehouse_Point', axis=1, inplace=True) 
WarehouseGoodHead.drop('id', axis=1, inplace=True) 
WarehouseGoodHead.drop('postcode', axis=1, inplace=True)
WarehouseGoodTail['Warehouse_Point']=WarehouseGoodTail.apply(lambda row: (str(row['latitude'])+','+str(row['longitude'])),axis=1) 
WarehouseGoodTail['Warehouse_Address']=WarehouseGoodTail.apply(lambda row:
geolocator.reverse(row['Warehouse_Point']).address,axis=1)
WarehouseGoodTail.drop('Warehouse_Point', axis=1, inplace=True) 
WarehouseGoodTail.drop('id', axis=1, inplace=True) 
WarehouseGoodTail.drop('postcode', axis=1, inplace=True)
WarehouseGood=WarehouseGoodHead.append(WarehouseGoodTail, ignore_index=True) 
print(WarehouseGood)
sFileName=sFileDir + '/' + OutputFileName 
WarehouseGood.to_csv(sFileName, index= False)
print('Done!!')
print('Nishi Jain-53004230036')

Output:



























Practical 5
AIM: Processing Data
Theory:
Hubs are the containers for business keys. They are the most important facets of the data vault methodology. The more successfully one is able to identify business keys the less refining of the model will follow. 
Links stores the intersection of business keys (HUBS). Links can be considered the glue that holds the data vault model together. These tables allow for the data model to elegantly change over time because they can come and go as required by the business.
Satellites add all the color and description to the business keys (hubs) and relationships (links) in the data vault environment.  Satellites contain all the descriptive information, tracking change by start and end dates over time, to let one know the information in effect at any point in time. 
    A. Build the time hub, links and satellites.
Code:
from datetime import datetime
from datetime import timedelta
from pytz import timezone, all_timezones
import pandas as pd
import sqlite3 as sq
from pandas.io import sql
import uuid
pd.options.mode.chained_assignment = None
Base='D:/DSPrac/practical-data-science-master/VKHCG'
print('################################')
print('Working Base :',Base)
print('################################')
Company='03-Hillman'
InputDir='00-RawData'
InputFileName='VehicleData.csv'
sDataBaseDir=Base + '/'+ '01-Vermeulen/04-Transform/SQLite'
sDatabaseName=sDataBaseDir + '/Vermeulen.db'
conn1 = sq.connect(sDatabaseName)
sDataVaultDir=Base + '/88-DV'
sDatabaseName=sDataVaultDir + '/datavault.db'
conn2 = sq.connect(sDatabaseName)
base = datetime(2018,1,1,0,0,0)
numUnits=1000
date_list = [base - timedelta(hours=x) for x in range(0, numUnits)]
t=0
for i in date_list:
    now_utc=i.replace(tzinfo=timezone('UTC'))
    sDateTime=now_utc.strftime("%Y-%m-%d %H:%M:%S")
    print(sDateTime)
    sDateTimeKey=sDateTime.replace(' ','-').replace(':','-')
    t+=1
    IDNumber=str(uuid.uuid4())
    TimeLine=[('ZoneBaseKey', ['UTC']),('IDNumber', [IDNumber]),('nDateTimeValue', [now_utc]),('DateTimeValue', [sDateTime]),('DateTimeKey', [sDateTimeKey])]
    if t==1:
        TimeFrame = pd.DataFrame.from_items(TimeLine)
    else:
        TimeRow = pd.DataFrame.from_items(TimeLine)
        TimeFrame = TimeFrame.append(TimeRow)
TimeHub=TimeFrame[['IDNumber','ZoneBaseKey','DateTimeKey','DateTimeValue']]
TimeHubIndex=TimeHub.set_index(['IDNumber'],inplace=False)
TimeFrame.set_index(['IDNumber'],inplace=True)
sTable = 'Process-Time'
print('Storing :',sDatabaseName,' Table:',sTable)
TimeHubIndex.to_sql(sTable, conn1, if_exists="replace")
################################################################
sTable = 'Hub-Time'
print('Storing :',sDatabaseName,' Table:',sTable)
TimeHubIndex.to_sql(sTable, conn2, if_exists="replace")
################################################################
active_timezones=all_timezones
z=0
for zone in active_timezones:
    t=0
    for j in range(TimeFrame.shape[0]):
        now_date=TimeFrame['nDateTimeValue'][j]
        DateTimeKey=TimeFrame['DateTimeKey'][j]
        now_utc=now_date.replace(tzinfo=timezone('UTC'))
        sDateTime=now_utc.strftime("%Y-%m-%d %H:%M:%S")
        now_zone = now_utc.astimezone(timezone(zone))
        sZoneDateTime=now_zone.strftime("%Y-%m-%d %H:%M:%S")
        print(sZoneDateTime)
        t+=1
        z+=1
        IDZoneNumber=str(uuid.uuid4())
        TimeZoneLine=[('ZoneBaseKey', ['UTC']),
                  ('IDZoneNumber', [IDZoneNumber]),
                  ('DateTimeKey', [DateTimeKey]),
                  ('UTCDateTimeValue', [sDateTime]),
                  ('Zone', [zone]),
                  ('DateTimeValue', [sZoneDateTime])]
        if t==1:
            TimeZoneFrame = pd.DataFrame.from_items(TimeLine)
        else:
            TimeZoneRow = pd.DataFrame.from_items(TimeLine)
            TimeZoneFrame = TimeZoneFrame.append(TimeRow)
            TimeZoneFrameIndex=TimeZoneFrame.set_index(['IDZoneNumber'],inplace=False)
            sZone=zone.replace('/','-').replace(' ','')
#############################################################
sTable = 'Process-Time-'+sZone
print('Storing :',sDatabaseName,' Table:',sTable)
TimeZoneFrameIndex.to_sql(sTable, conn1, if_exists="replace")
#################################################################
#############################################################
sTable = 'Satellite-Time-'+sZone
print('Storing :',sDatabaseName,' Table:',sTable)
TimeZoneFrameIndex.to_sql(sTable, conn2, if_exists="replace")
#################################################################
print('################')
print('Vacuum Databases')
sSQL="VACUUM;"
sql.execute(sSQL,conn1)
sql.execute(sSQL,conn2)
print('Done!!!!!')

Output:



    B. Golden Nominal.
A golden nominal record is a single person’s record, with distinctive references for use by all systems. This gives the system a single view of the person. I use first name, other names, last name, and birth date as my golden nominal. The data we have in the assess directory requires a birth date to become a golden nominal. The program will generate a golden nominal using our sample data set
Code: 
import sqlite3 as sq
import pandas as pd
from pandas.io import sql
from datetime import datetime, timedelta
from pytz import timezone, all_timezones
from random import randint
import uuid
Base='D:/DSPrac/practical-data-science-master/VKHCG'
print('Working Base :',Base)
Company='04-Clark'
sInputFileName='02-Assess/01-EDS/02-Python/Assess_People.csv'
sDataBaseDir=Base + '/' + Company + '/05-Organise/SQLite'
sDatabaseName=sDataBaseDir + '/clark.db'
conn1 = sq.connect(sDatabaseName)
sDataVaultDir=Base + '/88-DV'
sDatabaseName=sDataVaultDir + '/datavault.db'
conn2 = sq.connect(sDatabaseName)
### Import Female Data
sFileName=Base + '/' + Company + '/' + sInputFileName
print('Loading :',sFileName)
print('################################')
print(sFileName)
RawData=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1")
RawData.drop_duplicates(subset=None, keep='first', inplace=True)
start_date = datetime(1900,1,1,0,0,0)
start_date_utc=start_date.replace(tzinfo=timezone('UTC'))
HoursBirth=100*365*24
RawData['BirthDateUTC']=RawData.apply(lambda row:
        (start_date_utc + timedelta(hours=randint(0, HoursBirth)))
        ,axis=1)
zonemax=len(all_timezones)-1
RawData['TimeZone']=RawData.apply(lambda row:
        (all_timezones[randint(0, zonemax)])
        ,axis=1)
RawData['BirthDateISO']=RawData.apply(lambda row:
        row["BirthDateUTC"].astimezone(timezone(row['TimeZone']))
        ,axis=1)
RawData['BirthDateKey']=RawData.apply(lambda row:
        row["BirthDateUTC"].strftime("%Y-%m-%d %H:%M:%S")
        ,axis=1)
RawData['BirthDate']=RawData.apply(lambda row:
        row["BirthDateISO"].strftime("%Y-%m-%d %H:%M:%S")
        ,axis=1)
RawData['PersonID']=RawData.apply(lambda row:
        str(uuid.uuid4())
        ,axis=1)
Data=RawData.copy()
Data.drop('BirthDateUTC', axis=1,inplace=True)
Data.drop('BirthDateISO', axis=1,inplace=True)
indexed_data = Data.set_index(['PersonID'])
print('################################')
sTable='Process_Person'
print('Storing :',sDatabaseName,' Table:',sTable)
indexed_data.to_sql(sTable, conn1, if_exists="replace")
PersonHubRaw=Data[['PersonID','FirstName','SecondName','LastName','BirthDateKey']]
PersonHubRaw['PersonHubID']=RawData.apply(lambda row:
        str(uuid.uuid4())
        ,axis=1)
PersonHub=PersonHubRaw.drop_duplicates(subset=None, \
                                       keep='first',\
                                       inplace=False)
indexed_PersonHub = PersonHub.set_index(['PersonHubID'])
sTable = 'Hub-Person'
print('Storing :',sDatabaseName,' Table:',sTable)
indexed_PersonHub.to_sql(sTable, conn2, if_exists="replace")
PersonSatelliteGenderRaw=Data[['PersonID','FirstName','SecondName','LastName'\
                               ,'BirthDateKey','Gender']]
PersonSatelliteGenderRaw['PersonSatelliteID']=RawData.apply(lambda row:
    str(uuid.uuid4())
    ,axis=1)
PersonSatelliteGender=PersonSatelliteGenderRaw.drop_duplicates(subset=None, \
                                                               keep='first', \
                                                               inplace=False)
indexed_PersonSatelliteGender = PersonSatelliteGender.set_index(['PersonSatelliteID'])
sTable = 'Satellite-Person-Gender'
print('Storing :',sDatabaseName,' Table:',sTable)
indexed_PersonSatelliteGender.to_sql(sTable, conn2, if_exists="replace")
################################################################
PersonSatelliteBirthdayRaw=Data[['PersonID','FirstName','SecondName','LastName',\
                                 'BirthDateKey','TimeZone','BirthDate']]
PersonSatelliteBirthdayRaw['PersonSatelliteID']=RawData.apply(lambda row:
    str(uuid.uuid4())
    ,axis=1)
PersonSatelliteBirthday=PersonSatelliteBirthdayRaw.drop_duplicates(subset=None, \
                                                                   keep='first',\
                                                                   inplace=False)
indexed_PersonSatelliteBirthday = PersonSatelliteBirthday.set_index(['PersonSatelliteID'])
sTable = 'Satellite-Person-Names'
print('Storing :',sDatabaseName,' Table:',sTable)
indexed_PersonSatelliteBirthday.to_sql(sTable, conn2, if_exists="replace")
################################################################
sFileDir=Base + '/' + Company + '/03-Process/01-EDS/02-Python'
sOutputFileName = sTable + '.csv'
sFileName=sFileDir + '/' + sOutputFileName
print('Storing :', sFileName)
print('################################')
RawData.to_csv(sFileName, index = False)
print('Vacuum Databases')
sSQL="VACUUM;"
sql.execute(sSQL,conn1)
sql.execute(sSQL,conn2)
print('Done!!')
print('Nishi Jain-53004230036')

Output:




    C. Vehicles as object hub and satellite using python and SQLite.
Vehicles
The international classification of vehicles is a complex process. There are standards, but these are not universally applied or similar between groups or countries
Code: 
import pandas as pd
import sqlite3 as sq
from pandas.io import sql
import uuid
pd.options.mode.chained_assignment = None
Base='D:/DSPrac/practical-data-science-master/VKHCG'
print('Working Base :',Base)
print('################################')
Company='03-Hillman'
InputDir='00-RawData'
InputFileName='VehicleData.csv'
sDataBaseDir=Base + '/'+ '01-Vermeulen/04-Transform/SQLite'
sDatabaseName=sDataBaseDir + '/Vermeulen.db'
conn1 = sq.connect(sDatabaseName)
sDataVaultDir=Base + '/88-DV'
sDatabaseName=sDataVaultDir + '/datavault.db'
conn2 = sq.connect(sDatabaseName)
sFileName=Base + '/' + Company + '/' + InputDir + '/' + InputFileName
print('Loading :',sFileName)
VehicleRaw=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1")
sTable='Process_Vehicles'
print('Storing :',sDatabaseName,' Table:',sTable)
VehicleRaw.to_sql(sTable, conn1, if_exists="replace")
VehicleRawKey=VehicleRaw[['Make','Model']].copy()
VehicleKey=VehicleRawKey.drop_duplicates()
VehicleKey['ObjectKey']=VehicleKey.apply(lambda row:
str('('+ str(row['Make']).strip().replace(' ', '-').replace('/', '-').lower() +
')-(' + (str(row['Model']).strip().replace(' ', '-').replace(' ', '-').lower())
+')')
,axis=1)
VehicleKey['ObjectType']=VehicleKey.apply(lambda row:
'vehicle'
,axis=1)
VehicleKey['ObjectUUID']=VehicleKey.apply(lambda row:
str(uuid.uuid4())
,axis=1)
#Vehicle Hub
VehicleHub=VehicleKey[['ObjectType','ObjectKey','ObjectUUID']].copy()
VehicleHub.index.name='ObjectHubID'
sTable = 'Hub-Object-Vehicle'
print('Storing :',sDatabaseName,' Table:',sTable)
VehicleHub.to_sql(sTable, conn2, if_exists="replace")
#Vehicle Satellite
VehicleSatellite=VehicleKey[['ObjectType','ObjectKey','ObjectUUID','Make','Model']].copy()
VehicleSatellite.index.name='ObjectSatelliteID'
sTable = 'Satellite-Object-Make-Model'
print('Storing :',sDatabaseName,' Table:',sTable)
VehicleSatellite.to_sql(sTable, conn2, if_exists="replace")
#Vehicle Dimension
sView='Dim-Object'
print('Storing :',sDatabaseName,' View:',sView)
sSQL="CREATE VIEW IF NOT EXISTS [" + sView + "] AS"
sSQL=sSQL+ " SELECT DISTINCT"
sSQL=sSQL+ " H.ObjectType,"
sSQL=sSQL+ " H.ObjectKey AS VehicleKey,"
sSQL=sSQL+ " TRIM(S.Make) AS VehicleMake,"
sSQL=sSQL+ " TRIM(S.Model) AS VehicleModel"
sSQL=sSQL+ " FROM"
sSQL=sSQL+ " [Hub-Object-Vehicle] AS H"
sSQL=sSQL+ " JOIN"
sSQL=sSQL+ " [Satellite-Object-Make-Model] AS S"
sSQL=sSQL+ " ON"
sSQL=sSQL+ " H.ObjectType=S.ObjectType"
sSQL=sSQL+ " AND"
sSQL=sSQL+ " H.ObjectUUID=S.ObjectUUID;"
sql.execute(sSQL,conn2)
print('Loading :',sDatabaseName,' Table:',sView)
sSQL=" SELECT DISTINCT"
sSQL=sSQL+ " VehicleMake,"
sSQL=sSQL+ " VehicleModel"
sSQL=sSQL+ " FROM"
sSQL=sSQL+ " [" + sView + "]"
sSQL=sSQL+ " ORDER BY"
sSQL=sSQL+ " VehicleMake"
sSQL=sSQL+ " AND"
sSQL=sSQL+ " VehicleMake;"
DimObjectData=pd.read_sql_query(sSQL, conn2)
DimObjectData.index.name='ObjectDimID'
DimObjectData.sort_values(['VehicleMake','VehicleModel'],inplace=True, ascending=True)
print(DimObjectData)
print('################')
print('Vacuum Databases')
sSQL="VACUUM;"
sql.execute(sSQL,conn1)
sql.execute(sSQL,conn2)
print('################')
conn1.close()
conn2.close()
print('Done!!')
print('Nishi Jain-53004230036')

Output:



    D. Human- Environment interaction.
The interaction of humans with their environment is a major relationship that guides people’s behavior and the characteristics of the location. Activities such as mining and other industries, roads, and landscaping at a location create both positive and negative effects on the environment, but also on humans. A location earmarked as a green belt, to assist in reducing the carbon footprint, or a new interstate change its current and future characteristics. The location is a main data source for the data science, and, normally, we find unknown or unexpected effects on the data insights.
Code: 
import pandas as pd
import sqlite3 as sq
from pandas.io import sql
import uuid
Base='D:/DSPrac/practical-data-science-master/VKHCG'
print('Working Base :',Base)
print('################################')
Company='01-Vermeulen'
InputAssessGraphName='Assess_All_Animals.gml'
EDSAssessDir='03-Hillman/02-Assess/01-EDS'
InputAssessDir=EDSAssessDir + '/02-Python'
sFileAssessDir=Base + '/' + Company + '/' + InputAssessDir
sDataBaseDir=Base + '/' + Company + '/04-Transform/SQLite'
sDatabaseName=sDataBaseDir + '/Vermeulen.db'
conn1 = sq.connect(sDatabaseName)
sDataVaultDir=Base + '/88-DV'
sDatabaseName=sDataVaultDir + '/datavault.db'
conn2 = sq.connect(sDatabaseName)
t=0
tMax=360*180
for Longitude in range(-180,180,1):
 for Latitude in range(-90,90,1):
     t+=1
     IDNumber=str(uuid.uuid4())
     LocationName='L'+format(round(Longitude,3)*1000, '+07d') +\
     '-'+format(round(Latitude,3)*1000, '+07d')
     print('Create:',t,' of ',tMax,':',LocationName)
     LocationLine=[('ObjectBaseKey', ['GPS']),
                   ('IDNumber', [IDNumber]),
                   ('LocationNumber', [str(t)]),
                   ('LocationName', [LocationName]),
                   ('Longitude', [Longitude]),
                   ('Latitude', [Latitude])]
 if t==1:
     LocationFrame = pd.DataFrame.from_items(LocationLine)
 else:
     LocationRow = pd.DataFrame.from_items(LocationLine)
     LocationFrame = LocationFrame.append(LocationRow)
LocationHubIndex=LocationFrame.set_index(['IDNumber'],inplace=False)
sTable = 'Process-Location'
print('Storing :',sDatabaseName,' Table:',sTable)
LocationHubIndex.to_sql(sTable, conn1, if_exists="replace")
sTable = 'Hub-Location'
print('Storing :',sDatabaseName,' Table:',sTable)
LocationHubIndex.to_sql(sTable, conn2, if_exists="replace")
print('################')
print('Vacuum Databases')
sSQL="VACUUM;"
sql.execute(sSQL,conn1)
sql.execute(sSQL,conn2)
print('Done!!')
print('Nishi Jain-53004230036')

Output:














Practical 6
AIM: Transforming Data
Theory:
The Transform superstep allows you, as a data scientist, to take data from the data vault and formulate answers to questions raised by your investigations. The transformation step is the data science process that converts results into insights. It takes standard data science techniques and methods to attain insight and knowledge about the data that then can be transformed into actionable decisions, which, through storytelling, you can explain to non-data scientists what you have discovered in the data lake.
A. Program to show the details of hub: person being born.
Code: 
from datetime import datetime
from pytz import timezone
import pandas as pd
import sqlite3 as sq
import uuid
pd.options.mode.chained_assignment = None
Base='D:/DSPrac/practical-data-science-master/VKHCG'
print('################################')
print('################################')
Company='01-Vermeulen'
InputDir='00-RawData'
InputFileName='VehicleData.csv'
sDataBaseDir=Base + '/' + Company + '/04-Transform/SQLite'
sDatabaseName=sDataBaseDir + '/Vermeulen.db'
conn1 = sq.connect(sDatabaseName)
sDataVaultDir=Base + '/88-DV'
sDatabaseName=sDataVaultDir + '/datavault.db'
conn2 = sq.connect(sDatabaseName)
sDataWarehouseDir=Base + '/99-DW'
sDatabaseName=sDataWarehouseDir + '/datawarehouse.db'
conn3 = sq.connect(sDatabaseName)
print('\n#################################')
print('Time Category')
print('UTC Time')
BirthDateUTC = datetime(1960,12,20,10,15,0)
BirthDateZoneUTC=BirthDateUTC.replace(tzinfo=timezone('UTC'))
BirthDateZoneStr=BirthDateZoneUTC.strftime("%Y-%m-%d %H:%M:%S")
BirthDateZoneUTCStr=BirthDateZoneUTC.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)")
print(BirthDateZoneUTCStr)
print('#################################')
print('Birth Date in Reykjavik :')
BirthZone = 'Atlantic/Reykjavik'
BirthDate = BirthDateZoneUTC.astimezone(timezone(BirthZone))
BirthDateStr=BirthDate.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)")
BirthDateLocal=BirthDate.strftime("%Y-%m-%d %H:%M:%S")
print(BirthDateStr)
print('#################################')
IDZoneNumber=str(uuid.uuid4())
sDateTimeKey=BirthDateZoneStr.replace(' ','-').replace(':','-')
TimeLine=[('ZoneBaseKey', ['UTC']),
          ('IDNumber', [IDZoneNumber]),
          ('DateTimeKey', [sDateTimeKey]),
          ('UTCDateTimeValue', [BirthDateZoneUTC]),
          ('Zone', [BirthZone]),
          ('DateTimeValue', [BirthDateStr])]
TimeFrame = pd.DataFrame.from_items(TimeLine)
TimeHub=TimeFrame[['IDNumber','ZoneBaseKey','DateTimeKey','DateTimeValue']]
TimeHubIndex=TimeHub.set_index(['IDNumber'],inplace=False)
sTable = 'Hub-Time-Gunnarsson'
print('\n#################################')
print('Storing :',sDatabaseName,'\n Table:',sTable)
print('\n#################################')
TimeHubIndex.to_sql(sTable, conn2, if_exists="replace")
sTable = 'Dim-Time-Gunnarsson'
TimeHubIndex.to_sql(sTable, conn3, if_exists="replace")
TimeSatellite=TimeFrame[['IDNumber','DateTimeKey','Zone','DateTimeValue']]
TimeSatelliteIndex=TimeSatellite.set_index(['IDNumber'],inplace=False)
BirthZoneFix=BirthZone.replace(' ','-').replace('/','-')
sTable = 'Satellite-Time-' + BirthZoneFix + '-Gunnarsson'
print('\n#################################')
print('Storing :',sDatabaseName,'\n Table:',sTable)
print('\n#################################')                              
TimeSatelliteIndex.to_sql(sTable, conn2, if_exists="replace")
sTable = 'Dim-Time-' + BirthZoneFix + '-Gunnarsson'
TimeSatelliteIndex.to_sql(sTable, conn3, if_exists="replace")
print('\n#################################')
print('Person Category')
FirstName = 'Guðmundur'
LastName = 'Gunnarsson'
print('Name:',FirstName,LastName)
print('Birth Date:',BirthDateLocal)
print('Birth Zone:',BirthZone)
print('UTC Birth Date:',BirthDateZoneStr)
print('#################################')
IDPersonNumber=str(uuid.uuid4())
PersonLine=[('IDNumber', [IDPersonNumber]),
            ('FirstName', [FirstName]),
            ('LastName', [LastName]),
            ('Zone', ['UTC']),
            ('DateTimeValue', [BirthDateZoneStr])]
PersonFrame = pd.DataFrame.from_items(PersonLine)
TimeHub=PersonFrame
TimeHubIndex=TimeHub.set_index(['IDNumber'],inplace=False)
sTable = 'Hub-Person-Gunnarsson'
print('\n#################################')
print('Storing :',sDatabaseName,'\n Table:',sTable)
print('\n#################################')
TimeHubIndex.to_sql(sTable, conn2, if_exists="replace")
sTable = 'Dim-Person-Gunnarsson'
TimeHubIndex.to_sql(sTable, conn3, if_exists="replace")

Output:


    B. Building a dimension person, time and fact PersonBornAtTime.
Code: 
from datetime import datetime
from pytz import timezone
import pandas as pd
import sqlite3 as sq
import uuid
pd.options.mode.chained_assignment = None
Base='D:/DSPrac/practical-data-science-master/VKHCG'
print('################################')
print('Working Base :', Base)
print('################################')
Company='01-Vermeulen'
sDataBaseDir=Base + '/' + Company + '/04-Transform/SQLite'
sDatabaseName=sDataBaseDir + '/Vermeulen.db'
conn1 = sq.connect(sDatabaseName)
sDataWarehousetDir=Base + '/99-DW'
sDatabaseName=sDataWarehousetDir + '/datawarehouse.db'
conn2 = sq.connect(sDatabaseName)
print('\n#################################')
print('Time Dimension')
BirthZone = 'Atlantic/Reykjavik'
BirthDateUTC = datetime(1960,12,20,10,15,0)
BirthDateZoneUTC=BirthDateUTC.replace(tzinfo=timezone('UTC')) 
BirthDateZoneStr=BirthDateZoneUTC.strftime("%Y-%m-%d %H:%M:%S") 
BirthDateZoneUTCStr=BirthDateZoneUTC.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)")
BirthDate = BirthDateZoneUTC.astimezone(timezone(BirthZone)) 
BirthDateStr=BirthDate.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)")
BirthDateLocal=BirthDate.strftime("%Y-%m-%d %H:%M:%S")
IDTimeNumber=str(uuid.uuid4()) 
TimeLine=[('TimeID', [IDTimeNumber]),
          ('UTCDate', [BirthDateZoneStr]),
          ('LocalTime', [BirthDateLocal]),
          ('TimeZone', [BirthZone])] 
TimeFrame = pd.DataFrame.from_items(TimeLine) 
DimTime=TimeFrame
DimTimeIndex=DimTime.set_index(['TimeID'],inplace=False)
sTable = 'Dim-Time'
print('\n#################################')
print('Storing :',sDatabaseName,'\n Table:',sTable)
print('\n#################################')
DimTimeIndex.to_sql(sTable, conn1, if_exists="replace")
DimTimeIndex.to_sql(sTable, conn2, if_exists="replace")
print('\n#################################')
print('Dimension Person')
print('\n#################################')
FirstName = 'Guðmundur'
LastName = 'Gunnarsson'
IDPersonNumber=str(uuid.uuid4()) 
PersonLine=[('PersonID', [IDPersonNumber]),
              ('FirstName', [FirstName]),
              ('LastName', [LastName]),
              ('Zone', ['UTC']),
              ('DateTimeValue', [BirthDateZoneStr])] 
PersonFrame = pd.DataFrame.from_items(PersonLine) 
DimPerson=PersonFrame
DimPersonIndex=DimPerson.set_index(['PersonID'],inplace=False)
sTable = 'Dim-Person'
print('\n#################################')
print('Storing :',sDatabaseName,'\n Table:',sTable)
print('\n#################################')
DimPersonIndex.to_sql(sTable, conn1, if_exists="replace")
DimPersonIndex.to_sql(sTable, conn2, if_exists="replace")
print('\n#################################')
print('Fact - Person - time')
print('\n#################################')
IDFactNumber=str(uuid.uuid4()) 
PersonTimeLine=[('IDNumber', [IDFactNumber]),
                ('IDPersonNumber', [IDPersonNumber]),
                ('IDTimeNumber', [IDTimeNumber])] 
PersonTimeFrame = pd.DataFrame.from_items(PersonTimeLine) 
FctPersonTime=PersonTimeFrame
FctPersonTimeIndex=FctPersonTime.set_index(['IDNumber'],inplace=False)
sTable = 'Fact-Person-Time'
print('\n#################################')
print('Storing :',sDatabaseName,'\n Table:',sTable)
print('\n#################################')
FctPersonTimeIndex.to_sql(sTable, conn1, if_exists="replace")
FctPersonTimeIndex.to_sql(sTable, conn2, if_exists="replace")

Output:


    C. Building a data warehouse for transform superstep.
Code: 
from datetime import datetime
from pytz import timezone
import pandas as pd
import sqlite3 as sq
import uuid
pd.options.mode.chained_assignment = None
Base='D:/DSPrac/practical-data-science-master/VKHCG'
print('################################')
print('Working Base :',Base, ' using ', sys.platform)
print('################################')
Company='01-Vermeulen'
sDataBaseDir=Base + '/' + Company + '/04-Transform/SQLite'
sDatabaseName=sDataBaseDir + '/Vermeulen.db'
conn1 = sq.connect(sDatabaseName)
sDataVaultDir=Base + '/88-DV'
sDatabaseName=sDataVaultDir + '/datavault.db'
conn2 = sq.connect(sDatabaseName)
sDataWarehouseDir=Base + '/99-DW'
sDatabaseName=sDataWarehouseDir + '/datawarehouse.db'
conn3 = sq.connect(sDatabaseName)
sSQL=" SELECT DateTimeValue FROM [Hub-Time];"
DateDataRaw=pd.read_sql_query(sSQL, conn2)
DateData=DateDataRaw.head(1000)
print(DateData)
print('\n#################################')
print('Time Dimension')
print('\n#################################')
t=0
mt=DateData.shape[0]
for i in range(mt):
    BirthZone = ('Atlantic/Reykjavik','Europe/London','UCT')
    for j in range(len(BirthZone)):
        t+=1
        print(t,mt*3)
        BirthDateUTC = datetime.strptime(DateData['DateTimeValue'][i],"%Y-%m-%d %H:%M:%S")
        BirthDateZoneUTC=BirthDateUTC.replace(tzinfo=timezone('UTC'))
        BirthDateZoneStr=BirthDateZoneUTC.strftime("%Y-%m-%d %H:%M:%S")
        BirthDateZoneUTCStr=BirthDateZoneUTC.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)")
        BirthDate = BirthDateZoneUTC.astimezone(timezone(BirthZone[j]))
        BirthDateStr=BirthDate.strftime("%Y-%m-%d %H:%M:%S (%Z) (%z)")
        BirthDateLocal=BirthDate.strftime("%Y-%m-%d %H:%M:%S")
        IDTimeNumber=str(uuid.uuid4())
        TimeLine=[('TimeID', [str(IDTimeNumber)]),
                  ('UTCDate', [str(BirthDateZoneStr)]),
                  ('LocalTime', [str(BirthDateLocal)]),
                  ('TimeZone', [str(BirthZone)])]
        if t==1:
            TimeFrame = pd.DataFrame.from_items(TimeLine)
        else:
            TimeRow = pd.DataFrame.from_items(TimeLine)
            TimeFrame=TimeFrame.append(TimeRow)
DimTime=TimeFrame
DimTimeIndex=DimTime.set_index(['TimeID'],inplace=False)
sTable = 'Dim-Time'
print('\n#################################')
print('Storing :',sDatabaseName,'\n Table:',sTable)
print('\n#################################')
DimTimeIndex.to_sql(sTable, conn1, if_exists="replace")
DimTimeIndex.to_sql(sTable, conn3, if_exists="replace")
sSQL=" SELECT " + \
    " FirstName," + \
    " SecondName," + \
    " LastName," + \
    " BirthDateKey " + \
    " FROM [Hub-Person];"
PersonDataRaw=pd.read_sql_query(sSQL, conn2)
PersonData=PersonDataRaw.head(1000)
print('\n#################################')
print('Dimension Person')
print('\n#################################')
t=0
mt=DateData.shape[0]
for i in range(mt):
    t+=1
    print(t,mt)
    FirstName = str(PersonData["FirstName"])
    SecondName = str(PersonData["SecondName"])
    if len(SecondName) > 0:
        SecondName=""
    LastName = str(PersonData["LastName"])
    BirthDateKey = str(PersonData["BirthDateKey"])
    IDPersonNumber=str(uuid.uuid4())
    PersonLine=[('PersonID', [str(IDPersonNumber)]),
                ('FirstName', [FirstName]),
                ('SecondName', [SecondName]),
                ('LastName', [LastName]),
                ('Zone', [str('UTC')]),
                ('BirthDate', [BirthDateKey])]
    if t==1:
        PersonFrame = pd.DataFrame.from_items(PersonLine)
    else:
        PersonRow = pd.DataFrame.from_items(PersonLine)
        PersonFrame = PersonFrame.append(PersonRow)
DimPerson=PersonFrame
print(DimPerson)
DimPersonIndex=DimPerson.set_index(['PersonID'],inplace=False)
sTable = 'Dim-Person'
print('\n#################################')
print('Storing :',sDatabaseName,'\n Table:',sTable)
print('\n#################################')
DimPersonIndex.to_sql(sTable, conn1, if_exists="replace")
DimPersonIndex.to_sql(sTable, conn3, if_exists="replace")

Output:


    D. Write python program to demonstrate Simple Linear Regression.
Simple Linear Regression
Linear regression is used if there is a relationship or significant association between the
variables. This can be checked by scatterplots. If no linear association appears between
the variables, fitting a linear regression model to the data will not provide a useful model.
A linear regression line has equations in the following form:
Y = a + bX,
Where, X = explanatory variable and
Y = dependent variable
b = slope of the line
a = intercept (the value of y when x = 0)
Code: 
import pandas as pd
import sqlite3 as sq
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score
Base='D:/DSPrac/practical-data-science-master/VKHCG'
print('################################')
print('Working Base :',Base, ' using ', sys.platform)
print('################################')
Company='01-Vermeulen'
sDataBaseDir=Base + '/' + Company + '/04-Transform/SQLite'
sDatabaseName=sDataBaseDir + '/Vermeulen.db'
conn1 = sq.connect(sDatabaseName)
sDataVaultDir=Base + '/88-DV'
sDatabaseName=sDataVaultDir + '/datavault.db'
conn2 = sq.connect(sDatabaseName)
sDataWarehouseDir=Base + '/99-DW'
sDatabaseName=sDataWarehouseDir + '/datawarehouse.db'
conn3 = sq.connect(sDatabaseName)
t=0
tMax=((300-100)/10)*((300-30)/5)
for heightSelect in range(100,300,10):
    for weightSelect in range(30,300,5):
        height = round(heightSelect/100,3)
        weight = int(weightSelect)
        bmi = weight/(height*height)
        if bmi <= 18.5:
            BMI_Result=1
        elif bmi > 18.5 and bmi < 25:
            BMI_Result=2
        elif bmi > 25 and bmi < 30:
            BMI_Result=3
        elif bmi > 30:
            BMI_Result=4
        else:
            BMI_Result=0
        PersonLine=[('PersonID', [str(t)]),
                    ('Height', [height]),
                    ('Weight', [weight]),
                    ('bmi', [bmi]),
                    ('Indicator', [BMI_Result])]
        t+=1
        print('Row:',t,'of',tMax)
        if t==1:
            PersonFrame = pd.DataFrame.from_items(PersonLine)
        else:
            PersonRow = pd.DataFrame.from_items(PersonLine)
            PersonFrame = PersonFrame.append(PersonRow)
DimPerson=PersonFrame
DimPersonIndex=DimPerson.set_index(['PersonID'],inplace=False)
sTable = 'Transform-BMI'
print('\n#################################')
print('Storing :',sDatabaseName,'\n Table:',sTable)
print('\n#################################')
DimPersonIndex.to_sql(sTable, conn1, if_exists="replace")
sTable = 'Person-Satellite-BMI'
print('\n#################################')
print('Storing :',sDatabaseName,'\n Table:',sTable)
print('\n#################################')
DimPersonIndex.to_sql(sTable, conn2, if_exists="replace")
sTable = 'Dim-BMI'
print('\n#################################')
print('Storing :',sDatabaseName,'\n Table:',sTable)
print('\n#################################')
DimPersonIndex.to_sql(sTable, conn3, if_exists="replace")
fig = plt.figure()
PlotPerson=DimPerson[DimPerson['Indicator']==1]
x=PlotPerson['Height']
y=PlotPerson['Weight']
plt.plot(x, y, ".")
PlotPerson=DimPerson[DimPerson['Indicator']==2]
x=PlotPerson['Height']
y=PlotPerson['Weight']
plt.plot(x, y, "o")
PlotPerson=DimPerson[DimPerson['Indicator']==3]
x=PlotPerson['Height']
y=PlotPerson['Weight']
plt.plot(x, y, "+")
PlotPerson=DimPerson[DimPerson['Indicator']==4]
x=PlotPerson['Height']
y=PlotPerson['Weight']
plt.plot(x, y, "^")
plt.axis('tight')
plt.title("BMI Curve")
plt.xlabel("Height(meters)")
plt.ylabel("Weight(kg)")
plt.plot()
 # Load the diabetes dataset
diabetes = datasets.load_diabetes()
# Use only one feature
diabetes_X = diabetes.data[:, np.newaxis, 2]
diabetes_X_train = diabetes_X[:-30]
diabetes_X_test = diabetes_X[-50:]
diabetes_y_train = diabetes.target[:-30]
diabetes_y_test = diabetes.target[-50:]
regr = linear_model.LinearRegression()
regr.fit(diabetes_X_train, diabetes_y_train)
diabetes_y_pred = regr.predict(diabetes_X_test)
print('Coefficients: \n', regr.coef_)
print("Mean squared error: %.2f"% mean_squared_error(diabetes_y_test, diabetes_y_pred))
print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))
plt.scatter(diabetes_X_test, diabetes_y_test, color='black')
plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)
plt.xticks(())
plt.yticks(())
plt.axis('tight')
plt.title("Diabetes")
plt.xlabel("BMI")
plt.ylabel("Age")
plt.show()

Output:


Practical 7
AIM: Organizing Data
Theory:
Organize Superstep
The Organize superstep takes the complete data warehouse you built at the end of the Transform superstep and subsections it into business-specific data marts. A data mart is the access layer of the data warehouse environment built to expose data to the users. The data mart is a subset of the data warehouse and is generally oriented to a specific business group.
Horizontal Style
Performing horizontal-style slicing or subsetting of the data warehouse is achieved by applying a filter
technique that forces the data warehouse to show only the data for a specific preselected set of filtered
outcomes against the data population. The horizontal-style slicing selects the subset of rows from the
population while preserving the columns. That is, the data science tool can see the complete record for the
records in the subset of records
    A. Write Python program to perform the horizontal style subset of slice of the data warehouse data.
Code: 
import pandas as pd
import sqlite3 as sq
sDatabaseName='D:/DSPrac/practical-data-science-master/VKHCG/99-DW/datawarehouse.db'
conn1 = sq.connect(sDatabaseName)
sDatabaseName='D:/DSPrac/practical-data-science-master/VKHCG/99-DW/datamart.db'
conn2 = sq.connect(sDatabaseName) 
sTable = 'Dim-BMI'
print('Loading :',sDatabaseName,' Table:',sTable)
sSQL="SELECT * FROM [Dim-BMI];"
PersonFrame0=pd.read_sql_query(sSQL, conn1)
sTable = 'Dim-BMI'
print('Loading :',sDatabaseName,' Table:',sTable)
sSQL="SELECT PersonID,\
       Height,\
       Weight,\
       bmi,\
       Indicator\
  FROM [Dim-BMI]\
  WHERE \
  Height > 1.5 \
  and Indicator = 1\
  ORDER BY  \
       Height,\
       Weight;"
PersonFrame1=pd.read_sql_query(sSQL, conn1)
DimPerson=PersonFrame1
DimPersonIndex=DimPerson.set_index(['PersonID'],inplace=False)
sTable = 'Dim-BMI-Horizontal'
print('Storing :',sDatabaseName,'\n Table:',sTable)
DimPersonIndex.to_sql(sTable, conn2, if_exists="replace")
sTable = 'Dim-BMI-Horizontal'
print('Loading :',sDatabaseName,' Table:',sTable)
print('################################')
sSQL="SELECT * FROM [Dim-BMI];"
PersonFrame2=pd.read_sql_query(sSQL, conn2)
print('Full Data Set (Rows):', PersonFrame0.shape[0])
print('Full Data Set (Columns):', PersonFrame0.shape[1])
print('################################')
print('Horizontal Data Set (Rows):', PersonFrame2.shape[0])
print('Horizontal Data Set (Columns):', PersonFrame2.shape[1])
print('################################')
print('Nishi Jain-53004230036')
    
Output:



    B. Write Python program to perform the vertical style subset of slice of the data warehouse data.
Vertical Style
Performing vertical-style slicing or subsetting of the data warehouse is achieved by applying a filter technique that forces the data warehouse to show only the data for specific preselected filtered outcomes against the data population. The vertical-style slicing selects the subset of columns from the population, while preserving the rows. That is, the data science tool can see only the preselected columns from a record for all the records in the population.
Code: 
import pandas as pd
import sqlite3 as sq
sDatabaseName='D:/DSPrac/practical-data-science-master/VKHCG/99-DW/datawarehouse.db'
conn1 = sq.connect(sDatabaseName)
sDatabaseName='D:/DSPrac/practical-data-science-master/VKHCG/99-DW/datamart.db'
conn2 = sq.connect(sDatabaseName) 
sTable = 'Dim-BMI'
print('Loading :',sDatabaseName,' Table:',sTable)
sSQL="SELECT * FROM [Dim-BMI];"
PersonFrame0=pd.read_sql_query(sSQL, conn1)
sTable = 'Dim-BMI'
print('Loading :',sDatabaseName,' Table:',sTable)
sSQL="SELECT \
       Height,\
       Weight,\
       Indicator\
  FROM [Dim-BMI];"
PersonFrame1=pd.read_sql_query(sSQL, conn1)
DimPerson=PersonFrame1
DimPersonIndex=DimPerson.set_index(['Indicator'],inplace=False)
sTable = 'Dim-BMI-Vertical'
print('Storing :',sDatabaseName,'\n Table:',sTable)
DimPersonIndex.to_sql(sTable, conn2, if_exists="replace")
sTable = 'Dim-BMI-Vertical'
print('Loading :',sDatabaseName,' Table:',sTable)
sSQL="SELECT * FROM [Dim-BMI-Vertical];"
PersonFrame2=pd.read_sql_query(sSQL, conn2)
print('################################')
print('Full Data Set (Rows):', PersonFrame0.shape[0])
print('Full Data Set (Columns):', PersonFrame0.shape[1])
print('################################')
print('Horizontal Data Set (Rows):', PersonFrame2.shape[0])
print('Horizontal Data Set (Columns):', PersonFrame2.shape[1])
print('################################')
print('Nishi Jain-53004230036')

Output:

    C. Write Python program to perform the island style subset of slice of the data warehouse data.
Island Style
Performing island-style slicing or subsetting of the data warehouse is achieved by applying a combination of horizontal- and vertical-style slicing. This generates a subset of specific rows and specific columns reduced at the same time.
Code: 
import pandas as pd
import sqlite3 as sq
sDatabaseName='D:/DSPrac/practical-data-science-master/VKHCG/99-DW/datawarehouse.db'
conn1 = sq.connect(sDatabaseName)
sDatabaseName='D:/DSPrac/practical-data-science-master/VKHCG/99-DW/datamart.db'
conn2 = sq.connect(sDatabaseName) 
sTable = 'Dim-BMI'
print('Loading :',sDatabaseName,' Table:',sTable)
sSQL="SELECT * FROM [Dim-BMI];"
PersonFrame0=pd.read_sql_query(sSQL, conn1) 
sTable = 'Dim-BMI'
print('Loading :',sDatabaseName,' Table:',sTable)
sSQL="SELECT \
       Height,\
       Weight,\
       Indicator\
  FROM [Dim-BMI]\
  WHERE Indicator > 2\
  ORDER BY  \
       Height,\
       Weight;"
PersonFrame1=pd.read_sql_query(sSQL, conn1)
DimPerson=PersonFrame1
DimPersonIndex=DimPerson.set_index(['Indicator'],inplace=False)
sTable = 'Dim-BMI-Vertical'
print('Storing :',sDatabaseName,'\n Table:',sTable)
DimPersonIndex.to_sql(sTable, conn2, if_exists="replace")
sTable = 'Dim-BMI-Vertical'
print('Loading :',sDatabaseName,' Table:',sTable)
print('################################')
sSQL="SELECT * FROM [Dim-BMI-Vertical];"
PersonFrame2=pd.read_sql_query(sSQL, conn2)
print('Full Data Set (Rows):', PersonFrame0.shape[0])
print('Full Data Set (Columns):', PersonFrame0.shape[1])
print('################################')
print('Horizontal Data Set (Rows):', PersonFrame2.shape[0])
print('Horizontal Data Set (Columns):', PersonFrame2.shape[1])
print('################################')
print('Nishi Jain-53004230036')

Output:



    D. Write Python program to perform the secure vault style subset of slice of the data warehouse data and attach the result to the person who performs the query.
Secure Vault Style
The secure vault is a version of one of the horizontal, vertical, or island slicing techniques, but the outcome is also attached to the person who performs the query. This is common in multi-security environments, where different users are allowed to see different data sets.
This process works well, if you use a role-based access control (RBAC) approach to restricting system access to authorized users. The security is applied against the “role,” and a person can then, by the security system, simply be added or removed from the role, to enable or disable access.
Code: 
import pandas as pd
import sqlite3 as sq
sDatabaseName='D:/DSPrac/practical-data-science-master/VKHCG/99-DW/datawarehouse.db'
conn1 = sq.connect(sDatabaseName)
sDatabaseName='D:/DSPrac/practical-data-science-master/VKHCG/99-DW/datamart.db'
conn2 = sq.connect(sDatabaseName) 
sTable = 'Dim-BMI'
print('Loading :',sDatabaseName,' Table:',sTable)
sSQL="SELECT * FROM [Dim-BMI];"
PersonFrame0=pd.read_sql_query(sSQL, conn1)
sTable = 'Dim-BMI'
print('Loading :',sDatabaseName,' Table:',sTable)
sSQL="SELECT \
       Height,\
       Weight,\
       Indicator,\
       CASE Indicator\
       WHEN 1 THEN 'Pip'\
       WHEN 2 THEN 'Norman'\
       WHEN 3 THEN 'Grant'\
       ELSE 'Sam'\
       END AS Name\
  FROM [Dim-BMI]\
  WHERE Indicator > 2\
  ORDER BY  \
       Height,\
       Weight;"
PersonFrame1=pd.read_sql_query(sSQL, conn1)
DimPerson=PersonFrame1
DimPersonIndex=DimPerson.set_index(['Indicator'],inplace=False)
sTable = 'Dim-BMI-Secure'
print('Storing :',sDatabaseName,'\n Table:',sTable)
DimPersonIndex.to_sql(sTable, conn2, if_exists="replace")
sTable = 'Dim-BMI-Secure'
print('Loading :',sDatabaseName,' Table:',sTable)
sSQL="SELECT * FROM [Dim-BMI-Secure] WHERE Name = 'Sam';"
PersonFrame2=pd.read_sql_query(sSQL, conn2)
print('################################')
print('Full Data Set (Rows):', PersonFrame0.shape[0])
print('Full Data Set (Columns):', PersonFrame0.shape[1])
print('################################')
print('Horizontal Data Set (Rows):', PersonFrame2.shape[0])
print('Horizontal Data Set (Columns):', PersonFrame2.shape[1])
print('################################')
print('Only Sams Data appears as follows:')
print(PersonFrame2.head())
print('################################')
print('Nishi Jain-53004230036')

Output:



    E. Write Python program to demonstrate Association rule mining.
Theory 
Association Rule Mining
Association rule learning is a rule-based machine-learning method for discovering interesting relations between variables in large databases, similar to the data you will find in a data lake. The technique enables you to investigate the interaction between data within the same population. Lift is simply estimated by the ratio of the joint probability of two items x and y, divided by the product of their individual probabilities:
Code: 
!pip install mlxtend
import pandas as pd
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules
InputFileName='Online-Retail-Billboard.xlsx'
sFileAssessDir='D:/DSPrac/practical-data-science-master/VKHCG/01-Vermeulen/02-Assess/01-EDS/02-Python'
sFileName='D:/DSPrac/practical-data-science-master/VKHCG/01-Vermeulen/00-RawData/' + InputFileName
df = pd.read_excel(sFileName)
print(df.shape)
df['Description'] = df['Description'].str.strip()
df.dropna(axis=0, subset=['InvoiceNo'], inplace=True)
df['InvoiceNo'] = df['InvoiceNo'].astype('str')
df = df[~df['InvoiceNo'].str.contains('C')]
basket = (df[df['Country'] =="France"]
          .groupby(['InvoiceNo', 'Description'])['Quantity']
          .sum().unstack().reset_index().fillna(0)
          .set_index('InvoiceNo'))
def encode_units(x):
    if x <= 0:
        return 0
    if x >= 1:
        return 1
basket_sets = basket.applymap(encode_units)
basket_sets.drop('POSTAGE', inplace=True, axis=1)
frequent_itemsets = apriori(basket_sets, min_support=0.07, use_colnames=True)
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)
print(rules.head())
print('#####################################')
rules[ (rules['lift'] >= 6) &
       (rules['confidence'] >= 0.8) ]
sProduct1='ALARM CLOCK BAKELIKE GREEN'
print(sProduct1)
print(basket[sProduct1].sum())
sProduct2='ALARM CLOCK BAKELIKE RED'
print(sProduct2)
print(basket[sProduct2].sum())
basket2 = (df[df['Country'] =="Germany"]
          .groupby(['InvoiceNo', 'Description'])['Quantity']
          .sum().unstack().reset_index().fillna(0)
          .set_index('InvoiceNo'))
basket_sets2 = basket2.applymap(encode_units)
basket_sets2.drop('POSTAGE', inplace=True, axis=1)
frequent_itemsets2 = apriori(basket_sets2, min_support=0.05, use_colnames=True)
rules2 = association_rules(frequent_itemsets2, metric="lift", min_threshold=1)
print(rules2[ (rules2['lift'] >= 4) &
        (rules2['confidence'] >= 0.5)])
print('Done!! ############################################')
print('Nishi Jain-53004230036')

Output:

    F. Write Python program to create network routing diagram in organizing superstep.
Theory 
Create a Network Routing Diagram
I will guide you through a possible solution for the requirement, by constructing an island-style Organize
superstep that uses a graph data model to reduce the records and the columns on the data set.
Code: 
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
pd.options.mode.chained_assignment = None
sInputFileName='D:/DSPrac/practical-data-science-master/VKHCG/01-Vermeulen/02-Assess/01-EDS/02-Python/Assess-Network-Routing-Company.csv'
sOutputFileName1='D:/DSPrac/practical-data-science-master/VKHCG/01-Vermeulen/05-Organise/01-EDS/02-Python/Organise-Network-Routing-Company.gml'
sOutputFileName2='D:/DSPrac/practical-data-science-master/VKHCG/01-Vermeulen/05-Organise/01-EDS/02-Python/Organise-Network-Routing-Company.png'
sFileName= sInputFileName
print('################################')
print('Loading :',sFileName)
CompanyData=pd.read_csv(sFileName,header=0,low_memory=False, encoding="latin-1")
print('################################')
print(CompanyData.head())
print(CompanyData.shape)
G=nx.Graph()
for i in range(CompanyData.shape[0]):
    for j in range(CompanyData.shape[0]):
        Node0=CompanyData['Company_Country_Name'][i]
        Node1=CompanyData['Company_Country_Name'][j]
        if Node0 != Node1:
            G.add_edge(Node0,Node1)
for i in range(CompanyData.shape[0]):
    Node0=CompanyData['Company_Country_Name'][i]
    Node1=CompanyData['Company_Place_Name'][i] + '('+ CompanyData['Company_Country_Name'][i] + ')'
    if Node0 != Node1:
        G.add_edge(Node0,Node1)
print('Nodes:', G.number_of_nodes())   
print('Edges:', G.number_of_edges())       
sFileName= sOutputFileName1
print('################################')
print('Storing :',sFileName)
nx.write_gml(G, sFileName)      
sFileName= sOutputFileName2
print('################################')
print('Storing Graph Image:',sFileName)
print('################################')
print('Done!! #####################')
print('Nishi Jain-53004230036')
plt.figure(figsize=(15, 15))
pos=nx.spectral_layout(G,dim=2)
nx.draw_networkx_nodes(G,pos, node_color='k', node_size=10, alpha=0.8)
nx.draw_networkx_edges(G, pos,edge_color='r', arrows=False, style='dashed')
nx.draw_networkx_labels(G,pos,font_size=12,font_family='sans-serif',font_color='b')
plt.axis('off')
plt.savefig(sFileName,dpi=600)
plt.show()

Output:


Practical 8
AIM: Generating Data
Theory:
Report Superstep
The Report superstep is the step in the ecosystem that enhances the data science findings with the art of
storytelling and data visualization. You can perform the best data science, but if you cannot execute a
respectable and trustworthy Report step by turning your data science into actionable business insights, you have achieved no advantage for your business.
Vermeulen PLC
Vermeulen requires a map of all their customers’ data links
    A. Write a python program to perform data visualization to create following graphs using profit data.
Graphics
This section will now guide you through a number of visualizations that particularly useful in presenting data to my customers.
1]Pie Graph 
Code: 
import pandas as pd
import matplotlib as ml
from matplotlib import pyplot as plt
GBase ='D:/DSPrac/practical-data-science-master/VKHCG/01-Vermeulen/06-Report/01-EDS/02-Python/'
ml.style.use('ggplot')
data=[
['London', 29.2, 17.4],
['Glasgow', 18.8, 11.3],
['Cape Town', 15.3, 9.0],
['Houston', 22.0, 7.8],
['Perth', 18.0, 23.7],
['San Francisco', 11.4, 33.3]
]
os_new=pd.DataFrame(data)
pd.Index(['Item', 'Value', 'Value Percent', 'Conversions', 'ConversionPercent','URL', 'Stats URL'],dtype='object')
os_new.rename(columns = {0 : "Warehouse Location"}, inplace=True)
os_new.rename(columns = {1 : "Profit 2016"}, inplace=True)
os_new.rename(columns = {2 : "Profit 2017"}, inplace=True)
explode = (0, 0, 0, 0, 0, 0.1)
labels=os_new['Warehouse Location']
colors_mine = ['yellowgreen', 'gold', 'lightskyblue', 'lightcoral',
'lightcyan','lightblue']
os_new.plot(figsize=(10, 10),kind="pie", y="Profit 2017",autopct='%.2f%%', \
            shadow=True, explode=explode, legend = False, colors = colors_mine,\
            labels=labels, fontsize=20)
sPicNameOut1=GBase+'pie_explode.png'
plt.savefig(sPicNameOut1,dpi=600)
print('Nishi Jain-53004230036')

Output:



2]Double Pie Graph 
Code: 
import pandas as pd
import matplotlib as ml
from matplotlib import pyplot as plt
GBase ='D:/DSPrac/practical-data-science-master/VKHCG/01-Vermeulen/06-Report/01-EDS/02-Python/'
ml.style.use('ggplot')
data=[
['London', 29.2, 17.4],
['Glasgow', 18.8, 11.3],
['Cape Town', 15.3, 9.0],
['Houston', 22.0, 7.8],
['Perth', 18.0, 23.7],
['San Francisco', 11.4, 33.3]
]
os_new=pd.DataFrame(data)
pd.Index(['Item', 'Value', 'Value Percent', 'Conversions', 'ConversionPercent','URL', 'Stats URL'],dtype='object')
os_new.rename(columns = {0 : "Warehouse Location"}, inplace=True)
os_new.rename(columns = {1 : "Profit 2016"}, inplace=True)
os_new.rename(columns = {2 : "Profit 2017"}, inplace=True)
explode = (0, 0, 0, 0, 0, 0)
labels=os_new['Warehouse Location']
colors_mine = ['yellowgreen', 'gold', 'lightskyblue', 'lightcoral','lightcyan','lightblue']
os_new.plot(figsize=(10, 5),kind="pie", y=['Profit 2016','Profit 2017'],autopct='%.2f%%', \
shadow=True, explode=explode, legend = False,colors =
colors_mine,\
subplots=True, labels=labels, fontsize=10)
sPicNameOut2=GBase+'pie.png'
plt.savefig(sPicNameOut2, dpi=600)
print('Nishi Jain-53004230036')

Output:








3]Line Graph 
Code: 
import pandas as pd
import matplotlib as ml
from matplotlib import pyplot as plt
GBase ='D:/DSPrac/practical-data-science-master/VKHCG/01-Vermeulen/06-Report/01-EDS/02-Python/'
ml.style.use('ggplot')
data=[
['London', 29.2, 17.4],
['Glasgow', 18.8, 11.3],
['Cape Town', 15.3, 9.0],
['Houston', 22.0, 7.8],
['Perth', 18.0, 23.7],
['San Francisco', 11.4, 33.3]
]
os_new=pd.DataFrame(data)
pd.Index(['Item', 'Value', 'Value Percent', 'Conversions', 'ConversionPercent','URL', 'Stats URL'],dtype='object')
os_new.rename(columns = {0 : "Warehouse Location"}, inplace=True)
os_new.rename(columns = {1 : "Profit 2016"}, inplace=True)
os_new.rename(columns = {2 : "Profit 2017"}, inplace=True)
os_new.iloc[:5].plot(figsize=(10, 10),kind='line',x='Warehouse Location',\
 y=['Profit 2016','Profit 2017']);
sPicNameOut3=GBase+'line.png'
plt.savefig(sPicNameOut3,dpi=600)
print('Nishi Jain-53004230036')

Output:

4]Vertical Bar Graph 
Code: 
import pandas as pd
import matplotlib as ml
from matplotlib import pyplot as plt
GBase ='D:/DSPrac/practical-data-science-master/VKHCG/01-Vermeulen/06-Report/01-EDS/02-Python/'
ml.style.use('ggplot')
data=[
['London', 29.2, 17.4],
['Glasgow', 18.8, 11.3],
['Cape Town', 15.3, 9.0],
['Houston', 22.0, 7.8],
['Perth', 18.0, 23.7],
['San Francisco', 11.4, 33.3]
]
os_new=pd.DataFrame(data)
pd.Index(['Item', 'Value', 'Value Percent', 'Conversions', 'ConversionPercent','URL', 'Stats URL'],dtype='object')
os_new.rename(columns = {0 : "Warehouse Location"}, inplace=True)
os_new.rename(columns = {1 : "Profit 2016"}, inplace=True)
os_new.rename(columns = {2 : "Profit 2017"}, inplace=True)
os_new.iloc[:5].plot(figsize=(10, 10),kind='bar',x='Warehouse Location',\
 y=['Profit 2016','Profit 2017']);
sPicNameOut4=GBase+'bar.png'
plt.savefig(sPicNameOut4,dpi=600)
print('Nishi Jain-53004230036')

Output:

5]Horizontal Bar Graph 
Code: 
import pandas as pd
import matplotlib as ml
from matplotlib import pyplot as plt
GBase ='D:/DSPrac/practical-data-science-master/VKHCG/01-Vermeulen/06-Report/01-EDS/02-Python/'
ml.style.use('ggplot')
data=[
['London', 29.2, 17.4],
['Glasgow', 18.8, 11.3],
['Cape Town', 15.3, 9.0],
['Houston', 22.0, 7.8],
['Perth', 18.0, 23.7],
['San Francisco', 11.4, 33.3]
]
os_new=pd.DataFrame(data)
pd.Index(['Item', 'Value', 'Value Percent', 'Conversions', 'ConversionPercent','URL', 'Stats URL'],dtype='object')
os_new.rename(columns = {0 : "Warehouse Location"}, inplace=True)
os_new.rename(columns = {1 : "Profit 2016"}, inplace=True)
os_new.rename(columns = {2 : "Profit 2017"}, inplace=True)
os_new.iloc[:5].plot(figsize=(10, 10),kind='barh',x='Warehouse Location',\
 y=['Profit 2016','Profit 2017']);
sPicNameOut5=GBase+'hbar.png'
plt.savefig(sPicNameOut5,dpi=600)
print('Nishi Jain-53004230036')

Output:

6]Area Graph 
Code: 
import pandas as pd
import matplotlib as ml
from matplotlib import pyplot as plt
GBase ='D:/DSPrac/practical-data-science-master/VKHCG/01-Vermeulen/06-Report/01-EDS/02-Python/'
ml.style.use('ggplot')
data=[
['London', 29.2, 17.4],
['Glasgow', 18.8, 11.3],
['Cape Town', 15.3, 9.0],
['Houston', 22.0, 7.8],
['Perth', 18.0, 23.7],
['San Francisco', 11.4, 33.3]
]
os_new=pd.DataFrame(data)
pd.Index(['Item', 'Value', 'Value Percent', 'Conversions', 'ConversionPercent','URL', 'Stats URL'],dtype='object')
os_new.rename(columns = {0 : "Warehouse Location"}, inplace=True)
os_new.rename(columns = {1 : "Profit 2016"}, inplace=True)
os_new.rename(columns = {2 : "Profit 2017"}, inplace=True)
os_new.iloc[:5].plot(figsize=(10, 10),kind='area',x='Warehouse Location',\
 y=['Profit 2016','Profit 2017'],stacked=False);
sPicNameOut6=GBase+'area.png'
plt.savefig(sPicNameOut6,dpi=600)
print('Nishi Jain-53004230036')

Output:

7]Scatter Graph 
Code: 
import pandas as pd
import matplotlib as ml
from matplotlib import pyplot as plt
GBase ='D:/DSPrac/practical-data-science-master/VKHCG/01-Vermeulen/06-Report/01-EDS/02-Python/'
ml.style.use('ggplot')
data=[
['London', 29.2, 17.4],
['Glasgow', 18.8, 11.3],
['Cape Town', 15.3, 9.0],
['Houston', 22.0, 7.8],
['Perth', 18.0, 23.7],
['San Francisco', 11.4, 33.3]
]
os_new=pd.DataFrame(data)
pd.Index(['Item', 'Value', 'Value Percent', 'Conversions', 'ConversionPercent','URL', 'Stats URL'],dtype='object')
os_new.rename(columns = {0 : "Warehouse Location"}, inplace=True)
os_new.rename(columns = {1 : "Profit 2016"}, inplace=True)
os_new.rename(columns = {2 : "Profit 2017"}, inplace=True)
os_new.iloc[:5].plot(figsize=(10, 10),kind='scatter',x='Profit 2016',\
 y='Profit 2017',color='DarkBlue',marker='D');
sPicNameOut7=GBase+'scatter.png'
plt.savefig(sPicNameOut7,dpi=600)
print('Nishi Jain-53004230036')

Output:

8]Hex Bin Graph 
Code:
import pandas as pd
import matplotlib as ml
from matplotlib import pyplot as plt
GBase ='D:/DSPrac/practical-data-science-master/VKHCG/01-Vermeulen/06-Report/01-EDS/02-Python/'
ml.style.use('ggplot')
data=[
['London', 29.2, 17.4],
['Glasgow', 18.8, 11.3],
['Cape Town', 15.3, 9.0],
['Houston', 22.0, 7.8],
['Perth', 18.0, 23.7],
['San Francisco', 11.4, 33.3]]
os_new=pd.DataFrame(data)
pd.Index(['Item', 'Value', 'Value Percent', 'Conversions', 'ConversionPercent','URL', 'Stats URL'],dtype='object')
os_new.rename(columns = {0 : "Warehouse Location"}, inplace=True)
os_new.rename(columns = {1 : "Profit 2016"}, inplace=True)
os_new.rename(columns = {2 : "Profit 2017"}, inplace=True)
os_new.iloc[:5].plot(figsize=(13, 10),kind='hexbin',x='Profit 2016',\
 y='Profit 2017', gridsize=25);
sPicNameOut8=GBase+'hexbin.png'
plt.savefig(sPicNameOut8,dpi=600)
print('Nishi Jain-53004230036')

Output:

B. Write a python program to perform data visualization to create following advanced graphs/plots using profit data.
1]Kernel Density Estimation (KDE)Graph 
Code:
import pandas as pd
import matplotlib as ml
from matplotlib import pyplot as plt
import numpy as np
Base ='D:/DSPrac/practical-data-science-master/VKHCG/01-Vermeulen/06-Report/01-EDS/02-Python/'
ml.style.use('ggplot')
fig1=plt.figure(figsize=(10, 10))
ser = pd.Series(np.random.randn(1000))
ser.plot(figsize=(10, 10),kind='kde')
sPicNameOut1=Base+'kde.png'
plt.savefig(sPicNameOut1,dpi=600)
plt.tight_layout()
print('Nishi Jain-53004230036')
plt.show()

Output:


2]Scatter Matrix 
Code:
import pandas as pd
import matplotlib as ml
from matplotlib import pyplot as plt
import numpy as np
Base ='D:/DSPrac/practical-data-science-master/VKHCG/01-Vermeulen/06-Report/01-EDS/02-Python/'
ml.style.use('ggplot')
fig2=plt.figure(figsize=(10, 10))
from pandas.plotting import scatter_matrix
df = pd.DataFrame(np.random.randn(1000, 5), columns=['Y2014','Y2015','Y2016', 'Y2017', 'Y2018'])
scatter_matrix(df, alpha=0.2, figsize=(10, 10), diagonal='kde')
sPicNameOut2=Base+'scatter_matrix.png'
plt.savefig(sPicNameOut2,dpi=600)
plt.tight_layout()
print('Nishi Jain-53004230036')
plt.show()

Output:


3]Andrew’s Curves 
Code:
import pandas as pd
from matplotlib import pyplot as plt
Base ='D:/DSPrac/practical-data-science-master/VKHCG'
sDataFile=Base+'/01-Vermeulen/00-RawData/irisdata.csv'
data = pd.read_csv(sDataFile)
from pandas.plotting import andrews_curves
plt.figure(figsize=(10, 10))
andrews_curves(data, 'Name')
sPicNameOut1=Base+'/01-Vermeulen/06-Report/01-EDS/02-Python/andrews_curves.png'
plt.savefig(sPicNameOut1,dpi=600)
plt.tight_layout()
print('Nishi Jain-53004230036')
plt.show()

Output:





4]Parallel Coordinates 
Code: 
import pandas as pd
from matplotlib import pyplot as plt
Base ='D:/DSPrac/practical-data-science-master/VKHCG'
sDataFile=Base+'/01-Vermeulen/00-RawData/irisdata.csv'
data = pd.read_csv(sDataFile)
from pandas.plotting import parallel_coordinates
plt.figure(figsize=(10, 10))
parallel_coordinates(data, 'Name')
sPicNameOut2=Base+'/01-Vermeulen/06-Report/01-EDS/02-Python/parallel_coordinates.png'
plt.savefig(sPicNameOut2,dpi=600)
plt.tight_layout()
print('Nishi Jain-53004230036')
plt.show()

Output:



5]RADVIZ method 
Code: 
import pandas as pd
from matplotlib import pyplot as plt
Base ='D:/DSPrac/practical-data-science-master/VKHCG'
sDataFile=Base+'/01-Vermeulen/00-RawData/irisdata.csv'
data = pd.read_csv(sDataFile)
from pandas.plotting import radviz
plt.figure(figsize=(10, 10))
radviz(data, 'Name')
sPicNameOut3=Base+'/01-Vermeulen/06-Report/01-EDS/02-Python/radviz.png'
plt.savefig(sPicNameOut3,dpi=600)
plt.tight_layout()
print('Nishi Jain-53004230036')
plt.show()

Output:





6]Lag plot 
Code: 
import pandas as pd
from matplotlib import style
from matplotlib import pyplot as plt
import numpy as np
Base ='D:/DSPrac/practical-data-science-master/VKHCG'
style.use('ggplot')
from pandas.plotting import lag_plot
plt.figure(figsize=(10, 10))
data = pd.Series(0.1 * np.random.rand(1000) + \
 0.9 * np.sin(np.linspace(-99 * np.pi, 99 * np.pi, num=1000)))
lag_plot(data)
sPicNameOut1=Base+'/01-Vermeulen/06-Report/01-EDS/02-Python/lag_plot.png'
plt.savefig(sPicNameOut1,dpi=600)
plt.tight_layout()
print('Nishi Jain-53004230036')
plt.show()

Output:


7]Autocorrelation Plot
Code:
import pandas as pd
from matplotlib import style
from matplotlib import pyplot as plt
import numpy as np
Base ='D:/DSPrac/practical-data-science-master/VKHCG'
style.use('ggplot')
from pandas.plotting import autocorrelation_plot
plt.figure(figsize=(10, 10))
data = pd.Series(0.7 * np.random.rand(1000) + \
 0.3 * np.sin(np.linspace(-9 * np.pi, 9 * np.pi, num=1000)))
autocorrelation_plot(data)
sPicNameOut2=Base+'/01-Vermeulen/06-Report/01-EDS/02-Python/autocorrelation_plot.png'
plt.savefig(sPicNameOut2,dpi=600)
plt.tight_layout()
print('Nishi Jain-53004230036')
plt.show()

Output:





8]Bootstrap Plot 
Code: 
import pandas as pd
from matplotlib import style
from matplotlib import pyplot as plt
import numpy as np
Base ='D:/DSPrac/practical-data-science-master/VKHCG'
style.use('ggplot')
from pandas.plotting import bootstrap_plot
data = pd.Series(np.random.rand(1000))
plt.figure(figsize=(10, 10))
bootstrap_plot(data, size=50, samples=500, color='grey')
sPicNameOut3=Base+'/01-Vermeulen/06-Report/01-EDS/02-Python/bootstrap_plot.png'
plt.savefig(sPicNameOut3,dpi=600)
plt.tight_layout()
print('Nishi Jain-53004230036')
plt.show()

Output:



9]Contour Graphs 
Code:
import matplotlib
import numpy as np
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt
Base='D:/DSPrac/practical-data-science-master/VKHCG'
matplotlib.rcParams['xtick.direction'] = 'out'
matplotlib.rcParams['ytick.direction'] = 'out'
delta = 0.025
x = np.arange(-3.0, 3.0, delta)
y = np.arange(-2.0, 2.0, delta)
X, Y = np.meshgrid(x, y)
Z1 = mlab.bivariate_normal(X, Y, 1.0, 1.0, 0.0, 0.0)
Z2 = mlab.bivariate_normal(X, Y, 1.5, 0.5, 1, 1)
# difference of Gaussians
Z = 10.0 * (Z2 - Z1)
plt.figure(figsize=(10, 10))
CS = plt.contour(X, Y, Z)
plt.clabel(CS, inline=1, fontsize=10)
plt.title('Simply default with labels')
sPicNameOut0=Base+'/01-Vermeulen/06-Report/01-EDS/02-Python/contour0.png'
plt.savefig(sPicNameOut0,dpi=600)
plt.tight_layout()
plt.show()

Output:


10]3D Graph
Code:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn import datasets
Base ='D:/DSPrac/practical-data-science-master/VKHCG'
np.random.seed(5)
centers = [[1, 1], [-1, -1], [1, -1]]
iris = datasets.load_iris()
X = iris.data
y = iris.target
fig = plt.figure(1, figsize=(12, 8))
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
plt.cla()
pca = decomposition.PCA(n_components=3)
pca.fit(X)
X = pca.transform(X)
for name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:
	ax.text3D(X[y == label, 0].mean(),
     	     X[y == label, 1].mean() + 1.5,
          	X[y == label, 2].mean(), name,
              horizontalalignment='center',
 bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))
y = np.choose(y, [1, 2, 0]).astype(np.float)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.Spectral,edgecolor='k',marker='p',s=300)
ax.w_xaxis.set_ticklabels([])
ax.w_yaxis.set_ticklabels([])
ax.w_zaxis.set_ticklabels([])
sPicNameOut0=Base+'/01-Vermeulen/06-Report/01-EDS/02-Python/3DPlot.png'
plt.savefig(sPicNameOut0,dpi=600)
print('Nishi Jain-53004230036')
plt.show()

Output:

Practical 9
Aim: Data Visualization with Power BI
Case Study: Sales Data
    Step 1: Connect to an Excel workbook
    1. Launch Power BI Desktop
    2. From the Home ribbon, select Get Data. Excel is one of the Most Common data Collections, so you can select it directly from the Get Data menu.
    3. If you select the Get Data button directly, you can also select File> Excel and select Connect.
    4. In the Open File dialog box, select the Products.xlsx file.
You can also open the Query Editor by selecting Edit Queries from the Home ribbon in Power BI Desktop. The following steps are performed in Query Editor.
    1. In Query Editor, select the ProductID, ProductName, QuantityPerUnit, and UnitsInStock columns
(use Ctrl+ Click to select more than one columns, or Shift+Click to select columns that   are beside each other)
    2. Select Remove Columns -> Remove Other Columns from the ribbon, or right-click on a columns header and click Remove Other Columns.

    3. Change the data type of the UnitsInStock column
             For the Excel workbook, products in stock will always be a whole number, so in this step you confirm the UnitsInStock column’s datatype is Whole Number.
    1. Select the UnitsInStock column.
    2. Select the Data Type drop-down button in the Home ribbon
    3. If not already a Whole Number, set Whole Number for data type from the drop down (the Data Type: button also displays the data type for the current selection).

Task 2: Import order data from an OData feed
You import data into Power BI Desktop from the sample Northwind OData feed at the following
URL, which you can copy (and then paste) in the steps below:
http://services.odata.org/V3/Northwind/Northwind.svc/



Step 1: Connect to an OData feed
1. From the Home ribbon tab in Query Editor, select Get Data.
2. Browse to the OData Feed data source.
3. In the OData Feed dialog box, paste the URL for the Northwind OData feed.
4. Select OK.


Step 2: Expand the Order_Details table
Expand the Order_Details table that is related to the Orders table, to combine the ProductID,
UnitPrice, and Quantity columns from Order_Details into the Orders table.
The Expand operation combines columns from a related table into a subject table. When the query runs, rows from the related table (Order_Details) are combined into rows from the subject table
(Orders).
After you expand the Order_Details table, three new columns and additional rows are added to the
Orders table, one for each row in the nested or related table.
1. In the Query View, scroll to the Order_Details column.
2. In the Order_Details column, select the expand icon ().
3. In the Expand drop-down: a. Select (Select All Columns) to clear all columns.
Select ProductID, UnitPrice, and Quantity.
click OK.


Step 3: Remove other columns to only display columns of interest
In this step you remove all columns except OrderDate, ShipCity, ShipCountry,
Order_Details.ProductID, Order_Details.UnitPrice, and Order_Details.Quantity columns. In the previous task, you used Remove Other Columns. For this task, you remove selected columns.
In the Query View, select all columns by completing a.
a. Click the first column (OrderID).
b. Shift+Click the last column (Shipper).
c. Now that all columns are selected, use Ctrl+Click to unselect the following columns:
OrderDate, ShipCity, ShipCountry, Order_Details.ProductID, Order_Details.UnitPrice, and
Order_Details.Quantity.

Now that only the columns we want to remove are selected, right-click on any selected column
header and click Remove Columns.
Step 4: Calculate the line total for each Order_Details row
Power BI Desktop lets you to create calculations based on the columns you are importing, so you can
enrich the data that you connect to. In this step, you create a Custom Column to calculate the line
total for each Order_Details row.
Calculate the line total for each Order_Details row:
1. In the Add Column ribbon tab, click Add Custom Column.
2. In the Add Custom Column dialog box, in the Custom Column Formula textbox, enter
[Order_Details.UnitPrice] * [Order_Details.Quantity].
3. In the New column name textbox, enter LineTotal.


Step 5: Set the datatype of the LineTotal field
1. Right click the LineTotal column.
2. Select Change Type and choose Decimal Number.

Step 6: Rename and reorder columns in the query
1. In Query Editor, drag the LineTotal column to the left, after ShipCountry.

2. Remove the Order_Details. prefix from the Order_Details.ProductID, Order_Details.UnitPrice
and Order_Details.Quantity columns, by double-clicking on each column header, and then deleting
that text from the column name.

Task 3: Combine the Products and Total Sales queries
1.First, we need to load the model that we created in Query Editor into Power BI Desktop. From the Home Ribbon of Query Editor, select Close & Load.
2. Power BI Desktop loads the data from the two queries
3. Once the data is loaded, select the Manage Relationships button Home ribbon
4. Select the New… button
5. When we attempt to create the relationship, we see that one already exists! As shown in the
Create Relationship dialog (by the shaded columns), the ProductsID fields in each query
already have an established relationship.

6.Select Cancel, and then select Relationship view in Power BI Desktop.


Task 4: Build visuals using your data
Step 1: Create charts showing Units in Stock by Product and Total Sales by Year



Step 2: Drag OrderDate to the canvas beneath the first chart, then drag LineTotal (again, the Fields pane) onto the visual, then select Line Chart. The following visualization is created.
Step 3: Next, drag ShipCountry to a spare on the canvas in the top right. Because you selected a geographic field, a map was created automatically. Now drag LineTotal to the Values field; the circles on the map for each country are now relative in size to the LineTotal for orders shipped to that country.



