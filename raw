1
INDEX
Sr.no Title Date Pg.no Sign
1 Design an Expert system using AIML. 29/07/24 2
2 Implement Bayes Theorem using Python. 5/08/24 5
3 Implement Conditional Probability and joint
probability using Python.
26/08/24 7
4 Create a simple rule-based system in Prolog for
diagnosing a common illness based on symptoms.
3/09/24 9
5 Design a Fuzzy based application using Python 28/09/24 11
6 Simulate artificial neural network model with both
feedforward and back-propagation approach.
16/09/24 14
7 Simulate genetic algorithm with suitable example
using Python any other platform
14/09/24 18
8 Design intelligent agent using any AI algorithm.
design expert tutoring system
28/09/24 20
9 Design an application to simulate language parser. 5/10/24 22
10 Develop the semantic net using python. 5/10/24 24
2
Practical 1
Aim: Design an Expert system using AIML
Theory:
In an expert system there are three main components: User Interface, Inference Engine and
Knowledge Base
User Interface: Uses various user interfaces, such as menus, graphics, and dashboards, or
Natural Language Processing (NLP) to communicate with the user.
Expert System: A software program that makes decisions or provides advice based on
databases of expert knowledge in various contexts, such as medical diagnosis. An expert
system is a computer program that solves problems in a specialized field that typically calls for
human expertise using techniques from artificial intelligence. A well-organized collection of
data about the system's domain is called a knowledge base.
The knowledge base's facts are interpreted and assessed by the inference engine, which then
outputs the desired results or an answer.
Code:
Defining Flu.aiml
<aiml version="1.0.1" encoding="UTF-8">
<category>
<pattern>WHAT ARE THE SYMPTOMS OF FLU </pattern>
<template>
Flu symptoms usually include fever, chills, muscle aches, cough, congestion, runny
nose, headaches, and fatigue.
</template>
</category>
<category>
<pattern>I HAVE FEVER AND COUGH</pattern>
3
<template>
These symptoms could be associated with the flu. However, I recommend visiting a
healthcare professional for an accurate diagnosis.
</template>
</category>
<category>
<pattern>IS FLU CONTAGIOUS</pattern>
<template>
Yes, flu is highly contagious and can spread easily from person
to person.
</template>
</category>
<category>
<pattern>HOW CAN I PREVENT FLU</pattern>
<template>
The best way to prevent the flu is by getting a flu vaccine each year. Additionally,
wash your hands frequently, avoid close contact with sick people, and maintain a healthy
lifestyle.
</template>
</category>
<category>
<pattern>THANK YOU</pattern>
<template>
You're welcome! Take care and stay healthy.
</template>
</category>
<category>
<pattern>BYE</pattern>
<template>
Goodbye! Feel free to reach out if you have more questions.
</template>
</category>
<category>
<pattern>FLU*</pattern>
<template>
Could you please provide more details about your symptoms so
that I can assist you better?
</template>
</category>
</aiml>
Jupyter Code:
import aiml
kernel = aiml.Kernel()
kernel.learn("flu.aiml")
print("Expert System for Identifying Flu Symptoms")
print("Type 'bye' to exit the conversation.")
while True:
user_input = input("You: ")
if user_input.lower() == "bye":
4
print("System: Goodbye! Stay healthy.")
break
response = kernel.respond(user_input.upper())
print(f"System: {response}")
Output:
5
Practical 2
Aim: Implement Bayes Theorem using Python.
Theory:
Bayes' Theorem is a fundamental concept in probability theory that describes how to update
the probability of a hypothesis based on new evidence. It's widely used in various fields such
as medicine, finance, and machine learning.
Bayes' Theorem Formula
P(A|B) – the probability of event A occurring, given event B has occurred. P(B|A) – the
probability of event B occurring, given event A has occurred. P(A) – the probability of event
A.
Events:
A: Flower is Setosa
B: sepal length being greater than 5.0 cm
Steps Breakdown:
1. Prior Probability P(A): The probability of the flower being setosa (without
any conditions).
2. Likelihood P(B∣A): The probability of the sepal length being greater than
5.0 cm, given the flower is setosa.
3. Marginal Probability P(B): The probability of the sepal length being greater
than 5.0 cm, across the whole dataset.
4. Posterior Probability P(A∣B): The probability that a flower is setosa, given
that its sepal length is greater than 5.0 cm.
Code:
import pandas as pd
def bayes_theorem(prior_A, likelihood_B_given_A, marginal_B):
"""
Calculate the posterior probability using Bayes' Theorem
:param prior_A: P(A) - Prior probability of A
:param likelihood_B_given_A: P(B|A) - Likelihood of B given A
:param marginal_B: P(B) - Marginal probability of B
6
:return: P(A|B) - Posterior probability of A given B
"""
return (likelihood_B_given_A * prior_A) / marginal_B
# Load the Iris dataset
def load_iris_dataset(file_path):
return pd.read_csv(file_path)
# Calculate prior probability P(A)
def calculate_prior(data, class_col, class_value):
return len(data[data[class_col] == class_value]) / len(data)
# Calculate likelihood P(B|A)
def calculate_likelihood(data, class_col, class_value, feature_col,
feature_condition):
subset = data[data[class_col] == class_value]
return len(subset[subset[feature_col] > feature_condition]) /
len(subset)
# Calculate marginal probability P(B)
def calculate_marginal(data, feature_col, feature_condition):
return len(data[data[feature_col] > feature_condition]) / len(data)
# Apply Bayes' Theorem on the Iris dataset
def apply_bayes_to_iris(file_path, class_col, class_value, feature_col,
feature_condition):
# Load dataset
data = load_iris_dataset(file_path)
# Calculate prior P(A)
prior_A = calculate_prior(data, class_col, class_value)
# Calculate likelihood P(B|A)
likelihood_B_given_A = calculate_likelihood(data, class_col,
class_value, feature_col, feature_condition)
# Calculate marginal probability P(B)
marginal_B = calculate_marginal(data, feature_col, feature_condition)
# Apply Bayes' Theorem
posterior_A_given_B = bayes_theorem(prior_A, likelihood_B_given_A,
marginal_B)
return posterior_A_given_B
# Example usage:
# Assume we want to calculate the probability P(Class='setosa' |
SepalLength > 5.0)
file_path = 'iris.csv' # Path to the iris dataset file
class_col = 'species' # The column representing the class (A)
class_value = 'virginica' # The class value we're interested in (A)
feature_col = 'sepal_length' # The feature we're using (B)
feature_condition = 5.0 # The condition on the feature (B > 5.0)
# Calculate posterior probability P(setosa|sepal_length > 5.0)
posterior_probability = apply_bayes_to_iris(file_path, class_col,
class_value, feature_col, feature_condition)
print(f"P({class_value} | {feature_col} > {feature_condition}) =
{posterior_probability:.4f}")
print()
Output:
7
Practical 3
Aim: Implement Conditional Probability and joint probability using Python.
Theory:
Conditional Probability (P(A|B)): Conditional probability is a measure of the likelihood of
event A occurring given that event B has occurred. In mathematical terms, it's defined as:
P(A|B) = P(A ∩ B) / P(B)
where P(A ∩ B) represents the joint probability of events A and B occurring together.
Joint Probability (P(A ∩ B)): The joint probability of two events A and B is a measure of
how likely it is that both events will occur simultaneously. In mathematical terms, it's defined
as:
P(A ∩ B) = P(A) × P(B|A)
Code Explanation:
Loading Penguins Dataset: The code loads the penguins dataset from a CSV file using
Pandas.
Calculating Joint Probability (P(Species ∩ Island)): The joint probability is calculated by
creating a pivot table that represents the number of penguins in each species category across
different island categories.
pivot_table = pd.crosstab(df['species'], df['island'], normalize=True)
This code calculates the joint probability as:
P(Adelie ∩ Dream) = P(Adelie) × P(Dream|Adelie)
Calculating Conditional Probability (P(Species|Island)): The conditional probability is
calculated by dividing each cell in the pivot table by the sum of all cells across a specific
island category.
conditional_probability = pivot_table.div(pivot_table.sum(axis=0), axis=1)
This code calculates the conditional probability as:
P(Adelie|Dream) = P(Adelie ∩ Dream) / P(Dream)
Key Features and Assumptions:
The code assumes:
Independence: The events (species and island) are assumed to be independent, meaning that
the likelihood of one event does not influence the other.
Mutual Exclusivity: The code implicitly assumes that each species category is mutually
exclusive across different island categories.
Code:
import pandas as pd
df = pd.read_csv('penguins.csv')
print("Data Preview:")
print(df.head())
pivot_table = pd.crosstab(df['species'], df['island'], normalize=True)
print("\nJoint Probability is represented in the pivot table (Species vs
Island):")
print(pivot_table)
8
p_adelie_given_dream = pivot_table.loc['Adelie', 'Dream']
print()
print('The joint probability of an Adelie penguin being found on Dream
Island.')
print(f"\nP(Adelie | Dream) = {p_adelie_given_dream:.4f}")
conditional_probability = pivot_table.div(pivot_table.sum(axis=0), axis=1)
print("\nConditional Probability of Species given Island:")
print(conditional_probability)
normalized=True
p_adelie_given_dream = conditional_probability.loc['Adelie', 'Dream']
print()
print('The conditional probability of an Adelie penguin being found on
Dream Island.')
print(f"\nP(Adelie | Dream) = {p_adelie_given_dream:.4f}")
Output:
9
Practical 4
Aim: Create a simple rule-based system in Prolog for diagnosing a common illness
based on symptoms.
Theory:
Prolog expressions are comprised of the following truth-functional symbols, which
have the same interpretation as in the predicate calculus.
Code:
%Facts:Define symptoms
symptom(fever).
symptom(cough).
symptom(sore_throat).
symptom(body_aches).
symptom(runny_nose).
symptom(headache).
symptom(fatigue).
%Facts:Define possible illnesses
condition(cold).
condition(flu).
condition(strep_throat).
%Rules: Diagnosing based on the presence of symptoms
diagnose(cold):-
symptom(runny_nose),
symptom(cough),
symptom(sore_throat),
\+ symptom(fever). %Absence of fever
diagnose(flu):-
symptom(fever),
symptom(cough),
symptom(body_aches),
symptom(headache),
symptom(fatigue).
diagnose(sterp_throat):-
symptom(sore_throat),
symptom(fever),
\+symptom(cough). %Absence of cough
%Alternative:Diagnosing based on rule covering all possible symptoms
diagnose(unknown):-
\+diagnose(cold),
\+diagnose(flu),
\+diagnose(strep_throat).
%You can ask Prolog:
?-diagnose(Condition).
10
Output:
11
Practical 5
Aim: Design a Fuzzy based application using Python.
Theory:
A fuzzy logic system is a mathematical framework that handles reasoning with uncertainty
and imprecision. Unlike traditional binary logic (where variables are either true or false, i.e.,
0 or 1), fuzzy logic allows for degrees of truth, where values can range between 0 and 1. This
makes fuzzy logic particularly useful in systems that involve human-like reasoning, where
decisions are not strictly binary but involve some level of vagueness.
Example: Consider an air conditioning system:
- Inputs: Temperature and humidity (both can be fuzzy).
- Outputs: Fan speed (also fuzzy).
- Fuzzy rules might say: "If the temperature is high and the humidity is low, then set
fan speed to high."
Applications:
- Control systems: Air conditioning, washing machines, and automatic transmission in
cars.
- Decision-making systems: Medical diagnosis, stock market prediction, etc.
Fuzzy logic is well-suited for systems where precise data is unavailable, and human-like
reasoning is required to make decisions under uncertainty.
Code:
import numpy as np
import skfuzzy as fuzz
from skfuzzy import control as ctrl
import matplotlib.pyplot as plt
traffic_density = ctrl.Antecedent(np.arange(0, 101, 1), 'traffic_density')
time_of_day = ctrl.Antecedent(np.arange(0, 25, 1), 'time_of_day')
green_light_duration = ctrl.Consequent(np.arange(0, 61, 1),
'green_light_duration')
traffic_density['low'] = fuzz.trimf(traffic_density.universe, [0, 0, 50])
traffic_density['medium'] = fuzz.trimf(traffic_density.universe, [30, 50,
70])
traffic_density['high'] = fuzz.trimf(traffic_density.universe, [50, 100,
100])
time_of_day['non_peak'] = fuzz.trimf(time_of_day.universe, [0, 0, 12])
time_of_day['peak'] = fuzz.trimf(time_of_day.universe, [10, 24, 24])
green_light_duration['short'] = fuzz.trimf(green_light_duration.universe,
[0, 0, 20])
12
green_light_duration['moderate'] =
fuzz.trimf(green_light_duration.universe, [15, 30, 45])
green_light_duration['long'] = fuzz.trimf(green_light_duration.universe,
[40, 60, 60])
traffic_density.view()
time_of_day.view()
green_light_duration.view()
rule1 = ctrl.Rule(traffic_density['low'] & time_of_day['non_peak'],
green_light_duration['short'])
rule2 = ctrl.Rule(traffic_density['low'] & time_of_day['peak'],
green_light_duration['moderate'])
rule3 = ctrl.Rule(traffic_density['medium'] & time_of_day['non_peak'],
green_light_duration['moderate'])
rule4 = ctrl.Rule(traffic_density['medium'] & time_of_day['peak'],
green_light_duration['long'])
rule5 = ctrl.Rule(traffic_density['high'] & time_of_day['non_peak'],
green_light_duration['long'])
rule6 = ctrl.Rule(traffic_density['high'] & time_of_day['peak'],
green_light_duration['long'])
green_light_ctrl = ctrl.ControlSystem([rule1, rule2, rule3, rule4, rule5,
rule6])
green_light_sim = ctrl.ControlSystemSimulation(green_light_ctrl)
green_light_sim.input['traffic_density'] = 75 # High traffic
green_light_sim.input['time_of_day'] = 18 # Peak hours
green_light_sim.compute()
print(f"Recommended Green Light Duration:
{green_light_sim.output['green_light_duration']} seconds")
green_light_duration.view(sim=green_light_sim)
plt.show()
Output:
13
14
15
Practical 6
Aim: Simulate artificial neural network model with both feedforward and
backpropagation approach.
Theory:
Feedforward Neural Network:
In a Feedforward Neural Network, information moves in one direction only: from the input
layer through the hidden layers to the output layer. There are no loops or cycles in the
network.
How it works: Input data is passed through the network, with each neuron applying a weighted
sum of its inputs and an activation function to produce an output. This continues layer by layer
until the final output is obtained.
Backpropagation: Backpropagation is the learning algorithm used to train an ANN. It adjusts
the weights of the network to minimize the error between the predicted output and the actual
target.
16
Code:
import numpy as np
# Sigmoid Activation Function
def sigmoid(x):
return 1 / (1 + np.exp(-x))
# Derivative of the Sigmoid Function for backpropagation
def sigmoid_derivative(x):
return x * (1 - x)
# ANN class to simulate feedforward and backpropagation
class ArtificialNeuralNetwork:
def __init__(self, input_size, hidden_size, output_size,
learning_rate=0.5):
# Initialize weights randomly
self.weights_input_hidden = np.random.rand(input_size, hidden_size)
self.weights_hidden_output = np.random.rand(hidden_size,
output_size)
# Initialize biases randomly
self.bias_hidden = np.random.rand(1, hidden_size)
self.bias_output = np.random.rand(1, output_size)
# Set the learning rate
self.learning_rate = learning_rate
# Feedforward process
def feedforward(self, X):
# Hidden layer activation
self.hidden_input = np.dot(X, self.weights_input_hidden) +
self.bias_hidden
self.hidden_output = sigmoid(self.hidden_input)
# Output layer activation
self.output_input = np.dot(self.hidden_output,
self.weights_hidden_output) + self.bias_output
self.output = sigmoid(self.output_input)
return self.output
# Backpropagation process
def backpropagation(self, X, y):
# Error at the output layer
output_error = y - self.output
output_delta = output_error * sigmoid_derivative(self.output)
# Error at the hidden layer
hidden_error = output_delta.dot(self.weights_hidden_output.T)
hidden_delta = hidden_error *
sigmoid_derivative(self.hidden_output)
# Update the weights and biases using the deltas
self.weights_hidden_output +=
self.hidden_output.T.dot(output_delta) * self.learning_rate
self.weights_input_hidden += X.T.dot(hidden_delta) *
self.learning_rate
17
self.bias_output += np.sum(output_delta, axis=0, keepdims=True) *
self.learning_rate
self.bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) *
self.learning_rate
# Train the neural network
def train(self, X, y, epochs):
for epoch in range(epochs):
# Feedforward
self.feedforward(X)
# Backpropagation
self.backpropagation(X, y)
# Print loss every 100 epochs
if (epoch + 1) % 100 == 0:
loss = np.mean(np.square(y - self.output))
print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss:.6f}')
# Example usage
if __name__ == "__main__":
# Input dataset (XOR problem)
X = np.array([[0, 0],
[0, 1],
[1, 0],
[1, 1]])
# Output dataset (XOR output)
y = np.array([[0],
[1],
[1],
[0]])
# Parameters
input_size = X.shape[1] # 2 features in input
hidden_size = 2 # 2 neurons in hidden layer
output_size = 1 # 1 output neuron (binary classification)
# Create the neural network
ann = ArtificialNeuralNetwork(input_size, hidden_size, output_size,
learning_rate=0.5)
# Train the neural network
ann.train(X, y, epochs=10000)
# Test the neural network
output = ann.feedforward(X)
print("\nPredicted Output after training:")
print(output)
18
Output:
19
Practical 7
Aim: Simulate genetic algorithm with suitable example using Python any other
platform.
Theory:
Genetic Algorithms are a type of optimization technique inspired by the process of natural
selection. They're particularly useful for solving complex problems that involve multiple
parameters or variables.
In the given code, we are using a genetic algorithm to find a sequence of characters
(target_string = "HELLO") with a certain fitness level. The fitness function here checks if the
generated string is equal to the target string. If so, it means we've found the optimal solution.
Genetic Algorithms work by:
Initializing a population of candidate solutions (in this case, strings).
Evaluating each candidate's fitness based on a predefined metric.
Selecting parents from the current generation to reproduce and create offspring.
Applying crossover and mutation operators to generate new candidates.
Repeating steps 3-4 until a termination condition is met.
Code:
import random
import string
target_string = "HELLO"
population_size = 50
mutation_rate = 0.01
generations = 200
def fitness(individual):
return sum(1 for a, b in zip(individual, target_string) if a == b)
def create_population(size):
return [''.join(random.choices(string.ascii_uppercase,
k=len(target_string))) for _ in range(size)]
def select_parents(population):
tournament = random.sample(population, 5)
return max(tournament, key=fitness)
def crossover(parent1, parent2):
crossover_point = random.randint(1, len(parent1) - 1)
return parent1[:crossover_point] + parent2[crossover_point:]
def mutate(individual):
individual = list(individual)
for i in range(len(individual)):
if random.random() < mutation_rate:
individual[i] = random.choice(string.ascii_uppercase)
return ''.join(individual)
population = create_population(population_size)
for generation in range(generations):
best_individual = max(population, key=fitness)
20
print(f"Generation {generation}: Best individual: {best_individual},
Fitness: {fitness(best_individual)}")
if fitness(best_individual) == len(target_string):
break
new_population = []
for _ in range(population_size):
parent1 = select_parents(population)
parent2 = select_parents(population)
child = crossover(parent1, parent2)
child = mutate(child)
new_population.append(child)
population = new_population
best_individual = max(population, key=fitness)
print(f"Best individual: {best_individual}, Fitness:
{fitness(best_individual)}")
Output:
21
Practical 8
Aim: Design intelligent agent using any AI algorithm. design expert tutoring system
Theory:
Artificial Neural Network (ANN)
Artificial Neural Networks are computational models inspired by the structure and function
of biological neural networks. They're powerful tools for classification, regression, and other
machine learning tasks.
The given code implements an ANN with a single hidden layer to classify inputs into binary
outputs. The sigmoid activation function is used in both the hidden and output layers.
ANNs work by:
Initializing weights and biases randomly.
Forward propagating input data through the network, using activation functions (e.g.,
sigmoid) to introduce non-linearity.
Computing the error between predicted outputs and actual labels.
Backpropagating this error through the network to adjust weights and biases.
Repeating steps 2-4 until convergence or a termination condition is met.
Code:
class MathTutor:
def __init__(self):
self.operations = {
'+': lambda a, b: a + b,
'-': lambda a, b: a - b,
'*': lambda a, b: a * b,
'/': lambda a, b: a/b,
}
def explain_operation(self, operator):
explanation = {
'+': "Addition adds two numbers together.",
'-': "Subtraction subtracts the second number from the first.",
'*': "Multiplication gives the product of two numbers.",
'': "Division divides the first number by the second.",
}
return explanation.get(operator, "Invalid operation.")
def perform_operation(self, operator, a, b):
if operator in self.operations:
return self.operations[operator](a, b)
else:
return None
if __name__ == "__main__":
tutor = MathTutor()
# Example usage:
operator = '+'
a, b = 24, 8
print(tutor.explain_operation(operator))
result = tutor.perform_operation(operator, a, b)
print(f"Result of {a} {operator} {b} = {result}")
22
Output:
23
Practical 9
Aim: Design an application to simulate language parser.
Theory:
Knowledge Representation is a field in artificial intelligence (AI) that deals with how to
formally capture and organize information about the world in a way that a machine can
understand and reason with.
KR systems use structured formats (e.g., logic, semantic networks, ontologies) to represent
facts, concepts, and relationships between objects. This allows AI systems to process
complex data, reason about it, and draw conclusions.
In short, KR provides a framework for storing and manipulating knowledge in a machine-
readable format, enabling intelligent decision-making and problem-solving.
Code:
class SimpleParser:
def __init__(self, expr):
self.tokens = expr.replace('(', ' ( ').replace(')', ' ) ').split()
self.pos = 0
def parse(self):
return self.expr()
def advance(self):
self.pos += 1
def current_token(self):
return self.tokens[self.pos] if self.pos < len(self.tokens) else
None
def expr(self):
result = self.term()
while self.current_token() in ('+', '-'):
if self.current_token() == '+':
self.advance()
result += self.term()
elif self.current_token() == '-':
self.advance()
result -= self.term()
return result
def term(self):
result = self.factor()
while self.current_token() in ('*', ''):
if self.current_token() == '*':
self.advance()
result *= self.factor()
elif self.current_token() == '':
self.advance()
result = self.factor()
return result
def factor(self):
token = self.current_token()
24
if token.isdigit():
self.advance()
return int(token)
elif token == '(':
self.advance()
result = self.expr()
self.advance() # skip ')'
return result
raise ValueError("Invalid syntax")
if __name__ == "__main__":
expr = "(80 + 5) * 2"
parser = SimpleParser(expr)
result = parser.parse()
print(f"Result of '{expr}' is {result}")
Output:
25
Practical 10
Aim: Develop the semantic net using python.
Theory:
Semantic Network:
A semantic network is a data structure used to represent knowledge in the form of concepts
(nodes) and their interrelationships (edges or links). It is a graphical model that depicts how
different concepts in a particular domain are connected and how they relate to each other
semantically.
Example: Consider a semantic network for animals:
- Concepts (Nodes): "Dog", "Cat", "Animal", "Mammal".
- Relationships (Edges): "Dog is a Mammal", "Mammal is a Animal", "Dog has Fur",
"Dog can Bark".
In this network:
- Dog is connected to Mammal by an "is a" relationship.
- Dog is connected to Bark by a "can" relationship.
Code:
class SemanticNetwork:
def __init__(self):
self.network = {}
def add_concept(self, concept):
if concept not in self.network:
self.network[concept] = {'is_a': [], 'has_a': []}
def add_relation(self, relation, concept1, concept2):
self.add_concept(concept1)
self.add_concept(concept2)
self.network[concept1][relation].append(concept2)
def get_relations(self, concept):
return self.network.get(concept, {})
def display_network(self):
for concept, relations in self.network.items():
print(f"Concept: {concept}")
for relation, related_concepts in relations.items():
for related_concept in related_concepts:
print(f" {relation} -> {related_concept}")
if __name__ == "__main__":
sn = SemanticNetwork()
# Adding concepts and relations
sn.add_concept("Animal")
sn.add_concept("Bird")
sn.add_concept("Mammal")
sn.add_concept("Penguin")
sn.add_concept("Canary")
26
sn.add_relation("is_a", "Bird", "Animal")
sn.add_relation("is_a", "Mammal", "Animal")
sn.add_relation("is_a", "Penguin", "Bird")
sn.add_relation("is_a", "Canary", "Bird")
sn.add_relation("has_a", "Bird", "Wings")
sn.add_relation("has_a", "Canary", "Yellow_Feathers")
# Displaying the network
sn.display_network()
print()
Output:
27
INDEX
Sr.No Topic Date Pg.
No.
Sign
1 Write a program to implement the naïve Bayesian classifier
for a sample training data set stored as a file. Compute the
accuracy of the classifier, considering few test data sets.
05/08/24 28
2 Write a program to implement Decision Tree and Random
Forest with Prediction, Test Score and Confusion Matrix.
09/08/24 31
3 For a given set of training data examples stored in a file
implement Least Square Regression algorithm.
12/08/24 36
4 For a given set of training data examples stored in a file
implement Logistic Regression algorithm.
16/08/24 40
5 Write a program to demonstrate the working of the
decision tree based ID3 algorithm. Use an appropriate data
set for building the decision tree and apply this knowledge
to classify a new sample.
23/08/24 45
6 Write a program to implement k-Nearest Neighbour
algorithm to classify the iris data set.
30/08/24 49
7 Implement the different Distance methods (Euclidean)
with Prediction, Test Score and Confusion Matrix.
02/09/24 54
8 Implement the classification model using clustering for the
following techniques with K means clustering with
Prediction, Test Score and Confusion Matrix.
06/09/24 58
9 Implement the classification model using clustering for the
following techniques with hierarchical clustering with
Prediction, Test Score and Confusion Matrix.
09/09/24 61
10 Implement the Rule based method and test the same. 13/09/24 66
11 Write a program to construct a Bayesian network
considering medical data. Use this model to demonstrate
the diagnosis of heart patients using standard Heart Disease
Data Set.
20/09/24 68
12 Implement the non-parametric Locally Weighted
Regression algorithm in order to fit data points. Select
appropriate data set for your experiment and draw graphs.
27/09/24 71
13 Implement ANN to solve the XOR problem using forward/
backward propagation and sigmoid activation function
04/10/24 73
28
Practical 1
Aim : Write a program to implement the naïve Bayesian classifier for a
sample training data set stored as a .CSV file. Compute the accuracy of the
classifier, considering few test data sets.
Theory: A naïve Bayesian classifier is a simple and efficient probabilistic classifier based
on Bayes' theorem. It assumes that the features (or attributes) used for classification are
independent of each other given the class label, which is often not true in real-world
scenarios. Despite this simplifying assumption, naïve Bayes can perform surprisingly well,
especially in text classification tasks.
Key Components:
1. Bayes' Theorem: The foundation of the classifier, given by:
P(C∣X)= P(X∣C)⋅P(C) / P(X)
where:
o P(C∣X) is the posterior probability of class C given features X.
o P(X∣C) is the likelihood of features X given class C.
o P(C) is the prior probability of class C.
o P(X) is the prior probability of features X.
o
Code:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Social_Network_Ads.csv')
X = dataset.iloc[:, [2, 3]].values
y = dataset.iloc[:, 4].values
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25,
random_state = 0)
# Feature Scaling
29
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
# Fitting classifier to the Training set
from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(X_train, y_train)
# Predicting the Test set results
y_pred = classifier.predict(X_test)
# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
# Visualising the Training set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop =
X_set[:, 0].max() + 1, step = 0.01),
np.arange(start = X_set[:, 1].min() - 1, stop =
X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(),
X2.ravel()]).T).reshape(X1.shape),
alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Naive Bayes (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
30
plt.show()
# Visualising the Test set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop =
X_set[:, 0].max() + 1, step = 0.01),
np.arange(start = X_set[:, 1].min() - 1, stop =
X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(),
X2.ravel()]).T).reshape(X1.shape),
alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Naive Bayes (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()
Output:-
31
Practical 2
Aim: Write a program to implement Decision Tree and Random forest
with Prediction, Test Score and Confusion Matrix.
Theory:
Decision Tree Classifier: A Decision Tree is a flowchart-like structure that
makes decisions based on feature values. It's easy to visualize and interpret,
which makes it popular for both classification and regression tasks.
Key Steps:
1. Model Training: The tree is built by splitting the dataset into subsets
based on feature values, minimizing impurity (like Gini impurity or
entropy).
2. Prediction: To predict a class for a new instance, the model follows the
branches of the tree based on the feature values until it reaches a leaf
node.
3. Performance Evaluation: Common metrics include accuracy, test score,
and confusion matrix.
Random Forest Classifier: A Random Forest is an ensemble of decision trees.
It combines multiple trees to improve performance and control overfitting.
Key Steps:
1. Model Training: Randomly samples subsets of data and features to build
multiple decision trees.
2. Prediction: Each tree votes for a class, and the class with the most votes is
chosen as the final prediction.
3. Performance Evaluation: Same as decision trees, but generally shows
improved metrics due to reduced variance.
Code:-
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
32
from sklearn.metrics import accuracy_score, confusion_matrix,
classification_report
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names).iloc[:,:2]
y = pd.DataFrame(iris.target, columns=['species'])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
random_state=42)
def plot_decision_boundary(clf, X, y, title):
x_min, x_max = X.iloc[:,0].min()-1, X.iloc[:,0].max()+1
y_min, y_max = X.iloc[:,1].min()-1, X.iloc[:,1].max()+1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),
np.arange(y_min, y_max, 0.01))
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contour(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdYlBu)
plt.scatter(X.iloc[:,0], X.iloc[:,1], c=y.values.ravel(), s=40,
edgecolor='k', cmap=plt.cm.RdYlBu)
plt.title(title)
plt.xlabel(iris.feature_names[0])
plt.ylabel(iris.feature_names[1])
plt.show()
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)
dt_predictions = dt_model.predict(X_test)
dt_accuracy = accuracy_score(y_test, dt_predictions)
33
dt_confusion_matrix = confusion_matrix(y_test, dt_predictions)
print(f'Decision Tree Accuracy: {dt_accuracy}')
print('Decision Tree Classification Report:')
print(classification_report(y_test, dt_predictions))
sns.heatmap(dt_confusion_matrix, annot=True, fmt='d', cmap='Blues')
plt.title('DecisionTree Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()
plot_decision_boundary(dt_model, X_test, y_test, 'Decision Tree Decision
Boundary')
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train.values.ravel())
rf_predictions = rf_model.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_predictions)
rf_confusion_matrix = confusion_matrix(y_test, rf_predictions)
print(f'Random Forest Accuracy: {rf_accuracy}')
print('Random Forest Classification Report:')
print(classification_report(y_test, rf_predictions))
sns.heatmap(rf_confusion_matrix, annot=True, fmt='d', cmap='Greens')
plt.title('Random Forest Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()
plot_decision_boundary(rf_model, X_test, y_test, 'Random Forest Decision
Boundary')
Output:-
34
35
36
Practical 3
Aim : For a given set of training data examples stored in a .CSV file
implement Least Square Regression algorithm.
Theory: Least Squares Regression is a statistical method used to estimate the
relationship between one or more independent variables and a dependent
variable. The goal is to minimize the sum of the squares of the differences
(residuals) between observed and predicted values.
Key Concepts
1. Model Representation: For a linear regression model, the relationship can
be represented as:
y=β0+β1x1+β2x2+...+βnxn+ϵ
where:
o y is the dependent variable.
o x1,x2,...,xn are independent variables.
o β0,β1,...,βn are coefficients to be estimated.
o ϵ is the error term.
2. Objective: The objective of the least squares method is to find the
coefficients β\betaβ that minimize the cost function:
where mmm is the number of observations, yi is the actual value, and y^I is the
predicted value from the model.
Steps to Implement Least Squares Regression
1. Prepare the Data: Organize your dataset into independent variables
(features) and the dependent variable (target).
2. Calculate the Coefficients: The coefficients can be estimated using the
formula:
37
where X is the matrix of input features (with a column of ones for the intercept)
and y is the vector of target values.
3. Make Predictions: Use the estimated coefficients to predict the values of
the dependent variable.
4. Evaluate the Model: Common evaluation metrics include Mean Squared
Error (MSE) and R-squared.
Code:-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.datasets import fetch_california_housing
housing = fetch_california_housing()
X = pd.DataFrame(housing.data, columns = housing.feature_names)
y = pd.DataFrame(housing.target, columns = ['MEDV'])
plt.figure(figsize=(10,8))
sns.heatmap(X.corr(), annot=True, cmap='coolwarm')
plt.title("Correlation Heatmap of California Housing Features")
plt.show()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
random_state=42)
reg_model = LinearRegression()
reg_model.fit(X_train, y_train)
y_train_pred = reg_model.predict(X_train)
y_test_pred = reg_model.predict(X_test)
38
train_mse = mean_squared_error(y_train, y_train_pred)
test_mse = mean_squared_error(y_test, y_test_pred)
train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)
print(f'Training Mean Squared Error: {train_mse}')
print(f'Test Mean Squared Error: {test_mse}')
print(f'Training R^2 Score: {train_r2}')
print(f'Test R^2 Score: {test_r2}')
coefficients = pd.DataFrame(reg_model.coef_.T, X.columns,
columns=['Coefficients'])
print(coefficients)
plt.figure(figsize=(8,6))
plt.scatter(y_test, y_test_pred, c='blue')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r',
lw =3)
plt.xlabel('Actual Value')
plt.ylabel('Predicted Value')
plt.title('Actual VS Predicted Values (Test Set)')
Output:-
39
40
Practical 4
Aim: For a given set of training data examples stored in a .CSV file
implement Logistic Regression algorithm.
Theory: Logistic Regression is a statistical method used for binary classification problems,
where the goal is to model the probability that a given input belongs to a particular category.
Despite its name, it is a classification algorithm rather than a regression algorithm.
Key Concepts
1. Sigmoid Function: Logistic regression uses the logistic function (sigmoid function)
to model probabilities. The sigmoid function maps any real-valued number into the
range (0, 1):
where z is a linear combination of the input features.
2. Model Representation: The model can be expressed as:
where P(y=1∣X)P(y=1 | X)P(y=1∣X) is the probability of the positive class given features X.
x1,x2,...,xnx_1, x_2, ..., x_nx1,x2,...,xn are the independent variables. β0,β1,...,βn are the
coefficients to be learned.
3. Cost Function: The cost function for logistic regression is based on the likelihood of
the observed data, typically using the log loss (binary cross-entropy):
where y^i is the predicted probability for the instance.
4. Optimization: The coefficients are estimated using optimization techniques like
gradient descent to minimize the cost function.
Code:-
41
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix,
classification_report
from sklearn.datasets import load_breast_cancer
cancer_data = load_breast_cancer()
X = pd.DataFrame(cancer_data.data, columns= cancer_data.feature_names)
y = pd.DataFrame(cancer_data.target, columns= ['target'])
print('Dataset Head:')
print(X.head())
print('Target Distribution:')
print(y['target'].value_counts())
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
random_state = 42)
logreg = LogisticRegression(max_iter=10000, random_state=42)
logreg.fit(X_train, y_train.values.ravel())
y_pred = logreg.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)
print(f'Accuracy: {accuracy}')
print()
print('Confusion matrix:')
42
print(conf_matrix)
print()
print('Classification report:')
print(class_report)
plt.figure(figsize=(6,4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()
new_input = np.array([X.mean().values])
print(f'New input for prediction: {new_input}')
new_prediction = logreg.predict(new_input)
predicted_class = 'benign' if new_prediction == 1 else 'maligant'
print(f'Predicted class for the new input: {predicted_class}')
plt.figure(figsize=(6,4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion matrix - Test set')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()
Output:-
43
44
45
Practical 5
Aim: Write a program to demonstrate the working of the decision tree
based ID3 algorithm. Use an appropriate data set for building the decision
tree and apply this knowledge to classify a new sample.
Theory: The ID3 (Iterative Dichotomiser 3) algorithm is a popular decision tree algorithm
used for classification tasks. It uses a top-down, recursive approach to build a tree based on
the concept of information gain, which measures how much knowing the value of a feature
improves our ability to classify the target variable.
Key Concepts:
1. Entropy: A measure of impurity or randomness in a dataset. Lower entropy indicates
a more homogeneous dataset.
where pip_ipi is the proportion of class iii in the dataset SSS.
2. Information Gain: The reduction in entropy after splitting the dataset based on a
feature
where SvS_vSv is the subset of SSS for which attribute AAA has value vvv.
3. Choosing the Best Feature: The feature with the highest information gain is selected
for splitting.
Code:-
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn import tree as sk_tree
# Step 1: Parse the dataset
data = {
46
'Age': ['<=30', '<=30', '31-40', '>40', '>40', '>40', '31-40', '<=30',
'<=30', '>40', '<=30', '31-40', '31-40', '>40'],
'Income': ['High', 'High', 'High', 'Medium', 'Low', 'Low', 'Low',
'Medium', 'Low', 'Medium', 'Medium', 'Medium', 'High', 'Medium'],
'Student': ['No', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes',
'Yes', 'Yes', 'No', 'Yes', 'No'],
'Credit Rating': ['Fair', 'Excellent', 'Fair', 'Fair', 'Fair', 'Excellent',
'Excellent', 'Fair', 'Fair', 'Fair', 'Excellent', 'Excellent', 'Fair',
'Excellent'],
'Buys Computer': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No',
'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']
}
df = pd.DataFrame(data)
# Encode the categorical variables
df_encoded = df.apply(lambda x: pd.factorize(x)[0])
# Fit the decision tree classifier using Gini impurity
clf_gini = sk_tree.DecisionTreeClassifier(criterion='gini')
clf_gini = clf_gini.fit(df_encoded.iloc[:, :-1], df_encoded['Buys
Computer'])
# Convert the feature names from Index to list
feature_names = df.columns[:-1].tolist()
# Convert the class names to a list
class_names = df['Buys Computer'].unique().tolist()
# Plot the decision tree
plt.figure(figsize=(20,10))
sk_tree.plot_tree(clf_gini, feature_names=feature_names,
class_names=class_names, filled=True)
plt.show()
# Plot the decision tree
plt.figure(figsize=(20,10))
sk_tree.plot_tree(clf_gini, feature_names=feature_names,
class_names=class_names, filled=True)
plt.show()
# Function to print Gini impurity and chosen attribute at each split
def print_gini_and_splits(tree, feature_names):
47
tree_ = tree.tree_
feature_name = [
feature_names[i] if i != sk_tree._tree.TREE_UNDEFINED else
"undefined!"
for i in tree_.feature
]
print("Decision tree splits and Gini impurities:")
for i in range(tree_.node_count):
if tree_.children_left[i] != sk_tree._tree.TREE_LEAF:
print(f"Node {i} (Gini: {tree_.impurity[i]:.4f}): split on
feature '{feature_name[i]}'")
else:
print(f"Node {i} (Gini: {tree_.impurity[i]:.4f}): leaf node")
print_gini_and_splits(clf_gini, feature_names)
# Example test sample
test_sample = {
'Age': '<=30',
'Income': 'Medium',
'Student': 'Yes',
'Credit Rating': 'Fair'
}
# Encode the test sample
encoded_sample = pd.DataFrame([test_sample]).apply(lambda x:
pd.factorize(df[x.name])[0][df[x.name].tolist().index(x[0])])
# Predict using sklearn decision tree
sklearn_prediction = clf_gini.predict([encoded_sample])
decoded_prediction = pd.factorize(df['Buys
Computer'])[1][sklearn_prediction[0]]
print("Prediction for sklearn decision tree:", decoded_prediction)
print()
Output:-
48
49
Practical 6
Aim: Write a program to implement k-Nearest Neighbour algorithm to
classify the iris data set.
Theory: The k-Nearest Neighbors (k-NN) algorithm is a simple, yet powerful, instance-
based learning method used for classification and regression tasks. The core idea of k-NN is
to classify a data point based on the majority class of its k-nearest neighbors in the feature
space.
Key Concepts
1. Distance Metric: The most common distance metric used in k-NN is Euclidean
distance, which measures the straight-line distance between two points in Euclidean
space.
2. Choosing k: The parameter kkk determines how many neighbors to consider for
classifying a new instance. A small kkk can make the model sensitive to noise, while
a large kkk can smooth out class distinctions.
3. Classification: For classification tasks, the algorithm assigns the class label based on
the majority class among the k-nearest neighbors.
Code:-
# Step 1: Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix,
accuracy_score
from mpl_toolkits.mplot3d import Axes3D
# Step 2: Load and display the sample data
data = {
'Age': [19, 21, 20, 23, 31, 22, 35, 25, 23, 64, 30, 67, 35, 58, 24],
50
'Annual Income (k$)': [15, 15, 16, 16, 17, 17, 18, 18, 19, 19, 20, 20,
21, 21, 22],
'Spending Score (1-100)': [39, 81, 6, 77, 40, 76, 6, 94, 3, 72, 79, 65,
76, 76, 94],
'Segment': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1] # 0: Low-
value, 1: High-value
}
df = pd.DataFrame(data)
print("Sample Data:")
print(df.head())
# Step 3: Data Preprocessing
X = df[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]
y = df['Segment']
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
# Step 4: Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y,
test_size=0.2, random_state=42)
# Step 5: Apply KNN Algorithm
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
# Step 6: Evaluation
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
print("\nAccuracy Score:")
print(accuracy_score(y_test, y_pred))
51
# Step 7: Classify new user input
new_user_data = {'Age': [27], 'Annual Income (k$)': [23], 'Spending Score
(1-100)': [60]}
new_user_df = pd.DataFrame(new_user_data)
new_user_scaled = scaler.transform(new_user_df)
new_user_segment = knn.predict(new_user_scaled)
new_user_df['Segment'] = new_user_segment
print("\nNew User Data Prediction:")
print(new_user_df)
# Visualization: Scatter plot of the customer segments
plt.figure(figsize=(10, 6))
sns.scatterplot(x='Annual Income (k$)', y='Spending Score (1-100)',
hue='Segment', data=df, palette='Set1', marker='o', label='Existing Data')
sns.scatterplot(x='Annual Income (k$)', y='Spending Score (1-100)',
hue='Segment', data=new_user_df, palette='Set2', marker='X', s=200,
label='New User Data')
plt.title('Customer Segments with New User Input')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()
plt.show()
# Visualization: 3D plot for KNN decision boundaries and customer segments
including new user input
fig = plt.figure(figsize=(10, 6))
ax = fig.add_subplot(111, projection='3d')
# Plot the existing data with original values
ax.scatter(X['Age'], X['Annual Income (k$)'], X['Spending Score (1-100)'],
c=y, cmap='Set1', s=50, label='Existing Data')
# Plot the new user input with original values
ax.scatter(new_user_df['Age'], new_user_df['Annual Income (k$)'],
new_user_df['Spending Score (1-100)'], c='green', marker='X', s=200,
label='New User Data')
ax.set_xlabel('Age')
52
ax.set_ylabel('Annual Income (k$)')
ax.set_zlabel('Spending Score (1-100)')
plt.title('3D Plot of Customer Segments with New User Input')
ax.legend()
plt.show()
Output:-
53
54
Practical 7
Aim: Implement the different Distance methods (Euclidean) with
Prediction, Test Score and Confusion Matrix.
Theory: The k-Nearest Neighbors (k-NN) algorithm can use various distance metrics to
determine the similarity between data points. The most common distance metric is Euclidean
distance, but there are others such as Manhattan distance, Minkowski distance, and
Hamming distance.
Distance Metrics
1. Euclidean Distance: Measures the straight-line distance between two points in
Euclidean space.
2. Manhattan Distance: Measures the distance between two points in a grid-based path
(like moving along the axes).
3. Minkowski Distance: Generalized distance metric where ppp defines the type of
distance. For p=1p = 1p=1, it's Manhattan; for p=2p = 2p=2, it's Euclidean.
4. Hamming Distance: Measures the distance between two strings of equal length,
counting the positions where the corresponding symbols are different. It is mainly
used for categorical data.
Code:-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
55
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report,confusion_matrix
# Load the Iris dataset
iris = load_iris()
X = iris.data[:, :2] # Select only the first two features (sepal length
and sepal width)
y = iris.target
# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
random_state=42)
# Initialize k-NN classifier with different distance metrics
k = 3
# List of distance metrics to test
distance_metrics = ['euclidean', 'manhattan', 'chebyshev']
# Create subplots for each distance metric
fig, axes = plt.subplots(1, len(distance_metrics), figsize=(15, 5))
for i, metric in enumerate(distance_metrics):
knn_classifier = KNeighborsClassifier(n_neighbors=k, metric=metric)
# Fit the classifier to the training data
knn_classifier.fit(X_train, y_train)
# Make predictions on the test data
y_pred = knn_classifier.predict(X_test)
# Evaluate the classifier's performance
print(f"Distance Metric: {metric}")
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
print("\n")
56
# Visualize the dataset and decision boundaries for the current metric
ax = axes[i]
# Plot the training data points
ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis',
label='Training Data')
# Plot the testing data points
ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='viridis',
marker='x', s=100, label='Testing Data')
# Plot decision boundaries using the current metric
knn_classifier = KNeighborsClassifier(n_neighbors=k, metric=metric)
knn_classifier.fit(X, y)
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min,
y_max, 0.01))
Z = knn_classifier.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
ax.contourf(xx, yy, Z, cmap='viridis', alpha=0.5, levels=range(4))
ax.set_title(f'K-NN ({metric.capitalize()} Metric)')
ax.set_xlabel('Sepal Length (cm)')
ax.set_ylabel('Sepal Width (cm)')
ax.legend()
plt.show()
Output:-
57
58
Practical 8
Aim: Implement the classification model using clustering for the following
techniques with K means clustering with Prediction, Test Score and
Confusion Matrix
Theory: K-Means is primarily an unsupervised clustering algorithm that partitions data into
kkk clusters based on feature similarity. Although it's not inherently a classification method,
it can be used for classification tasks by first clustering the data and then assigning labels
based on the majority class in each cluster.
Key Concepts
1. K-Means Algorithm:
o Initialize kkk centroids randomly from the dataset.
o Assign each data point to the nearest centroid.
o Recalculate the centroids as the mean of all points assigned to each cluster.
o Repeat the assignment and update steps until convergence (no change in
centroids).
2. Using K-Means for Classification:
o Fit K-Means on the training data.
o Assign cluster labels to the training data.
o Use a majority vote to assign labels to the clusters.
o Predict the cluster of test data points and assign the corresponding labels based
on majority class.
Code:-
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.metrics import classification_report, confusion_matrix
#Load the Iris dataset
iris = load_iris()
X = iris.data[:, :2] #Select only the features (sepal lengthy and sepal
width)
59
y = iris.target
#Split database into traini9ng and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
random_state=42)
#Initalize K-Means clustering with the number of clusters equal to the
number of classes
n_clusters = len(np.unique(y))
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
#Fit K-Means clustering to the training data
kmeans.fit(X_train)
#Assign cluster labels to data points in test set
cluster_labels = kmeans.predict(X_test)
#Assign class labels to clusters based on thge most frequent class label in
each cluster
cluster_class_labels = []
for i in range(n_clusters):
cluster_indices = np.where(cluster_labels ==i)[0]
cluster_class_labels.append(np.bincount(y_test[cluster_indices]).argmax())
#Assign cluster class labels to data points in the test set
y_pred = np.array([cluster_class_labels[cluster_labels[i]] for i in
range(len(X_test))])
#Evaluate the classifier's performance
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
#Visualize the dataset and cluster cemters
plt.figure(figsize=(10, 6))
#Plot the training data points
plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis',
label='Training Data')
#Plot testing data
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='viridis',
marker='x', s=100, label='Testing Data')
60
#plt cluster centers
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
c='red', marker='o', s=100, label='Cluster Centers')
plt.xlabel('Sepal Length (cm)')
plt.ylabel('Sepal Width (cm)')
plt.title('K-Means Clustering with Class Labels on Iris Dataset')
plt.legend()
plt.show()
Output:-
61
Practical 9
Aim: Implement the classification model using clustering for the following
techniques with hierarchical clustering with Prediction, Test Score and
Confusion Matrix
Theory: Hierarchical clustering is another unsupervised clustering method that builds a
hierarchy of clusters. Unlike K-Means, which requires the number of clusters to be specified
in advance, hierarchical clustering creates a tree-like structure (dendrogram) that allows you
to choose the number of clusters after examining the results.
While hierarchical clustering is not inherently a classification method, we can use it to group
similar data points and then assign labels based on majority voting within each cluster.
Key Concepts
1. Hierarchical Clustering:
o It can be agglomerative (bottom-up) or divisive (top-down).
o Agglomerative clustering starts with each point as a single cluster and merges
them based on a distance metric (e.g., Euclidean distance).
2. Linkage Criteria: Determines how the distance between clusters is calculated.
o Single Linkage: Distance between the closest points of two clusters.
o Complete Linkage: Distance between the farthest points of two clusters.
o Average Linkage: Average distance between points in two clusters.
o Ward's Linkage: Minimizes the total within-cluster variance.
3. Classification:
o After clustering, we assign the true labels to each cluster and use them for
predictions.
Code:-
import pandas as pd
import numpy as np
from sklearn.cluster import AgglomerativeClustering
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
62
from sklearn.metrics import accuracy_score, confusion_matrix,
classification_report
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage
#Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target
#Step 1: Hierarchical Clustering with different Linkage Methods and Draw
denograms
n_clusters = 3 # Number of clusters
linkage_methods = ['ward', 'single', 'complete'] # Different Linkage
methods
cluster_labels = []
#Define figure and axes for dendrograms
plt.figure(figsize=(15, 5))
dendrogram_axes = []
for i, linkage_method in enumerate(linkage_methods):
labels = AgglomerativeClustering(n_clusters=n_clusters,
linkage=linkage_method).fit_predict(X)
cluster_labels.append(labels)
#Create a dendrgram for the current linkage method
dendrogram_data = linkage(X, method=linkage_method)
dendrogram_axes.append(plt.subplot(1, len(linkage_methods), i+1))
dendrogram(dendrogram_data, orientation='top', labels=labels)
plt.title(f"{linkage_method.capitalize()} Linkage Dendrogram")
plt.xlabel('Samples')
plt.ylabel('Distance')
#Plot clustering results for different linkage methods
63
plt.figure(figsize=(15, 5))
for i, linkage_method in enumerate(linkage_methods):
plt.subplot(1, len(linkage_methods), i + 1)
scatter = plt.scatter(X[:, 0], X[:, 1], c=cluster_labels[i],
cmap='viridis',
label=f'Clusters ({linkage_method.capitalize()}
Linkage)')
plt.title(f"{linkage_method.capitalize()} Linkage")
#Add legend to scatter plots
plt.legend(handles=scatter.legend_elements()[0], labels=[f'Cluster {i}' for
i in range(n_clusters)])
#sTEP 2 :fEATURE ENGINEERING (uSING CLUSTER ASSIGNMENT AS A feature)
X_with_cluster = np.column_stack((X, cluster_labels[-1])) # using complete
linkage
#Step 3: Classification
X_train, X_test, y_train, y_test = train_test_split(X_with_cluster, y,
test_size=0.2, random_state=42)
classifier = RandomForestClassifier(n_estimators=100, random_state=42)
classifier.fit(X_train, y_train)
#Step 4: Prediction
y_pred = classifier.predict(X_test)
#Step 5 : Test Score and Confusion Matrix
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
#Genrate classification report with zero_division parametrs
classification_rep = classification_report(y_test, y_pred, zero_division=0)
#Print cluster description
cluster_descriptions = {
'ward': 'Clusters based on Ward linkage interpretation.',
'single': 'Cluster based on Single linkage interpretation.',
64
'complete': 'Clusters based on Complete linkage interpretation.'
}
for method in linkage_methods:
print(f"Cluster Descriptions ({method.capitalize()} Linkage):")
print(cluster_descriptions[method.lower()]) # Convert to lowercase for
dictionary access
# Print accuracy, confusion matrix, and classification report
print("Accuracy:", accuracy)
print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", classification_rep)
plt.show()
Output:-
65
66
Practical 10
Aim: Implement the Rule based method and test the same.
Theory: Rule-based classification is a type of supervised learning where the model makes
predictions based on a set of "if-then" rules derived from the training data. These rules can be
simple or complex and are often derived from decision trees or created manually based on
domain knowledge.
Key Concepts
1. Rule Creation: Rules can be generated from training data by identifying patterns and
relationships between features and target labels. Common algorithms for generating
rules include:
o Decision Trees: Each path from the root to a leaf node represents a rule.
o Association Rule Learning: Such as Apriori or FP-Growth algorithms, which
identify relationships between variables.
2. Rule Evaluation: Rules can be evaluated based on metrics such as accuracy,
precision, recall, and F1-score.
3. Prediction: For a new instance, the model checks which rule(s) apply and makes
predictions based on those rules.
Code:-
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix,
classification_report
#Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target
#Split the data for testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,
random_state=42)
67
#Define a simple rule-based classifier function
def rule_based_classifier(x):
if x[2] < 2.0:
rule = "If feature 2 < 2.0, assign to Classd 0"
return 0 # Class 0
elif x[3] > 1.5:
rule = "If feature 2 >= 2.0 and feature 3 > 1.5, assign to Class 2"
return 2 # Class 2
else:
rule = "If feature 2 >= 2.0 and feature 3 <=1.5, assign to Class 1"
return 1 # Class 1
print("Rule:", rule)
# Apply the rule-based classifier to make predictions on the test set
y_pred = [rule_based_classifier(x) for x in X_test]
# Calculate accuracy, confusion matrix, and classification report
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred,
target_names=iris.target_names)
# Print the results
print("Accuracy:", accuracy)
print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", classification_rep)
Output:-
68
Practical 11
Aim: Write a program to construct a Bayesian network considering
medical data. Use this model to demonstrate the diagnosis of heart patients
using standard Heart Disease Data Set.
Theory: A Bayesian network is a probabilistic graphical model that represents a set of
variables and their conditional dependencies using a directed acyclic graph (DAG). In the
context of medical diagnosis, a Bayesian network can be used to model the relationships
between symptoms, risk factors, and diseases, allowing for probabilistic inference and
reasoning.
Key Concepts
1. Bayesian Network Structure:
o Nodes represent random variables (e.g., symptoms, diseases).
o Directed edges represent conditional dependencies between variables.
2. Conditional Probability Tables (CPTs): Each node has a CPT that quantifies the
effect of its parents on the node.
3. Inference: Given evidence (e.g., symptoms), the network can compute the probability
of various diseases.
Code:
import numpy as np
import pandas as pd
from pgmpy.models import BayesianNetwork
from pgmpy.estimators import ParameterEstimator, MaximumLikelihoodEstimator
from pgmpy.inference import VariableElimination
import networkx as nx
import matplotlib.pyplot as plt
data = pd.DataFrame (data={'Age': [30, 40, 50, 60, 70],
'Gender': ['Male', 'Female', 'Male', 'Female',
'Male'],
'ChestPain': ['Typical', 'Atypical', 'Typical',
'Atypical', 'Typical'],
69
'HeartDisease': ['Yes', 'No', 'Yes', 'No',
'Yes']})
model = BayesianNetwork([('Age', 'HeartDisease'),
('Gender', 'HeartDisease'),
('ChestPain', 'HeartDisease')])
model.fit(data, estimator=MaximumLikelihoodEstimator)
pos = nx.circular_layout(model)
nx.draw(model, pos, with_labels=True, node_size=5000, node_color="skyblue",
font_size=12, font_color="black")
plt.title("Bayesian Network Structure")
plt.show()
for cpd in model.get_cpds():
print("CPD of", cpd.variable)
print(cpd)
inference = VariableElimination(model)
query = inference.query(variables=['HeartDisease'], evidence={'Age':50,
'Gender': 'Male', 'ChestPain': 'Typical'})
print(query)
Output:-
70
71
Practical 12
Aim: Implement the non-parametric Locally Weighted Regression
algorithm in order to fit data points. Select appropriate data set for your
experiment and draw graphs.
Theory: Locally Weighted Regression (LWR) is a non-parametric regression technique that
fits a model to a subset of the data points that are close to the query point. This method is
particularly useful when the relationship between variables is not global but varies in
different regions of the input space.
Key Concepts
1. Non-parametric Nature: Unlike parametric models that assume a specific form (e.g.,
linear regression), LWR does not assume a global form for the function being
estimated.
2. Weighting Scheme: LWR assigns weights to the training examples based on their
distance to the query point. Typically, a Gaussian kernel is used for weighting:
where τ\tauτ controls the bandwidth of the kernel.
3. Local Model Fitting: For a query point xxx, a weighted linear regression model is
fitted using the nearby points, allowing for different fits in different regions of the
input space.
Code:-
import numpy as np
import matplotlib.pyplot as plt
# Seed for reproducibility
np.random.seed(0)
# Generate random dataset
X = np.sort(5 * np.random.rand(80, 1), axis=0)
y = np.sin(X).ravel()
y[::5] += 3 * (0.5 - np.random.rand(16))
# Locally Weighted Regression function
def locally_weighted_regression(query_point, X, y, tau=0.1):
m = X.shape[0]
72
# Calculate weights
weights = np.exp(-((X - query_point) * 2).sum(axis=1) / (2 * tau * 2))
W = np.diag(weights)
# Add bias term to X
X_bias = np.c_[np.ones((m, 1)), X]
# Calculate theta using weighted least squares
theta =
np.linalg.inv(X_bias.T.dot(W).dot(X_bias)).dot(X_bias.T).dot(W).dot(y)
# Predict for query_point
x_query = np.array([1, query_point])
prediction = x_query.dot(theta)
return prediction
# Generate test points
X_test = np.linspace(0, 5, 100)
# Predict using locally weighted regression
predictions = [locally_weighted_regression(query_point, X, y, tau=0.1) for
query_point in X_test]
# Plot results
plt.scatter(X, y, color='black', s=30, marker='o', label='Data Points')
plt.plot(X_test, predictions, color='blue', linewidth=2, label='LWR Fit')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Locally Weighted Regression')
plt.legend()
plt.show()
Output:-
73
Practical 13
Aim: Implement ANN to solve the XOR problem using forward/
backward propagation and sigmoid activation function.
Theory: The XOR (exclusive OR) problem is a classic example used to demonstrate the
capabilities of neural networks, particularly their ability to model non-linear relationships.
The XOR function outputs true (1) if exactly one of the inputs is true (1), and false (0)
otherwise.
Key Concepts
1. Neural Network Structure:
o Input Layer: Takes in two binary inputs.
o Hidden Layer: Typically contains multiple neurons to capture non-linear
patterns.
o Output Layer: Produces a single output indicating the XOR result.
2. Activation Function:
o Sigmoid Activation Function: σ(x)=11+e−x\sigma(x) = \frac{1}{1 + e^{-
x}}σ(x)=1+e−x1 This function squashes output values to a range between 0
and 1, making it suitable for binary classification.
3. Forward Propagation: Calculates the output of the network based on the current
weights and biases.
4. Backward Propagation: Adjusts the weights based on the error between predicted
and actual outputs using gradient descent.
Code:
import numpy as np
import matplotlib.pyplot as plt
def sigmoid(x):
return 1/(1+np.exp(-x))
def sigmoid_derivative(x):
return x*(1-x)
74
class NeuralNetwork:
def __init__(self, input_size, hidden_size, output_size):
self.weights_input_hidden = np.random.uniform(size=(input_size,
hidden_size))
self.weights_hidden_output = np.random.uniform(size=(hidden_size,
output_size))
def forward(self, X):
self.hidden_input = np.dot(X, self.weights_input_hidden)
self.hidden_output = sigmoid(self.hidden_input)
self.output = sigmoid(np.dot(self.hidden_output,
self.weights_hidden_output))
return self.output
def backward(self, X, y, learning_rate):
error_output = y-self.output
delta_output = error_output*sigmoid_derivative(self.output)
error_hidden = delta_output.dot(self.weights_hidden_output.T)
delta_hidden = error_hidden*sigmoid_derivative(self.hidden_output)
self.weights_hidden_output +=
self.hidden_output.T.dot(delta_output)*learning_rate
self.weights_input_hidden += X.T.dot(delta_hidden)*learning_rate
def train(self, X, y, learning_rate, epochs):
self.loss_history = []
for _ in range(epochs):
output = self.forward(X)
error = y-output
self.loss_history.append(np.mean(error**2))
self.backward(X,y,learning_rate)
def predict(self, X):
return self.forward(X)
X = np.array([[0,0],[0,1],[1,0],[1,1]])
y = np.array([[0], [1], [1], [0]])
input_size = 2
hidden_size = 4
output_size = 1
75
learning_rate = 0.1
epochs = 10000
nn = NeuralNetwork(input_size, hidden_size, output_size)
nn.train(X, y, learning_rate, epochs)
predictions = nn.predict(X)
plt.figure(figsize=(8, 6))
plt.scatter(X[:,0], X[:,1], c=y, cmap='viridis', label='XOR Data')
plt.scatter(X[:,0], X[:,1], c=np.round(predictions), cmap='plasma',
marker='x', s=200, label='Predictions')
plt.title('XOR Dataset and Predictions')
plt.xlabel('Input 1')
plt.ylabel('Input 2')
plt.legend()
for i in range(len(X)):
print(f"Input: {X[i]}, Actual: {y[i]}, Predicted:
{np.round(predictions[i])}")
plt.show()
Output:-
